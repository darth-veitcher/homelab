Having integrated the [Kubernetes API with Keycloak](01.kubernetes.md) in our previous exercise we'll now implement some integrated security around the Kubernetes dashboard which we installed and saw [earlier on](../../01.infrastructure/05.monitoring/01.kubernetes.dashboard.md).

As we noticed previously we could implement an authentication/authorisation step with the dashboard via a login page which allowed users to upload a kubeconfig file or enter a bearer token.

Alternatively, the dashboard supports the use of authorisation headers to supply bearer tokens `(Authorization: Bearer )`. If we can intercept the request to the dasboahrd and inject this header based on our OIDC id-token integration we could bypass this login screen and implement some Single Sign On (SSO).

???+ success "Checklist"
    **Pre-requisites**
    
    You'll need to have the following in place before proceeding:

    * [x] Kubernetes cluster with RBAC
    * [x] OpenLDAP+Keycloak installed
      * [x] A valid realm `development`
      * [x] A user `jamesveitch`
        * [x] `memberOf` the `cluster-admin` group
    * [x] Kubernetes API setup to accept tokens issued by Keycloak
    * [x] A Kubernetes `ClusterRoleBinding` for the `cluster-admin` role mapped against the `cluster-admin` LDAP group

    **We'll configure**

    * [ ] A Keycloak OIDC Client for the Kubernetes Dashboard
    * [ ] [Keycloak Gatekeeper](https://github.com/keycloak/keycloak-gatekeeper) (previously called `keycloak-proxy`) server to intercept requests

# Setup OIDC Client
Following similar steps to previously used we need to [create a new Client](../01.kubernetes#configure-keycloak).

* `Client ID`: kubernetes-dashboard
* `Protocol`: openid-connect
* `Root URL`: (leave this blank)

You're now presented with a fairly lengthy configuration screen. Use the following options as overrides to what should be provided as existing defaults.

* `Access Type`: confidential
* `Valid Redirect URIs`: https://dashboard.jamesveitch.dev/oauth/callback (this is what we define in the `Ingress`)

Once saved we now have a new client.

??? question "Mapping of Groups"
    Depending on how you setup the mapping of our `Groups` [previously](../01.kubernetes#telling-applications-about-our-users-groupsroles) you either need to create a specific client mapper again or (if you used the `Scope` it will just inherit).

# Setup `keycloak-gatekeeper`
We'll now need to modify our previous dashboard setup process to incorporate the proxy server. Previously we applied the following manifests:

| name               | description                                                               |
|--------------------|---------------------------------------------------------------------------|
| recommended        | creates service accounts, configmaps, roles and the dashboard deployment  
| certificate        | requests a trusted TLS certificate, via `cert-manager` from `LetsEncrypt`
| ingress            | configured an nginx ingress, using the certificate to direct inbound traffic on [dashboard.jamesveitch.dev](https://dashboard.jamesveitch.dev) to the dashboard service configured by `recommended` above

In order to effectively still maintain all of the existing goodness, plus then add in some security with `keycloak-gatekeeper`, we need to create a new `Deployment` and associated `Service` to intercept the requests and point the existing `Ingress` at this `Service` instead. Essentially introducing an additional hop.

The flow of traffic will now be:

Client --> Nginx (Ingress) --> <u>Keycloak Gatekeeper (Service)</u> --> Kubernetes Dashboard (Service) --> Kubernetes Dashboard (Pod) --> Kubernetes Dashboard (Container)

<small><i>Really the client will hit the DNS first and then a LoadBalancer but we'll skip that in the above for the sake of readability.</i></small>

???+ tip "Further Reading"
    There's some detailed [documentation](https://www.keycloak.org/docs/latest/securing_apps/index.html#_keycloak_generic_adapter) for configuring keycloak-gatekeeper in case the below isn't to your liking in terms of settings.

Firstly let's create an [encryption key](https://www.keycloak.org/docs/latest/securing_apps/index.html#encryption-key) for the proxy to use.

>In order to remain stateless and not have to rely on a central cache to persist the refresh_tokens, the refresh token is encrypted and added as a cookie using crypto/aes. The key must be the same if you are running behind a load balancer. The key length should be either 16 or 32 bytes, depending or whether you want AES-128 or AES-256.

```bash
# Generate 32bits of randomness, SHA256 hash it and then base64 encode before taking the
# first 32 characters
dd if=/dev/urandom bs=1 count=32 2>/dev/null | sha256sum | base64 | head -c 32 ; echo
```

???+ important "Add a mapper for `client_id`"
    As per [2.4.29. Known Issues](https://www.keycloak.org/docs/latest/securing_apps/index.html#known-issues) (I always love these...)

    >There is a known issue with the Keycloak server 4.7.0.Final in which Gatekeeper is unable to find the client_id in the aud claim. This is due to the fact the client_id is not in the audience anymore. The workaround is to add the "Audience" protocol mapper to the client with the audience pointed to the client_id. For more information, see [KEYCLOAK-8954](https://issues.jboss.org/browse/KEYCLOAK-8954).

    So we need to add a mapper specifically into our client in Keycloak to ensure the `client_id` is passed back. Otherwise you'll get a blank screen on redirecting (post-authentication) and your logs will show the below.

    ```bash
    1.5776252005434518e+09	error	no session found in request, redirecting for authorization	{"error": "authentication session not found"}
    1.5776252006586719e+09	error	unable to verify the id token	{"error": "oidc: JWT claims invalid: invalid claims, cannot find 'client_id' in 'aud' claim, aud=[realm-management account], client_id=kubernetes-dashboard"}
    ```

???+ important "proxy-buffer-size"
    You need to increase the nginx-ingress proxy buffer size. Note the use of the annotation on the Ingress.

    ```yaml
    nginx.ingress.kubernetes.io/proxy-buffer-size: "64k"
    ```

