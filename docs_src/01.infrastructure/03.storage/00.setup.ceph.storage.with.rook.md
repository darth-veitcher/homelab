In [part 2]() we installed Kubernetes and setup a user (in my case `adminlocal`) on our cluster with the ability to run administrative kubernetes commands.

etcd (key/value store), Rook, Promethues and Vault are all examples of technologies we will be using in our cluster and are deployed using [Kubernetes Operators](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/). In this section we'll be deploying the [Rook](https://rook.io) storage orchestrator with [Ceph](http://ceph.com) as a storage provider. 

# Components
## Rook
[Rook](https://rook.io) allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider.

More information on the architecure can be found in the [docs](https://rook.github.io/docs/rook/master/ceph-storage.html)

![Rook Architecture](https://rook.github.io/docs/rook/master/media/rook-architecture.png)

![Rook Kubernetes](https://rook.github.io/docs/rook/master/media/kubernetes.png)

## Ceph
Ceph provides three types of storage:

* `object`: compatible with S3 API
* `file`: files and directories (incl. NFS); and
* `block`: replicate a hard drive

There a 4 key components of the architecture to be aware of (shown above in the Rook diagram):

* `Monitor` (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication
* `Manager` daemon: keeps track of state in cluster and metrics
* `OSDs` (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage.
* `MSDs` (Metatdata Server): concerned with filesystem storage type only

Under the hood everything is stored as an object in logical storage pools.

# Installation and Setup
## Deployment of the Rook operator
See the Rook [quickstart](https://rook.io/docs/rook/v1.1/ceph-quickstart.html)

We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple `mon` components onto a single node. Similar to when we [removed the the control plane node taint](01.infrastructure/02.kubernetes/00.configuring.kubernetes.md#remove-the-control-plane-node-isolation-taint) Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes).

```bash
# create a working directory
mkdir -p ~/rook && \
cd ~/rook

# download the sample files
# all of these can be found here: https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/common.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/operator.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/cluster.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/toolbox.yaml

# modify the cluster spec
#   - allow multiple mons per node
sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml
```

In addition, because we've setup our encrypted data storage to be mounted at `/data/${BLKID}` we will edit the default storage options to remove the `useAllDevices` selection and, instead, specify the directories. See the [docs](https://rook.io/docs/rook/v1.1/ceph-cluster-crd.html#storage-configuration-cluster-wide-directories) for more details.

```bash
# set default storage location and remove default `useAllDevices: true`
sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml
# any devices starting with 'sd' (but not sda as that's our root filesystem)
sed -i.bak 's/deviceFilter:/deviceFilter: ^sd[^a]/' cluster.yaml
# encrypt them with LUKS
# see conversation https://github.com/rook/rook/issues/923#issuecomment-557651052
sed -i.bak 's/# encryptedDevice: "true"/encryptedDevice: "true"/' cluster.yaml
```

With these configurations now downloaded we'll apply them in the following order.

```bash
kubectl create -f ~/rook/common.yaml; \
kubectl create -f ~/rook/operator.yaml
```

???+ warning "Verify the `rook-ceph-operator` is in the `Running` state"
    Use `kubectl -n rook-ceph get pod` to check we have a running state.
    ```bash
    root@banks:~# kubectl -n rook-ceph get pod
    NAME                                            READY   STATUS    RESTARTS   AGE
    ...
    rook-ceph-operator-c8ff6447d-tbh5c              1/1     Running   0          6m18s
    ```

## Create the Rook cluster
Assuming the operator looks ok we can now create the cluster
```bash
kubectl create -f ~/rook/cluster.yaml
```

To verify the state of the cluster we will connect to the [Rook Toolbox](https://rook.io/docs/rook/v1.1/ceph-toolbox.html)

```bash
kubectl create -f ~/rook/toolbox.yaml
```

Wait for the toolbox pod to enter a `running` state:

```bash
kubectl -n rook-ceph get pod -l "app=rook-ceph-tools"
```

Once the rook-ceph-tools pod is running, you can connect to it with:
```bash
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
```

When inside the toolbox run `ceph status`.

???+ info "ceph status"
    ```bash
    [root@banks /]# ceph status
    cluster:
        id:     06da5ebc-d2f3-4366-a51c-db759d8bc664
        health: HEALTH_OK
    
    services:
        mon: 3 daemons, quorum a,b,c (age 2m)
        mgr: a(active, since 102s)
        osd: 2 osds: 2 up (since 33s), 2 in (since 33s)
    
    data:
        pools:   0 pools, 0 pgs
        objects: 0 objects, 0 B
        usage:   2.0 GiB used, 3.6 TiB / 3.6 TiB avail
        pgs:         
    ```

    * All mons should be in quorum
    * A mgr should be active
    * At least one OSD should be active
    * If the health is not HEALTH_OK, the warnings or errors should be investigated

    ### Troubleshooting: [errno 2]
    **NB:** You might get an error `unable to get monitor info from DNS SRV with service name: ceph-mon` or `[errno 2] error connecting to the cluster` when running `ceph status` in the toolbox if you've typed all of the above commands very quickly. This is usually because the cluster is still starting and waiting for all the monitors to come up and establish connections. Go get a cup of tea / wait a couple of minutes and try again.

    In the `cluster.yaml` spec the default number of `mon` instances is `3`. As a result if you don't have three of these pods running then your cluster is still initialising. You can run `kubectl -n rook-ceph logs -l "app=rook-ceph-operator"` to see an output of the logs from the operator and search for `mons running`. As you can see below it took mine around a minute to initialise all 3. To see what monitors you have run `kubectl -n rook-ceph get pod -l "app=rook-ceph-mon"`.

    ```bash hl_lines="3 5"
    $ kubectl -n rook-ceph get pod -l "app=rook-ceph-mon"
    NAME                               READY   STATUS    RESTARTS   AGE
    rook-ceph-mon-a-5d677b5849-t4xct   1/1     Running   0          82s
    rook-ceph-mon-b-6cfbcf8db4-7cwxp   1/1     Running   0          66s
    rook-ceph-mon-c-8f858c585-c9z5b    1/1     Running   0          50s
    ```

When you are done with the toolbox, you can remove the deployment:
```
kubectl -n rook-ceph delete deployment rook-ceph-tools
```

??? tip "If you want to delete the cluster and start again..."
    Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse. There are some additional pointers [here](https://rook.io/docs/rook/master/ceph-teardown.html) in the docs.
    ```bash
    kubectl delete -f toolbox.yaml; \
    kubectl delete -f cluster.yaml; \
    kubectl delete -f operator.yaml; \
    kubectl delete -f common.yaml; \
    rm -rf ~/rook; \
    sudo rm -rf /var/lib/rook/*
    ```

# Dashboard and Storage
We now have a cluster running but no configured storage or an ability to review status (other than logging into the toolbox).
