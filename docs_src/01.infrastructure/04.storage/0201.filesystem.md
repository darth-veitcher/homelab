As per the [docs](https://rook.io/docs/rook/v1.1/ceph-filesystem.html)

>A shared file system can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared filesystem.

```bash
cd ~/rook/storage
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/filesystem.yaml

# - replicas: 1  # we dont want to replicate this
# - failureDomain: osd  # we don't want it to require multiple nodes
sed -i.bak 's/size: 3/size: 1/g' filesystem.yaml; \
sed -i.bak 's/failureDomain: host/failureDomain: osd/g' filesystem.yaml; \
kubectl create -f ~/rook/storage/filesystem.yaml
```

Wait for the mds pods to start
```bash
adminlocal@banks:~/rook/storage$ kubectl -n rook-ceph get pod -l app=rook-ceph-mds
NAME                                    READY   STATUS    RESTARTS   AGE
rook-ceph-mds-myfs-a-84ccb448b5-9jbds   1/1     Running   0          21s
rook-ceph-mds-myfs-b-7d85c48b4c-72q9d   0/1     Pending   0          20s
```

**NB:** Because there is a `podAntiAffinity` spec in the filesystem.yaml `placement` section you may only see one pod running if we have a single node cluster. This is fine. Running a `ceph status` via the toolbox will reinforce this.

```bash hl_lines="6"
# ceph status
...
  services:
    mon: 3 daemons, quorum a,b,c (age 3d)
    mgr: a(active, since 3d)
    mds: myfs:1 {0=myfs-a=up:active}
    osd: 2 osds: 2 up (since 3d), 2 in (since 3d)
    rgw: 1 daemon active (my.store.a)
...
```

As with other storage options, we need to create a `StorageClass` for a Filesystem. This will use the CSI Driver (which is the preferred driver going forward for K8s 1.13 and newer).

```bash
cd ~/rook/storage
wget -O "storageclass-cephfs.yaml" https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml
kubectl create -f ~/rook/storage/storageclass-cephfs.yaml
```

## Consume the Shared File System: K8s Registry Sample
We'll deploy a private docker registry that uses this shared filesystem via a `PersistentVolumeClaim`.

```bash
cd ~/rook/
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml
kubectl create -f ~/rook/kube-registry.yaml
```

### Configure registry
See [Github docs](https://github.com/kubernetes/kubernetes/tree/release-1.9/cluster/addons/registry) for further details.

```bash
mkdir -p ~/registry
cd ~/registry
```

Now create a `service.yaml` file.

```
# file: ~/registry/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kube-registry
  namespace: kube-system
  labels:
    k8s-app: kube-registry-upstream
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "KubeRegistry"
spec:
  selector:
    k8s-app: kube-registry
  ports:
  - name: registry
    port: 5000
    protocol: TCP
```

Apply this with `kubectl create -f service.yaml`.

With the service created we'll use a `DaemonSet` to deploy a pod onto every node in the cluster (so that Dokcer sees it as `localhost`).

```bash
# file: ~/registry/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-registry-proxy
  namespace: kube-system
  labels:
    k8s-app: kube-registry-proxy
    kubernetes.io/cluster-service: "true"
    version: v0.4
spec:
  selector:
    matchLabels:
      name: kube-registry
  template:
    metadata:
      labels:
        k8s-app: kube-registry-proxy
        kubernetes.io/name: "kube-registry-proxy"
        kubernetes.io/cluster-service: "true"
        version: v0.4
        name: kube-registry
    spec:
      containers:
      - name: kube-registry-proxy
        image: gcr.io/google_containers/kube-registry-proxy:0.4
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        env:
        - name: REGISTRY_HOST
          value: kube-registry.kube-system.svc.cluster.local
        - name: REGISTRY_PORT
          value: "5000"
        ports:
        - name: registry
          containerPort: 80
          hostPort: 5000
```

Apply with a `kubectl create -f ~/registry/daemonset.yaml` and then check for completion of pods.

```bash
$ kubectl -n kube-system get pod -l 'name=kube-registry'
NAME                        READY   STATUS    RESTARTS   AGE
kube-registry-proxy-vtd56   1/1     Running   0          5m34s
```

We can check the registry has been deployed by running `curl localhost:5000/image` and expecting a `404` response.

```bash
$ curl localhost:5000/image
404 page not found
```

### Push an image to the registry
As per [Docker docs](https://www.docker.com/blog/how-to-use-your-own-registry/) we will push a small docker alpine image to our new local private repository.

```bash
sudo docker pull alpine
sudo docker images | grep alpine | grep latest
sudo docker tag 965ea09ff2eb 127.0.0.1:5000/alpine
sudo docker push 127.0.0.1:5000/alpine
```

### Mount the filesystem in toolbox to confirm
```bash
# Create the directory
mkdir /tmp/registry

# Detect the mon endpoints and the user secret for the connection
mon_endpoints=$(grep mon_host /etc/ceph/ceph.conf | awk '{print $3}')
my_secret=$(grep key /etc/ceph/keyring | awk '{print $3}')

# Mount the file system
mount -t ceph -o mds_namespace=myfs,name=admin,secret=$my_secret $mon_endpoints:/ /tmp/registry

# See your mounted file system
df -h
```

With the filesystem mounted we'll confirm there's an alpine repository now after our push above.
```bash
# find /tmp/registry -name "alpine"                 
/tmp/registry/volumes/csi/csi-vol-77b79f13-11ee-11ea-9848-7a3d11f24466/docker/registry/v2/repositories/alpine
```

# Teardown
```bash
kubectl delete -f ~/registry/daemonset.yaml; \
kubectl delete -f ~/registry/service.yaml; \
kubectl delete -f ~/rook/kube-registry.yaml
```
To delete the filesystem components and backing data, delete the Filesystem CRD. **Warning:** Data will be deleted
```bash
kubectl -n rook-ceph delete cephfilesystem myfs
```