In [part 2]() we installed Kubernetes and setup a user (in my case `adminlocal`) on our cluster with the ability to run administrative kubernetes commands.

etcd (key/value store), Rook, Promethues and Vault are all examples of technologies we will be using in our cluster and are deployed using [Kubernetes Operators](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/). In this section we'll be deploying the [Rook](https://rook.io) storage orchestrator with [Ceph](http://ceph.com) as a storage provider. 

???+ important
    This is not the final configuration I've used for my own storage server but is a good way to generically start with your own cluster setup. I suggest running through this first and then looking back at what I've done in the following areas. This section is all valid, I have though specifcially tailored my storage and cluster definitions to take into account requirements for lifecycle management (using cache-tiering) and device classes as well as local encryption (using [devicePathFilter](https://github.com/cybozu-go/rook/commit/55855c89a2cdf9721dd85755b4b1f3febd822434) to specify the LUKS devices):

    <!-- #TODO: update links -->
    * [Setting up the physical host (`clarke`)]()
    * Specific manifests I've used:
        * [Cluster CRD]()
        * [Cache Tierieing Storage Pools]()

# Components
## Rook
[Rook](https://rook.io) allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider.

More information on the architecure can be found in the [docs](https://rook.github.io/docs/rook/master/ceph-storage.html)

![Rook Architecture](https://rook.github.io/docs/rook/master/media/rook-architecture.png)

![Rook Kubernetes](https://rook.github.io/docs/rook/master/media/kubernetes.png)

## Ceph
Ceph provides three types of storage:

* `object`: compatible with S3 API
* `file`: files and directories (incl. NFS); and
* `block`: replicate a hard drive

There a 4 key components of the architecture to be aware of (shown above in the Rook diagram):

* `Monitor` (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication
* `Manager` daemon: keeps track of state in cluster and metrics
* `OSDs` (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage.
* `MSDs` (Metatdata Server): concerned with filesystem storage type only

Under the hood everything is stored as an object in logical storage pools.

# Installation and Setup
## Deployment of the Rook operator
See the Rook [quickstart](https://rook.io/docs/rook/v1.1/ceph-quickstart.html)

We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple `mon` components onto a single node. Similar to when we [removed the the control plane node taint](../02.kubernetes/00.configuring.kubernetes.md#remove-the-control-plane-node-isolation-taint) Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes).

```bash
# create a working directory
mkdir -p ~/rook && \
cd ~/rook

# download the sample files
# all of these can be found here: https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph
export ROOK_VERSION=1.2
wget https://raw.githubusercontent.com/rook/rook/release-${ROOK_VERSION}/cluster/examples/kubernetes/ceph/common.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-${ROOK_VERSION}/cluster/examples/kubernetes/ceph/operator.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-${ROOK_VERSION}/cluster/examples/kubernetes/ceph/cluster.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-${ROOK_VERSION}/cluster/examples/kubernetes/ceph/toolbox.yaml

# modify the cluster spec
#   - allow multiple mons per node
sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml
```

In addition, because we've setup our encrypted data storage to be mounted at `/data/${BLKID}` we will edit the default storage options to remove the `useAllDevices` selection and, instead, specify the directories. See the [docs](https://rook.io/docs/rook/v1.1/ceph-cluster-crd.html#storage-configuration-cluster-wide-directories) for more details.

```bash
# set default storage location and remove default `useAllDevices: true`
sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml
# any devices starting with 'sd' (but not sda as that's our root filesystem)
sed -i.bak 's/deviceFilter:/deviceFilter: ^sd[^a]/' cluster.yaml
# encrypt them with LUKS
# see conversation https://github.com/rook/rook/issues/923#issuecomment-557651052
sed -i.bak 's/# encryptedDevice: "true"/encryptedDevice: "true"/' cluster.yaml
```

???+ tip "Wiping disks for usage"
    Because Rook will fail if it finds an existing "in use" filesystem or disk we need to wipe the disks in the host which we want to use.

    For example. In use disks below (from a previous cluster).

    ???info "in use"
        ```bash
        $ lsblk

        NAME                                                                                                 MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
        loop0                                                                                                  7:0    0  88.5M  1 loop /snap/core/7270
        sda                                                                                                    8:0    0 111.3G  0 disk 
        ├─sda1                                                                                                 8:1    0     1M  0 part 
        └─sda2                                                                                                 8:2    0 111.3G  0 part /
        sdb                                                                                                    8:16   0   1.8T  0 disk 
        └─ceph--04176411--2495--4944--abae--4ac6cedd8b46-osd--data--20a17864--c8d4--48f7--8aa0--e9d393fbb9e4 253:1    0   1.8T  0 lvm  
        sdc                                                                                                    8:32   0   1.8T  0 disk 
        └─ceph--38ed1893--7746--477a--a8c8--dae66f8360e5-osd--data--0940f2cf--d1c2--489f--9c8a--31d75d759be8 253:0    0   1.8T  0 lvm  
        sdd                                                                                                    8:48   0     2T  0 disk 
        sde                                                                                                    8:64   0     2T  0 disk 
        sr0                                                                                                   11:0    1  1024M  0 rom 
        ```
    
    We can delete and wipe the disk of partition maps.
    ```bash
    # replicate the rook commands (and use regex to exclude sda like the manifest)
    export DISKS=$(lsblk --all --noheadings --list --output KNAME | grep sd[^a]); \
    for d in $DISKS; do \
        export DISK=/dev/$d; \
        sudo wipefs $DISK; \
        sudo vgremove -y $(sudo pvscan | grep $DISK | awk '{print $4}'); \
        sudo dd if=/dev/zero of=$DISK bs=512 count=10; \
    done
    ```

    Now, if appropriate, remove the LVM with the appropriate combination of `pvremove`, `lvremove`, `vgremove` etc.
    ```bash
    $ sudo vgremove -y $(sudo pvscan | grep $DISK | awk '{print $4}')

    Logical volume "osd-data-20a17864-c8d4-48f7-8aa0-e9d393fbb9e4" successfully removed
    Volume group "ceph-04176411-2495-4944-abae-4ac6cedd8b46" successfully removed
    ```

With these configurations now downloaded we'll apply them in the following order.

```bash
kubectl create -f ~/rook/common.yaml; \
kubectl create -f ~/rook/operator.yaml
```

???+ warning "Verify the `rook-ceph-operator` is in the `Running` state"
    Use `kubectl -n rook-ceph get pod` to check we have a running state.
    ```bash
    root@banks:~# kubectl -n rook-ceph get pod
    NAME                                            READY   STATUS    RESTARTS   AGE
    ...
    rook-ceph-operator-c8ff6447d-tbh5c              1/1     Running   0          6m18s
    ```

## Create the Rook cluster
Assuming the operator looks ok we can now create the cluster
```bash
kubectl create -f ~/rook/cluster.yaml
```

???+ info "Watch cluster creation"
    In order to monitor the cluster being created we can run `watch kubectl -n rook-ceph get pod`. The operator will take care of the orchestration but you should notice some phases of: 
    
    * a `detect-version` pod being created first (and then disappearing); followed by 
    * a series of `mon` pods being spun up (expect to see 3 of them by default `a`, `b`, `c`); followed by 
    * an `osd-prepare-<node>` pod for each of our nodes which is where the ceph volumes are being created on the disks we've supplied.

    ```bash
    NAME                                               READY   STATUS    RESTARTS   AGE
    csi-cephfsplugin-provisioner-56c8b7ddf4-tlnxm      4/4     Running   0          6m29s
    csi-cephfsplugin-provisioner-56c8b7ddf4-vbwvr      4/4     Running   0          6m29s
    csi-cephfsplugin-z5zxb                             3/3     Running   0          6m29s
    csi-rbdplugin-2pppd                                3/3     Running   0          6m29s
    csi-rbdplugin-provisioner-6ff4dd4b94-24q9d         5/5     Running   0          6m29s
    csi-rbdplugin-provisioner-6ff4dd4b94-m9f9x         5/5     Running   0          6m29s
    rook-ceph-crashcollector-clarke-85b98f894d-z6r86   1/1     Running   0          5m17s
    rook-ceph-mgr-a-5574b9fb9b-glfkb                   1/1     Running   0          5m17s
    rook-ceph-mon-a-679b66554d-6qbg7                   1/1     Running   0          6m2s
    rook-ceph-mon-b-84bf565bdc-49dwd                   1/1     Running   0          5m47s
    rook-ceph-mon-c-65997d4bf4-q2krn                   1/1     Running   0          5m32s
    rook-ceph-operator-6d8fb9498b-45ffm                1/1     Running   0          7m18s
    rook-ceph-osd-prepare-clarke-xx557                 1/1     Running   0          5m1s
    rook-discover-xnkx4                                1/1     Running   0          6m46s
    ```

    Once this `osd-prepare` pod has completed we should have a cluster available. If you want you can check the logs for this pod with a `kubectl -n rook-ceph logs rook-ceph-osd-prepare-clarke-xx557`.

    **NB:** This whole process can take a few minutes depending on the size of the disks you've selected. With ~6TB of HDDs and ~2TB of SSDs across 5 physical disks my `prepare` pod took 12mins and, start to finish, the whole cluster spin up (including the prepare) took 14mins. 

To verify the state of the cluster we will connect to the [Rook Toolbox](https://rook.io/docs/rook/v1.1/ceph-toolbox.html)

```bash
kubectl create -f ~/rook/toolbox.yaml
```

Wait for the toolbox pod to enter a `running` state:

```bash
kubectl -n rook-ceph get pod -l "app=rook-ceph-tools"
```

Once the rook-ceph-tools pod is running, you can connect to it with:
```bash
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
```

When inside the toolbox run `ceph status` after setting a custom prompt so we don't forget where we are.

```bash
export PS1="ceph-toolbox# "
```

???+ info "ceph status"
    ```bash
    [root@banks /]# ceph status
    cluster:
        id:     06da5ebc-d2f3-4366-a51c-db759d8bc664
        health: HEALTH_OK
    
    services:
        mon: 3 daemons, quorum a,b,c (age 2m)
        mgr: a(active, since 102s)
        osd: 2 osds: 2 up (since 33s), 2 in (since 33s)
    
    data:
        pools:   0 pools, 0 pgs
        objects: 0 objects, 0 B
        usage:   2.0 GiB used, 3.6 TiB / 3.6 TiB avail
        pgs:         
    ```

    * All mons should be in quorum
    * A mgr should be active
    * At least one OSD should be active
    * If the health is not HEALTH_OK, the warnings or errors should be investigated

    ### Toubleshooting: Not all OSDs (disks) are created
    The task to prepare a disk can vary in duration based on it's size and a number of other factors. Start off by checking that the `prepare` has actually finished.

    ```
    $ watch kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare

    NAME                                      READY   STATUS      RESTARTS   AGE
    rook-ceph-osd-prepare-banks.local-dlvk7   0/1     Completed   0          2m31s
    ```

    If this doesn't show `Completed` then it's still performing the tasks. Wait for it to complete and then go back into the toolbox and check the `ceph status` output again.

    ### Troubleshooting: [errno 2]
    **NB:** You might get an error `unable to get monitor info from DNS SRV with service name: ceph-mon` or `[errno 2] error connecting to the cluster` when running `ceph status` in the toolbox if you've typed all of the above commands very quickly. This is usually because the cluster is still starting and waiting for all the monitors to come up and establish connections. Go get a cup of tea / wait a couple of minutes and try again.

    In the `cluster.yaml` spec the default number of `mon` instances is `3`. As a result if you don't have three of these pods running then your cluster is still initialising. You can run `kubectl -n rook-ceph logs -l "app=rook-ceph-operator"` to see an output of the logs from the operator and search for `mons running`. As you can see below it took mine around a minute to initialise all 3. To see what monitors you have run `kubectl -n rook-ceph get pod -l "app=rook-ceph-mon"`.

    ```bash hl_lines="3 5"
    $ kubectl -n rook-ceph get pod -l "app=rook-ceph-mon"
    NAME                               READY   STATUS    RESTARTS   AGE
    rook-ceph-mon-a-5d677b5849-t4xct   1/1     Running   0          82s
    rook-ceph-mon-b-6cfbcf8db4-7cwxp   1/1     Running   0          66s
    rook-ceph-mon-c-8f858c585-c9z5b    1/1     Running   0          50s
    ```

When you are done with the toolbox, you can remove the deployment:
```
kubectl -n rook-ceph delete deployment rook-ceph-tools
```

??? tip "If you want to delete the cluster and start again..."
    Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse. There are some additional pointers [here](https://rook.io/docs/rook/master/ceph-teardown.html) in the docs.
    ```bash
    kubectl delete -f ~/rook; \
    rm -rf ~/rook; \
    sudo rm -rf /var/lib/rook/*
    ```

# Dashboard and Storage
We now have a cluster running but no configured storage or an ability to review status (other than logging into the toolbox).
