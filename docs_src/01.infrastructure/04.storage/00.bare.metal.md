As previously highlighted the overhead associated with maintaining a full Rook-Ceph cluster just wasn't worth it when I reached +8 physical disks. Each OSD container requests 1 vCPU and 1GB RAM which quickly started to cause issues on hardware I was attempting to run as a hyperconverged node.

As a result I've looked to replicate lifecycle management with the following broad goals whilst still maintaining the ability to pass persistent volumes to Kubernetes pods via NFS:

* Cache using SSDs for writes and reads
* Near-line storage in HDDs which can be swapped in and out on demand
* Cold storage on Google Drive

With the encrypted disks already setup from the hosts section previously I'm going to modify the existing setup (initially for Rook-Ceph) slightly.

* Use mergerfs to create a single filesystem from the above devices.
* Devices will be split as follows
  * `SSDs` - split into two partitions. One for media (cache) and one for data, docs and PVs (will become a RAID10 across 4 devices).
  * `HDDs` - Can partition the disks for `single` with btrfs.
* SnapRAID will be applied with an encrypted rclone mount used as the parity (no idea if this will work...)

# Partition SSDs
As I have 4 x 1TB SSDs I'm going to partition these to allow me to essentially replicate the `pool` type configuration I would have been able to implement with Ceph.

* `storage`: a RAID10 configuration which will be used to house Kubernetes PVs for containers, documents etc.
* `cache`: a RAID0 configuration which I'll use as a cache tier in front of the spinning rust.

```bash
sudo apt-get install -y gdisk

# Write the first partition table for SSD
# 450GB for RAID10
# Remainder for RAID0
cat <<EOF | sudo gdisk /dev/mapper/data-ssd-349eadd6-1037-4c87-8797-ecb929d03744
n
1
2048
450G
8300
n
2


8300
w
Y
EOF
```

We should now have a partition table on our first SSD as per the below.

```bash
$ sudo gdisk /dev/mapper/data-ssd-1bca6bcf-9b7b-435a-8867-24e71d330b73

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048       943718400   450.0 GiB   8300  Linux filesystem
   2       943720448      2000405134   503.9 GiB   8300  Linux filesystem
```

This can be copied to the other SSDs for ease.

```bash
sudo sgdisk --backup=table /dev/mapper/data-ssd-349eadd6-1037-4c87-8797-ecb929d03744

for ssd in $(ls /dev/mapper/data-ssd-*); do \
    sudo sgdisk --load-backup=table $ssd; sudo partprobe $ssd; done;
```

Running a `lsblk` now will show the SSD partitions.

```bash
$ lsblk -o name,size,rota,type

NAME                                                  SIZE ROTA TYPE
sdb                                                 953.9G    0 disk
└─data-ssd-1bca6bcf-9b7b-435a-8867-24e71d330b73     953.9G    0 crypt
  ├─data-ssd-1bca6bcf-9b7b-435a-8867-24e71d330b73p1   450G    0 part
  └─data-ssd-1bca6bcf-9b7b-435a-8867-24e71d330b73p2 503.9G    0 part
sdc                                                 953.9G    0 disk
└─data-ssd-66bc8083-375b-4dd1-bf5f-84f2f4694050     953.9G    0 crypt
  ├─data-ssd-66bc8083-375b-4dd1-bf5f-84f2f4694050p1   450G    0 part
  └─data-ssd-66bc8083-375b-4dd1-bf5f-84f2f4694050p2 503.9G    0 part
sdd                                                 953.9G    0 disk
└─data-ssd-687c92a4-05b9-46d1-b2ff-5a4a2c9ce821     953.9G    0 crypt
  ├─data-ssd-687c92a4-05b9-46d1-b2ff-5a4a2c9ce821p1   450G    0 part
  └─data-ssd-687c92a4-05b9-46d1-b2ff-5a4a2c9ce821p2 503.9G    0 part
sde                                                 953.9G    0 disk
└─data-ssd-c4405952-7444-48d0-8f96-8742b51df431     953.9G    0 crypt
  ├─data-ssd-c4405952-7444-48d0-8f96-8742b51df431p1   450G    0 part
  └─data-ssd-c4405952-7444-48d0-8f96-8742b51df431p2 503.9G    0 part
```

# Setup bcache, btrfs and RAID
I'm going to use BTRFS as my filesystem and, as a result, will skip the traditional use of `mdadm` for raid where possible. Instead I'll leverage the native filesystem capabilties.

In addition, for a speed boost, I will use `bcache` as a mechanism for having the 500GB SSD partitions sit in front of the spinning rust. As bcache complains about existing superblocks though from the btrfs filesystem on the SSDs we'll need to use mdadm there.

## SSDs
```bash
# Storage: Use raid10 for both data and metadata
sudo mkfs.btrfs -L storage -d raid10 -m raid10 $(ls /dev/mapper/data-ssd-*p1)

# Cache: Stripe the data without mirroring, metadata is mirrored
# sudo mkfs.btrfs -l cache -d raid0 -m raid1 $(ls /dev/mapper/data-ssd-*p2)
sudo mdadm --create --verbose /dev/md0 --level=0 --raid-devices=4 $(ls /dev/mapper/data-ssd-*p2)
```

## BCache
```bash
# Setup the caching
# sudo make-bcache -C $(ls /dev/mapper/data-ssd-*p2) -B $(ls /dev/mapper/data-hdd*)
sudo make-bcache --wipe-bcache --writeback -C /dev/md0 -B $(ls /dev/mapper/data-hdd*)
```

You now have the following by way of lsblk

```bash
$ lsblk

NAME                                                MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda                                                   8:0    0  10.9T  0 disk  
└─data-hdd-1d8f9e34-bfd5-488c-b32a-f1d9de01435b     253:2    0  10.9T  0 crypt 
  └─bcache1                                         252:128  0  10.9T  0 disk  
sdb                                                   8:16   0 953.9G  0 disk  
└─data-ssd-349eadd6-1037-4c87-8797-ecb929d03744     253:3    0 953.9G  0 crypt 
  ├─data-ssd-349eadd6-1037-4c87-8797-ecb929d03744p1 253:11   0   450G  0 part  
  └─data-ssd-349eadd6-1037-4c87-8797-ecb929d03744p2 253:12   0 503.9G  0 part  
    └─md0                                             9:0    0     2T  0 raid0 
      ├─bcache0                                     252:0    0  12.8T  0 disk  
      ├─bcache1                                     252:128  0  10.9T  0 disk  
      └─bcache2                                     252:256  0  12.8T  0 disk  
sdc                                                   8:32   0 953.9G  0 disk  
└─data-ssd-efc57689-fef6-405d-87c3-da30c428bd85     253:8    0 953.9G  0 crypt 
  ├─data-ssd-efc57689-fef6-405d-87c3-da30c428bd85p1 253:13   0   450G  0 part  
  └─data-ssd-efc57689-fef6-405d-87c3-da30c428bd85p2 253:14   0 503.9G  0 part  
    └─md0                                             9:0    0     2T  0 raid0 
      ├─bcache0                                     252:0    0  12.8T  0 disk  
      ├─bcache1                                     252:128  0  10.9T  0 disk  
      └─bcache2                                     252:256  0  12.8T  0 disk  
sdd                                                   8:48   0 953.9G  0 disk  
└─data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894     253:6    0 953.9G  0 crypt 
  ├─data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894p1 253:9    0   450G  0 part  
  └─data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894p2 253:10   0 503.9G  0 part  
    └─md0                                             9:0    0     2T  0 raid0 
      ├─bcache0                                     252:0    0  12.8T  0 disk  
      ├─bcache1                                     252:128  0  10.9T  0 disk  
      └─bcache2                                     252:256  0  12.8T  0 disk  
sde                                                   8:64   0 953.9G  0 disk  
└─data-ssd-ff14cc0d-5d89-485f-bcd2-282be6dca2b4     253:5    0 953.9G  0 crypt 
  ├─data-ssd-ff14cc0d-5d89-485f-bcd2-282be6dca2b4p1 253:15   0   450G  0 part  
  └─data-ssd-ff14cc0d-5d89-485f-bcd2-282be6dca2b4p2 253:16   0 503.9G  0 part  
    └─md0                                             9:0    0     2T  0 raid0 
      ├─bcache0                                     252:0    0  12.8T  0 disk  
      ├─bcache1                                     252:128  0  10.9T  0 disk  
      └─bcache2                                     252:256  0  12.8T  0 disk  
sdf                                                   8:80   0  12.8T  0 disk  
└─data-hdd-0662c6e8-214f-4071-8a64-52733d690a4a     253:4    0  12.8T  0 crypt 
  └─bcache0                                         252:0    0  12.8T  0 disk  
sdg                                                   8:96   0  12.8T  0 disk  
└─data-hdd-c101908c-04ab-477e-b748-8b1cfc6b8d0c     253:7    0  12.8T  0 crypt 
  └─bcache2                                         252:256  0  12.8T  0 disk  
```

## HDDs
```bash
# Media: Create a filesystem across the backing drives (metadata mirrored, linear data allocation)
# sudo mkfs.btrfs -d single -m raid1 $(ls /dev/mapper/data-hdd*)
sudo mkfs.btrfs -f -L media -d single -m raid1 $(ls /dev/bcache/by-uuid/*)
```

# Mount the filesystems
First, get the PARTUUID of the `storage` filesystem (it will be the same for all SSDs). We'll add this to fstab.

```bash
storage=$(sudo blkid -o value -s PARTUUID /dev/mapper/data-ssd-*p1 | head -n 1)
echo "PARTUUID=${storage}     /mnt/local/storage    btrfs    defaults    0    1" | sudo tee -a /etc/fstab
```

Now mount the backing filesystem

```bash
echo "/dev/bcache0     /mnt/local/media    btrfs    defaults    0    1" | sudo tee -a /etc/fstab
```

Make sure that the mdadm array is reconstructed on boot.

```bash
sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf
# update the initramfs, or initial RAM file system, so that the array will be available during the early boot process
sudo update-initramfs -u
```

And now add a hack into the `crontab` so that the kernel picks up the partitions in the encrypted disk when we reboot (after crypttab has decrypted into /dev/mapper).

```bash
sudo -s  # run this as root
crontab -e

...
@reboot sudo partprobe $(ls /dev/mapper/data-ssd-*)
```

# Create union structure
This setup has borrowed heavily from the [cloudbox]() project. Essentially I'm aiming to have files split and replicated across `local` (direct i/o), `cluster` (NFS shares) and `remote` (google drive mounted via rclone).

With a consistent folder structure in each I can then use [mergerfs]() to create a union filesystem which can be presented transparently to applications (who need have no understanding of where the files are being saved to or pulled from).

```bash
sudo mkdir -p /mnt/{local,cluster,remote,unionfs}/{media,downloads,transcodes,storage}
```

We can view this now with `sudo apt-get install -y tree`.

```
$ tree /mnt/

/mnt/
├── cluster
│   ├── downloads
│   ├── media
│   ├── storage
│   └── transcodes
├── key
├── local
│   ├── downloads
│   ├── media
│   ├── storage
│   └── transcodes
├── remote
│   ├── downloads
│   ├── media
│   ├── storage
│   └── transcodes
└── unionfs
    ├── downloads
    ├── media
    ├── storage
    └── transcodes
```

The idea now is that we create an overlay filesystem with mergerfs which will present a unified view inside `/mnt/unionfs/` which has transparently concatenated the data from `local`, `cluster` and `remote` in a way in which files on the fastest/closest storage are preferred. Applications don't care but under the hood we can now shuffle data from local --> cluster --> remote (and back again) depending on how we want to lifecycle manage storage.

## Setup mergerfs service
Install the binaries

```bash
sudo apt-get install -y mergerfs
```

Now configure a systemd service to mount

```bash
sudo tee /etc/systemd/system/mergerfs.service << EOF

EOF
```

## Check
After a reboot we can now check this with a `df -h` to see that our filesystems are mounted correctly.