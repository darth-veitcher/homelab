As previously highlighted the overhead associated with maintaining a full Rook-Ceph cluster just wasn't worth it when I reached +8 physical disks. Each OSD container requests 1 vCPU and 1GB RAM which quickly started to cause issues on hardware I was attempting to run as a hyperconverged node.

As a result I've looked to replicate lifecycle management with the following broad goals whilst still maintaining the ability to pass persistent volumes to Kubernetes pods via NFS:

* Cache using SSDs for writes and reads
* Near-line storage in HDDs which can be swapped in and out on demand
* Cold storage on Google Drive

With the encrypted disks already setup from the hosts section previously I'm going to modify the existing setup (initially for Rook-Ceph) slightly.

* Use mergerfs to create a single filesystem from the above devices.
* Devices will be split as follows
  * `SSDs` - split into two partitions. One for media (cache) and one for data, docs and PVs (will become a RAID10 across 4 devices).
  * `HDDs` - Can partition the disks for `single` with btrfs.
* SnapRAID will be applied with an encrypted rclone mount used as the parity (no idea if this will work...)

# Partition SSDs
As I have 4 x 1TB SSDs I'm going to partition these to allow me to essentially replicate the `pool` type configuration I would have been able to implement with Ceph.

* `storage`: a RAID10 configuration which will be used to house Kubernetes PVs for containers, documents etc.
* `cache`: a RAID0 configuration which I'll use as a cache tier in front of the spinning rust.

```bash
sudo apt-get install -y gdisk

# Write the first partition table for SSD
# 450GB for RAID10
# Remainder for RAID0
cat <<EOF | sudo gdisk /dev/mapper/data-ssd-349eadd6-1037-4c87-8797-ecb929d03744
n
1
2048
450G
8300
n
2


8300
w
Y
EOF
```

We should now have a partition table on our first SSD as per the below.

```bash
$ sudo gdisk /dev/mapper/data-ssd-1bca6bcf-9b7b-435a-8867-24e71d330b73

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048       943718400   450.0 GiB   8300  Linux filesystem
   2       943720448      2000405134   503.9 GiB   8300  Linux filesystem
```

This can be copied to the other SSDs for ease.

```bash
sudo sgdisk --backup=table /dev/mapper/data-ssd-349eadd6-1037-4c87-8797-ecb929d03744

for ssd in $(ls /dev/mapper/data-ssd-*); do \
    sudo sgdisk --load-backup=table $ssd; sudo partprobe $ssd; done;
```

Running a `lsblk` now will show the SSD partitions.

```bash
$ lsblk -o name,size,rota,type

NAME                                                  SIZE ROTA TYPE
sdb                                                 953.9G    0 disk
└─data-ssd-1bca6bcf-9b7b-435a-8867-24e71d330b73     953.9G    0 crypt
  ├─data-ssd-1bca6bcf-9b7b-435a-8867-24e71d330b73p1   450G    0 part
  └─data-ssd-1bca6bcf-9b7b-435a-8867-24e71d330b73p2 503.9G    0 part
sdc                                                 953.9G    0 disk
└─data-ssd-66bc8083-375b-4dd1-bf5f-84f2f4694050     953.9G    0 crypt
  ├─data-ssd-66bc8083-375b-4dd1-bf5f-84f2f4694050p1   450G    0 part
  └─data-ssd-66bc8083-375b-4dd1-bf5f-84f2f4694050p2 503.9G    0 part
sdd                                                 953.9G    0 disk
└─data-ssd-687c92a4-05b9-46d1-b2ff-5a4a2c9ce821     953.9G    0 crypt
  ├─data-ssd-687c92a4-05b9-46d1-b2ff-5a4a2c9ce821p1   450G    0 part
  └─data-ssd-687c92a4-05b9-46d1-b2ff-5a4a2c9ce821p2 503.9G    0 part
sde                                                 953.9G    0 disk
└─data-ssd-c4405952-7444-48d0-8f96-8742b51df431     953.9G    0 crypt
  ├─data-ssd-c4405952-7444-48d0-8f96-8742b51df431p1   450G    0 part
  └─data-ssd-c4405952-7444-48d0-8f96-8742b51df431p2 503.9G    0 part
```

# Setup bcache, btrfs and RAID
I'm going to use BTRFS as my filesystem and, as a result, will skip the traditional use of `mdadm` for raid where possible. Instead I'll leverage the native filesystem capabilties.

In addition, for a speed boost, I will use `bcache` as a mechanism for having the 500GB SSD partitions sit in front of the spinning rust. As bcache complains about existing superblocks though from the btrfs filesystem on the SSDs we'll need to use mdadm there.

## SSDs
```bash
# Storage: Use raid10 for both data and metadata
sudo mkfs.btrfs -L storage -d raid10 -m raid10 $(ls /dev/mapper/data-ssd-*p1)

# Cache: Stripe the data without mirroring, metadata is mirrored
# sudo mkfs.btrfs -l cache -d raid0 -m raid1 $(ls /dev/mapper/data-ssd-*p2)
sudo mdadm --create --verbose /dev/md0 --level=0 --raid-devices=4 $(ls /dev/mapper/data-ssd-*p2)
```

## BCache
```bash
# Setup the caching
# sudo make-bcache -C $(ls /dev/mapper/data-ssd-*p2) -B $(ls /dev/mapper/data-hdd*)
sudo make-bcache --wipe-bcache --writeback -C /dev/md0 -B $(ls /dev/mapper/data-hdd*)
```

You now have the following by way of lsblk

```bash
$ lsblk

NAME                                                MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda                                                   8:0    0  10.9T  0 disk  
└─data-hdd-1d8f9e34-bfd5-488c-b32a-f1d9de01435b     253:2    0  10.9T  0 crypt 
  └─bcache1                                         252:128  0  10.9T  0 disk  
sdb                                                   8:16   0 953.9G  0 disk  
└─data-ssd-349eadd6-1037-4c87-8797-ecb929d03744     253:3    0 953.9G  0 crypt 
  ├─data-ssd-349eadd6-1037-4c87-8797-ecb929d03744p1 253:11   0   450G  0 part  
  └─data-ssd-349eadd6-1037-4c87-8797-ecb929d03744p2 253:12   0 503.9G  0 part  
    └─md0                                             9:0    0     2T  0 raid0 
      ├─bcache0                                     252:0    0  12.8T  0 disk  
      ├─bcache1                                     252:128  0  10.9T  0 disk  
      └─bcache2                                     252:256  0  12.8T  0 disk  
sdc                                                   8:32   0 953.9G  0 disk  
└─data-ssd-efc57689-fef6-405d-87c3-da30c428bd85     253:8    0 953.9G  0 crypt 
  ├─data-ssd-efc57689-fef6-405d-87c3-da30c428bd85p1 253:13   0   450G  0 part  
  └─data-ssd-efc57689-fef6-405d-87c3-da30c428bd85p2 253:14   0 503.9G  0 part  
    └─md0                                             9:0    0     2T  0 raid0 
      ├─bcache0                                     252:0    0  12.8T  0 disk  
      ├─bcache1                                     252:128  0  10.9T  0 disk  
      └─bcache2                                     252:256  0  12.8T  0 disk  
sdd                                                   8:48   0 953.9G  0 disk  
└─data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894     253:6    0 953.9G  0 crypt 
  ├─data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894p1 253:9    0   450G  0 part  
  └─data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894p2 253:10   0 503.9G  0 part  
    └─md0                                             9:0    0     2T  0 raid0 
      ├─bcache0                                     252:0    0  12.8T  0 disk  
      ├─bcache1                                     252:128  0  10.9T  0 disk  
      └─bcache2                                     252:256  0  12.8T  0 disk  
sde                                                   8:64   0 953.9G  0 disk  
└─data-ssd-ff14cc0d-5d89-485f-bcd2-282be6dca2b4     253:5    0 953.9G  0 crypt 
  ├─data-ssd-ff14cc0d-5d89-485f-bcd2-282be6dca2b4p1 253:15   0   450G  0 part  
  └─data-ssd-ff14cc0d-5d89-485f-bcd2-282be6dca2b4p2 253:16   0 503.9G  0 part  
    └─md0                                             9:0    0     2T  0 raid0 
      ├─bcache0                                     252:0    0  12.8T  0 disk  
      ├─bcache1                                     252:128  0  10.9T  0 disk  
      └─bcache2                                     252:256  0  12.8T  0 disk  
sdf                                                   8:80   0  12.8T  0 disk  
└─data-hdd-0662c6e8-214f-4071-8a64-52733d690a4a     253:4    0  12.8T  0 crypt 
  └─bcache0                                         252:0    0  12.8T  0 disk  
sdg                                                   8:96   0  12.8T  0 disk  
└─data-hdd-c101908c-04ab-477e-b748-8b1cfc6b8d0c     253:7    0  12.8T  0 crypt 
  └─bcache2                                         252:256  0  12.8T  0 disk  
```

## HDDs
```bash
# Media: Create a filesystem across the backing drives (metadata mirrored, linear data allocation)
# sudo mkfs.btrfs -d single -m raid1 $(ls /dev/mapper/data-hdd*)
sudo mkfs.btrfs -f -L media -d single -m raid1 $(ls /dev/bcache/by-uuid/*)
```

# Mount the filesystems
First, get the PARTUUID of the `storage` filesystem (it will be the same for all SSDs). We'll add this to fstab.

```bash
storage=$(sudo blkid -o value -s PARTUUID /dev/mapper/data-ssd-*p1 | head -n 1)
echo "PARTUUID=${storage}     /mnt/local/storage    btrfs    defaults    0    1" | sudo tee -a /etc/fstab
```

Now mount the backing filesystem

```bash
echo "/dev/bcache0     /mnt/local/media    btrfs    defaults    0    1" | sudo tee -a /etc/fstab
```

Make sure that the mdadm array is reconstructed on boot.

```bash
sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf
# update the initramfs, or initial RAM file system, so that the array will be available during the early boot process
sudo update-initramfs -u
```

And now add a hack in as systemd service so that the kernel picks up the partitions in the encrypted disk when we reboot (after crypttab has decrypted into /dev/mapper). Otherwise your fstab entries will fail.

```bash
sudo /etc/systemd/system/partprobe.service << EOF
# /etc/systemd/system/partprobe.service
[Unit]
Description=Fulldisk Encryption Partition Discovery
Requires=dm-event.socket
After=cryptsetup.target
Before=local-fs.target unmount.target

[Service]
Type=oneshot
ExecStart=/bin/bash -c "partprobe $(ls /dev/mapper/data-ssd-*)"

[Install]
WantedBy=multi-user.target mergerfs.service
EOF
```

Now enable the service

```bash
sudo systemctl daemon-reload
sudo systemctl enable partprobe.service
```

We should now have our hidden partitions visible to the kernel whenever we restart the system.

# Create union structure
This setup has borrowed heavily from the [cloudbox]() project. Essentially I'm aiming to have files split and replicated across `local` (direct i/o), `cluster` (NFS shares) and `remote` (google drive mounted via rclone).

With a consistent folder structure in each I can then use [mergerfs]() to create a union filesystem which can be presented transparently to applications (who need have no understanding of where the files are being saved to or pulled from).

```bash
sudo mkdir -p /mnt/{local,cluster,remote,unionfs}/{media,downloads,transcodes,storage}
sudo chown -R $USER:$USER /mnt/{local,cluster,remote,unionfs}
```

We can view this now with `sudo apt-get install -y tree`.

```
$ tree /mnt/

/mnt/
├── cluster
│   ├── downloads
│   ├── media
│   ├── storage
│   └── transcodes
├── key
├── local
│   ├── downloads
│   ├── media
│   ├── storage
│   └── transcodes
├── remote
│   ├── downloads
│   ├── media
│   ├── storage
│   └── transcodes
└── unionfs
```

The idea now is that we create an overlay filesystem with mergerfs which will present a unified view inside `/mnt/unionfs/` which has transparently concatenated the data from `local`, `cluster` and `remote` in a way in which files on the fastest/closest storage are preferred. Applications don't care but under the hood we can now shuffle data from local --> cluster --> remote (and back again) depending on how we want to lifecycle manage storage.

First though let's check that our local filesystems all mount correctly with the `partprobe` service and fstab entries.

## Check
After a reboot we can check this with a `df -h`.

```bash
$ df -h

Filesystem                                                   Size  Used Avail Use% Mounted on
udev                                                          16G     0   16G   0% /dev
tmpfs                                                        3.2G  1.5M  3.2G   1% /run
/dev/mapper/clarke--vg-root                                  116G  2.3G  108G   3% /
tmpfs                                                         16G     0   16G   0% /dev/shm
tmpfs                                                        5.0M     0  5.0M   0% /run/lock
tmpfs                                                         16G     0   16G   0% /sys/fs/cgroup
/dev/nvme0n1p1                                               511M  6.1M  505M   2% /boot/efi
/dev/sdh2                                                     30G  1.9M   30G   1% /mnt/key
/dev/mapper/data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894p1  900G   18M  898G   1% /mnt/local/storage
/dev/bcache0                                                  37T   17M   37T   1% /mnt/local/media
tmpfs                                                        3.2G     0  3.2G   0% /run/user/1000
```

## Setup mergerfs service
A good reference for what we're trying to achieve is in the [README](https://github.com/trapexit/mergerfs/blob/master/README.md#tiered-caching) for mergerfs.

Install the binaries

```bash
MERGERFS_VERSION=2.28.3
mkdir -p /opt/mergerfs && cd /opt/mergerfs; \
wget "https://github.com/trapexit/mergerfs/releases/download/${MERGERFS_VERSION}/mergerfs_${MERGERFS_VERSION}.ubuntu-xenial_amd64.deb"; \
sudo dpkg -i "mergerfs_${MERGERFS_VERSION}.ubuntu-xenial_amd64.deb"
```

Now configure a systemd service to mount our two key filesystems

```bash
sudo tee /etc/systemd/system/mergerfs-storage.service << EOF
# /etc/systemd/system/mergerfs-storage.service

[Unit]
Description=MergerFS Mount
After=network-online.target

[Service]
Type=forking
GuessMainPID=no
ExecStart=/usr/bin/mergerfs \
  -o category.create=ff,async_read=false,cache.files=partial \
  -o dropcacheonclose=true,use_ino,minfreespace=0 \
  -o xattr=nosys,statfs_ignore=ro,allow_other,umask=002,noatime \
  /mnt/local/storage=RW:/mnt/cluster/storage=RW:/mnt/remote/storage=NC /mnt/unionfs/storage
ExecStop=/bin/fusermount -u /mnt/unionfs/storage

[Install]
WantedBy=default.target
EOF
```

```bash
sudo tee /etc/systemd/system/mergerfs-media.service << EOF
# /etc/systemd/system/mergerfs-media.service

[Unit]
Description=MergerFS Mount
After=network-online.target

[Service]
Type=forking
GuessMainPID=no
ExecStart=/usr/bin/mergerfs \
  -o category.create=ff,async_read=false,cache.files=partial \
  -o dropcacheonclose=true,use_ino,minfreespace=0 \
  -o xattr=nosys,statfs_ignore=ro,allow_other,umask=002,noatime \
  /mnt/local/media=RW:/mnt/cluster/media=RW:/mnt/remote/media=NC /mnt/unionfs/media
ExecStop=/bin/fusermount -u /mnt/unionfs/media

[Install]
WantedBy=default.target
EOF
```

Enable it with `sudo systemctl daemon-reload && sudo systemctl enable --now mergerfs-storage.service && sudo systemctl enable --now mergerfs-media.service`. A restart might be required.

## Check
After a reboot we can now check this with a `df -h` to see that our filesystems are mounted correctly.

```bash
$ df -h

Filesystem                                                   Size  Used Avail Use% Mounted on
udev                                                          16G     0   16G   0% /dev
tmpfs                                                        3.2G  1.5M  3.2G   1% /run
/dev/mapper/clarke--vg-root                                  116G  2.3G  108G   3% /
tmpfs                                                         16G     0   16G   0% /dev/shm
tmpfs                                                        5.0M     0  5.0M   0% /run/lock
tmpfs                                                         16G     0   16G   0% /sys/fs/cgroup
/dev/nvme0n1p1                                               511M  6.1M  505M   2% /boot/efi
/dev/sdh2                                                     30G  1.9M   30G   1% /mnt/key
/dev/mapper/data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894p1  900G   18M  898G   1% /mnt/local/storage
/dev/bcache0                                                  37T   17M   37T   1% /mnt/local/media
tmpfs                                                        3.2G     0  3.2G   0% /run/user/1000
local/storage:cluster/storage:remote/storage                1016G  2.3G 1006G   1% /mnt/unionfs/storage
local/media:cluster/media:remote/media                        37T  2.3G   37T   1% /mnt/unionfs/media
```

# Rclone
We will now mount our rclone remotes into the filesystem.

```bash
curl https://rclone.org/install.sh | sudo bash
sudo chown -R $USER:$USER /home/$USER/.config/rclone

tee /home/$USER/.config/rclone/rclone.conf << EOF
# Put your existing rclone config here
EOF
```

Once rclone is installed and your config is in place we can add some service files to mount these remotes into our union filesystem setup.

Let's ensure that `allow_other` is allowed in fuse first.

```bash
sudo sed -i.bak 's/#user_allow_other/user_allow_other/' /etc/fuse.conf 
```

```bash
sudo tee /etc/systemd/system/rclone_vfs-media.service << EOF
[Unit]
Description=Rclone VFS Mount (Media)
After=network-online.target

[Service]
User=${USER}
Group=${USER}
Type=notify
ExecStartPre=/bin/sleep 10
ExecStart=/usr/bin/rclone mount \
  --user-agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36' \
  --config=/home/${USER}/.config/rclone/rclone.conf \
  --allow-other \
  --allow-non-empty \
  --rc \
  --rc-addr=localhost:5572 \
  --fast-list \
  --drive-skip-gdocs \
  --vfs-read-chunk-size=64M \
  --vfs-read-chunk-size-limit=2048M \
  --buffer-size=64M \
  --poll-interval=1m \
  --dir-cache-time=168h \
  --timeout=10m \
  --drive-chunk-size=64M \
  --umask=002 \
  --syslog \
  -v \
  media:Media /mnt/remote/media
ExecStop=/bin/fusermount -uz /mnt/remote/media
Restart=on-abort
RestartSec=5
StartLimitInterval=60s
StartLimitBurst=3

[Install]
WantedBy=default.target
EOF
```

```bash
sudo tee /etc/systemd/system/rclone_vfs-downloads.service << EOF
[Unit]
Description=Rclone VFS Mount (Downloads)
After=network-online.target

[Service]
User=${USER}
Group=${USER}
Type=notify
ExecStartPre=/bin/sleep 10
ExecStart=/usr/bin/rclone mount \
  --user-agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36' \
  --config=/home/${USER}/.config/rclone/rclone.conf \
  --allow-other \
  --allow-non-empty \
  --rc \
  --rc-addr=localhost:5573 \
  --fast-list \
  --drive-skip-gdocs \
  --vfs-read-chunk-size=64M \
  --vfs-read-chunk-size-limit=2048M \
  --buffer-size=64M \
  --poll-interval=1m \
  --dir-cache-time=168h \
  --timeout=10m \
  --drive-chunk-size=64M \
  --umask=002 \
  --syslog \
  -v \
  downloads: /mnt/remote/downloads
ExecStop=/bin/fusermount -uz /mnt/remote/downloads
Restart=on-abort
RestartSec=5
StartLimitInterval=60s
StartLimitBurst=3

[Install]
WantedBy=default.target
EOF
```

With these in place let's enable `sudo systemctl daemon-reload && sudo systemctl enable --now rclone_vfs-media.service && sudo systemctl enable --now rclone_vfs-downloads.service`.

## Check
After a reboot we can now check this with a `df -h` to see that our filesystems are mounted correctly.

```bash
$ df -h

Filesystem                                                   Size  Used Avail Use% Mounted on
udev                                                          16G     0   16G   0% /dev
tmpfs                                                        3.2G  1.5M  3.2G   1% /run
/dev/mapper/clarke--vg-root                                  116G  2.3G  108G   3% /
tmpfs                                                         16G     0   16G   0% /dev/shm
tmpfs                                                        5.0M     0  5.0M   0% /run/lock
tmpfs                                                         16G     0   16G   0% /sys/fs/cgroup
/dev/nvme0n1p1                                               511M  6.1M  505M   2% /boot/efi
/dev/sdh2                                                     30G  1.9M   30G   1% /mnt/key
/dev/mapper/data-ssd-0516f5c7-749a-4fae-9475-5fc6a2256894p1  900G   18M  898G   1% /mnt/local/storage
/dev/bcache0                                                  37T   17M   37T   1% /mnt/local/media
local/media:cluster/media:remote/media                       1.1P   21T   37T  37% /mnt/unionfs/media
local/storage:cluster/storage:remote/storage                1016G  2.3G 1006G   1% /mnt/unionfs/storage
downloads:                                                   1.0P   21T  1.0P   2% /mnt/remote/downloads
media:Media                                                  1.0P   21T  1.0P   2% /mnt/remote/media
tmpfs                                                        3.2G     0  3.2G   0% /run/user/1000
```