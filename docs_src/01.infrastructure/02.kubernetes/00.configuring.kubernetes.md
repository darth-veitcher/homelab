In [part 1](001.configuring.physical.nodes.md) we setup and configured the physical nodes we are going to use for our kubernetes cluster, ensuring they could communicate between each other using a secure VPN.

We'll now:

* install kubernetes
* implement a CNI (network solution for pod communication)
* add a load balancer so that services can obtain `external` IP addresses
* give your user them the ability to run commands on the cluster

# Install Kubernetes
## Docker
As Kubernetes is an orchestration layer we will need to install a container runtime eninge. The below commands will install a compatible version of Docker.

???+ info "Kubernetes and Docker versions"
    It's worth being aware that Kubernetes only supports specific versions of docker and, as a result, you should check for compatability in their changelog. At time of writing the latest stable version of Kubernetes was 1.16. The [CHANGELOG-1.16](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#dependencies) shows that they have `validated` the following docker versions:

    >* The list of validated docker versions remains unchanged.
    >* The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09

```bash
export K8S_DOCKER_VERSION=18.09
sudo apt-get update; \
sudo apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common; \
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -; \
sudo add-apt-repository \
   "deb https://download.docker.com/linux/$(. /etc/os-release; echo "$ID") \
   $(lsb_release -cs) \
   stable"; \
sudo apt-get update && sudo apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep ${K8S_DOCKER_VERSION} | head -1 | awk '{print $3}')
```

??? info "Network status"
    With docker installed the network configuration and topology now looks like this.

    ![network](../../00.architecture/network.docker.drawio.svg)

    ```bash
    $ networkctl list

    IDX LINK             TYPE               OPERATIONAL SETUP     
      1 lo               loopback           carrier     unmanaged 
      2 eno1             ether              carrier     configured
      3 eno2             ether              carrier     configured
      4 eno3             ether              carrier     configured
      5 eno4             ether              carrier     configured
      6 bond0            ether              routable    configured
      7 docker0          ether              no-carrier  unmanaged 

    7 links listed.
    ```

    ```bash
    $ route -n

    Kernel IP routing table
    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
    0.0.0.0         192.168.0.1     0.0.0.0         UG    100    0        0 bond0
    172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
    192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 bond0
    192.168.0.1     0.0.0.0         255.255.255.255 UH    100    0        0 bond0
    ```

## Kubernetes
With docker now setup as a runtime we'll install Kubernetes as well as [jq](https://stedolan.github.io/jq/) (which will come in handy later).

```bash
sudo apt-get update && sudo apt-get install -y apt-transport-https; \
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -; \
sudo tee /etc/apt/sources.list.d/kubernetes.list <<EOF
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update; \
sudo apt-get install -y kubelet kubeadm kubectl jq
```

### Pre-flight fixes
Before intialising the cluster we need to change the following:

* modify the `cgroup` driver from `cgroupfs` to `systemd`
* disable `swap`

```bash
# Setup daemon.
sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

sudo mkdir -p /etc/systemd/system/docker.service.d

# Restart docker.
sudo systemctl daemon-reload
sudo systemctl restart docker
```

The swap should be disabled both for the current session with `sudo swapoff -a` and then in `/etc/fstab` (just add a comment `#` at the start of the line) so that this persists across reboots.

### Initialise
Initialise the node

```bash
sudo kubeadm config images pull; \
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
```

??? tip "Listening on a specific address"
    If you wanted to you can set the api for kubernetes to listen only on a specific address. This is useful if, for instance, you have a VPN connection between nodes. An example of how to initialise this is below. In this instance we're getting the IPv4 address of the wireguard interface.

    ```bash
    # Bind to VPN address
    sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address $(ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}')
    ```

You'll see a load of scrolling log text followed by the following indicating success and giving some next step instructions.

??? "Success"

    ```bash
    Your Kubernetes control-plane has initialized successfully!

    To start using your cluster, you need to run the following as a regular user:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

    You should now deploy a pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/

    Then you can join any number of worker nodes by running the following on each as root:

    kubeadm join 192.168.0.101:6443 --token 64we2d.5jzsjzpa0ysqzagl \
        --discovery-token-ca-cert-hash sha256:9d1dda6163e0e539588e0209f06b37d209d230a373b0167ea5f881cd60537178
    ```

Give your user the ability to run `kubectl`.

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

To test this works we can run a couple of kubernetes commands.

```bash
$ kubectl get nodes

NAME          STATUS     ROLES    AGE   VERSION
banks.local   NotReady   master   15s   v1.17.0
```

The node will show as `NotReady` until we add a container network interface (CNI) in the next step.

# Remove the Control plane node isolation taint
The official [docs](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#control-plane-node-isolation) highlight:

>By default, your cluster will not schedule pods on the control-plane node for security reasons.

This will be a problem if we only have one node running (e.g. homelab development) so we will remove the taint that prevents this.

```bash
kubectl taint nodes --all node-role.kubernetes.io/master-
```

This will remove the node-role.kubernetes.io/master taint from any nodes that have it, including the control-plane node, meaning that the scheduler will then be able to schedule pods everywhere.

# Add node labels
For later on we will add a `topology.kubernetes.io/region` label to the node as this is used to indicate failure domains for DR and also assists with the setup of VPN links. For more informatio see [Running in multiple zones](https://kubernetes.io/docs/setup/best-practices/multiple-zones/) and [Well-Known Labels, Annotations and Taints](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone) in the official kubernetes docs.

```bash
kubectl label node banks.local topology.kubernetes.io/region=mancave
```

???+ danger "Wiping your Kubernetes installation"
    On the off-chance that you ever want to completely uninstall kubernetes and associated resources you can run the commands below to purge from the system.
    ```bash
    kubeadm reset; \
    sudo apt-get purge -y kubeadm kubectl kubelet kubernetes-cni kube*; \
    sudo apt-get autoremove -y; \
    rm -rf ~/.kube; \
    sudo rm -rf /etc/kubernetes; \
    sudo rm -rf /var/lib/etcd
    ```
    \* *off-chance being the polite way of saying "this happens quite a lot when learning"...*

???+ info "Network status"
    With kubernetes installed the network configuration and topology still look the same until we apply a CNI for the pods to communicate with each other.
