We're going to create a couple of storage pools across our Ceph disks in order to showcase how [cache-tiering](https://docs.ceph.com/docs/hammer/rados/operations/cache-tiering/) can be applied to speed up performance of read/write operations using SSDs whilst still persisting longer-term storage onto cheaper HDD media.

# Create Storage Pools and Classes
We'll create one pool to then specifically cache the `data` from the filesystem.

```yaml
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: hot-storage
  namespace: rook-ceph
spec:
  failureDomain: osd
  replicated:
    size: 2
  deviceClass: ssd
```

Now login to the `rook-toolbox` and enable `CephFS` for the pool and then set it as the cache tier (with a maximum sixe of 2TB).

```bash
ceph osd pool application enable hot-storage cephfs --yes-i-really-mean-it
ceph osd tier add myfs-ec-data0 hot-storage
ceph osd tier cache-mode hot-storage writeback
ceph osd tier set-overlay myfs-ec-data0 hot-storage

ceph osd pool set hot-storage hit_set_type bloom
ceph osd pool set hot-storage hit_set_count 1
ceph osd pool set hot-storage hit_set_period 36000  # cache for 10hrs
let max=(1024*1024*1024*1024 * 3)  # 1024^4 = 1TB so * 3 for 3TB cache size
ceph osd pool set hot-storage target_max_bytes $max
ceph osd pool set hot-storage cache_target_dirty_ratio 0.4
ceph osd pool set hot-storage cache_target_dirty_high_ratio 0.6
ceph osd pool set hot-storage cache_target_full_ratio 0.8
```

# Testing Performance (without cache)
Before applying the tier I was getting ~150MiB/s throughput from clients when rsyncing a file from an attached HDD through to the mounted filesystem

# Testing Performance (with cache)
After applying the cache I saw speeds of +300MiB/s when rsyncing across.