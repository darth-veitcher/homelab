We're going to create a couple of storage pools across our Ceph disks in order to showcase how [cache-tiering](https://docs.ceph.com/docs/hammer/rados/operations/cache-tiering/) can be applied to speed up performance of read/write operations using SSDs whilst still persisting longer-term storage onto cheaper HDD media.

# Create Storage Pools and Classes
We'll create two pools:

* `hot-storage`: running on SSDs with a replica of `1` only
* `ecpool`: running on HDDs with `erasureCode` applied (roughly equivalent to a RAID5 configuration)

```yaml
# file: ~/rook/storage-pools.yaml
# Create a pool with only 1 replica for the
# SSDs to use.
# Use just ssds for this.
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: hot-storage
  namespace: rook-ceph
spec:
  failureDomain: osd
  replicated:
    size: 1
  deviceClass: ssd
---
# Create a pool with erasure coding for the
# backing tier to use for persistence (media).
# Use just hdds for this.
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: ecpool
  namespace: rook-ceph
spec:
  failureDomain: osd
  erasureCoded:
    dataChunks: 2
    codingChunks: 1
  deviceClass: hdd
```

Apply with `kubectl apply -f ~/rook/storage-pools.yaml`

## Storage Class
We now need to create associated `StorageClass` objects that use these pools.

```yaml
# file: ~/rook/storageClasses.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block-ecpool
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: ecpool
  imageFormat: "2"
  imageFeatures: layering
  # The secrets contain Ceph admin credentials.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, csi-provisioner
  # will set default as `ext4`.
  csi.storage.k8s.io/fstype: xfs
# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block-hot-storage
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: host-storage
  imageFormat: "2"
  imageFeatures: layering
  # The secrets contain Ceph admin credentials.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, csi-provisioner
  # will set default as `ext4`.
  csi.storage.k8s.io/fstype: xfs
# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete
```

Apply with `kubectl apply -f ~/rook/storageClasses.yaml`

# Testing Performance (without cache)
Before we apply the tiering we should generate a baseline for the disk performance. We'll create a simple deployment that consumes a persistent volume.



# Apply cacheing

# Testing Performance (with cache)
Go into `ceph-tools` and find the available pools

```bash
$ ceph osd pool ls

cachepool
ecpool
```

Now place the `cachepool` as the tier of `ecpool` in `writeback` mode so that every write and read to the ecpool are actually using the cachepool and benefit from its flexibility and speed.

```
$ ceph osd tier add ecpool cachepool; \
  ceph osd tier cache-mode cachepool writeback; \
  ceph osd tier set-overlay ecpool cachepool

pool 'cachepool' is now (or already was) a tier of 'ecpool'
set cache-mode for pool 'cachepool' to writeback
overlay for 'ecpool' is now (or already was) 'cachepool'
```

Now we can create some workloads that consume this and check. Using an existing `rclone.conf` file we will create a task which copies from a configured remote to our new storage.

```yaml

```