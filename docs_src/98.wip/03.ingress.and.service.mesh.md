As described in the official kubernetes [docs](https://kubernetes.io/docs/concepts/services-networking/ingress/) an `Ingress` object manages external access to services in a cluster, typically HTTP. They can provide load balancing, SSL termination and name-based virtual hosting.

We're going to combine a couple of sections of the official docs to deploy the following:

* Istio ingress with LetsEncrypt via cert-manager ([docs](https://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/));
* Mutual TLS between pods ([docs](https://istio.io/docs/tasks/security/authentication/authn-policy/#globally-enabling-istio-mutual-tls))
* Istio's CNI, so that privileged injection for sidecars no longer needed, ([docs](https://istio.io/docs/setup/additional-setup/cni/))

# Install
```bash
mkdir -p ~/istio; \
cd ~/istio

curl -L https://git.io/getLatestIstio | sh -

export ISTIO_DIR=$(ls -d istio-*)

# enable autocompletion and add to path
tee >> ~/.bash_profile <<EOL

# add istio to path
export PATH=~/istio/$ISTIO_DIR/bin:\$PATH

# add istioctl autocompletion
test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash
EOL

source ~/.bash_profile
```

Now we'll install the new Istio CNI plugin as per the [docs](https://istio.io/docs/setup/additional-setup/cni/) with a couple of additional tweaks as outlined for [ingress](ttps://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/).

Istio comes with a number of [Installation Configuration Profiles](https://istio.io/docs/setup/additional-setup/config-profiles/) that we want to [customise](https://istio.io/docs/setup/install/istioctl/#customizing-the-configuration) slightly before deploying.

First generate a manifest with the settings we want. We're deploying the `default` profile and adding in the necessary additional components for `sds` and `cni`.
```bash
istioctl manifest generate \
  --set values.gateways.istio-ingressgateway.sds.enabled=true \
  --set values.global.k8sIngress.enabled=true \
  --set values.global.k8sIngress.enableHttps=true \
  --set values.global.k8sIngress.gatewayName=ingressgateway \
  # --set cni.enabled=true \
> ~/istio/generated-manifest.yaml
```
Now apply `istioctl manifest apply -f ~/istio/generated-manifest.yaml`

??? error "the required ClusterRole:istio-cni is not ready"
    Currently using the `--set cni-enabled=true` flag doesn't seem to work. When you run an `istioctl verify-installation -f ~/istio/generated-manifest.yaml` it will present you with an error of `the required ClusterRole:istio-cni is not ready`.

    To be investigated further...

Reviewing the list of services on the cluster we should now see an `ingress-gateway` created with an `EXTERNAL-IP` allocated by MetalLB in the `istio-system` namespace.

???+ info "Running Services"
    ```bash hl_lines="7"
    $ kubectl get svc -A

    NAMESPACE      NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                                           AGE
    default        kubernetes               ClusterIP      10.96.0.1       <none>          443/TCP                                           12h
    istio-system   istio-citadel            ClusterIP      10.96.218.252   <none>          8060/TCP,15014/TCP                                82s
    istio-system   istio-galley             ClusterIP      10.96.101.76    <none>          443/TCP,15014/TCP,9901/TCP,15019/TCP              82s
    istio-system   istio-ingressgateway     LoadBalancer   10.96.113.231   192.168.0.200   15020:30088/TCP,80:32168/TCP,443:31899/TCP,....   80s
    istio-system   istio-pilot              ClusterIP      10.96.232.65    <none>          15010/TCP,15011/TCP,8080/TCP,15014/TCP            81s
    istio-system   istio-policy             ClusterIP      10.96.42.154    <none>          9091/TCP,15004/TCP,15014/TCP                      80s
    istio-system   istio-sidecar-injector   ClusterIP      10.96.147.243   <none>          443/TCP                                           82s
    istio-system   istio-telemetry          ClusterIP      10.96.8.149     <none>          9091/TCP,15004/TCP,15014/TCP,42422/TCP            80s
    istio-system   prometheus               ClusterIP      10.96.144.9     <none>          9090/TCP                                          82s
    kube-system    kube-dns                 ClusterIP      10.96.0.10      <none>          53/UDP,53/TCP,9153/TCP                            12h
    ```

We'll now create a deployment and see how many pods are created.

```yaml
# file: ~/istio/whoami.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whoami-deployment
  labels:
    app: whoami
spec:
  replicas: 1
  selector:
    matchLabels:
      app: whoami
  template:
    metadata:
      labels:
        app: whoami
    spec:
      containers:
      - name: whoami
        image: containous/whoami
        ports:
        - containerPort: 80
```

Apply the manifest with `kubectl apply -f ~/istio/whoami.yaml` and check the deployment and the number of pods created as a result.

```bash
$ kubectl get deployment -o wide

NAME                READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES              SELECTOR
whoami-deployment   1/1     1            1           13s   whoami       containous/whoami   app=whoami

$ kubectl get pod -l app=whoami

NAME                                 READY   STATUS    RESTARTS   AGE
whoami-deployment-5b4bb9c787-9cwg8   1/1     Running   0          10s
```

There's `1/1` ready for the pods which means we only have the single container running and [sidecar injection](https://istio.io/docs/setup/additional-setup/sidecar-injection) is not yet configured. We'll enable it for the `default` namespace and then delete the pod (forcing it to recreate with a sidecar).

```bash
$ kubectl label namespace default istio-injection=enabled
$ kubectl delete pod -l app=whoami
$ kubectl get pod -l app=whoami

NAME                                 READY   STATUS    RESTARTS   AGE
whoami-deployment-5b4bb9c787-snm9h   2/2     Running   0          8s
```

The `2/2` shows that we now have an injected sidecar container. We can inspect some details to see more with `kubectl describe pod -l app=whoami`. You should see the injected istio-proxy container and corresponding volumes.

???+ info "Sidecar Proxy"
    
    ```bash
    kubectl describe pod -l app=whoami
    ```

    ```yaml hl_lines="17 37 38"
    ...
    Containers:
      whoami:
        Container ID:   docker://5bbf03517d378e3e615a32396239f9704092f22e81fedcd00907d38891baaf07
        Image:          containous/whoami
        Image ID:       docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b
        Port:           80/TCP
        Host Port:      0/TCP
        State:          Running
          Started:      Fri, 13 Dec 2019 06:47:57 +0000
        Ready:          True
        Restart Count:  0
        Environment:    <none>
        Mounts:
          /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro)
      istio-proxy:
        Container ID:  docker://3e93f0370eecf8a5686e90013e3681c44f3b152891561bde8c56c3defd240d4b
        Image:         docker.io/istio/proxyv2:1.4.2
        Image ID:      docker-pullable://istio/proxyv2@sha256:c98b4d277b724a2ad0c6bf22008bd02ddeb2525f19e35cdefe8a3181313716e7
        Port:          15090/TCP
        Host Port:     0/TCP
        ...
    ...
    Events:
      Type    Reason     Age   From                  Message
      ----    ------     ----  ----                  -------
      Normal  Scheduled  90s   default-scheduler     Successfully assigned default/whoami-deployment-5b4bb9c787-snm9h to banks.local
      Normal  Pulled     89s   kubelet, banks.local  Container image "docker.io/istio/proxyv2:1.4.2" already present on machine
      Normal  Created    89s   kubelet, banks.local  Created container istio-init
      Normal  Started    88s   kubelet, banks.local  Started container istio-init
      Normal  Pulling    87s   kubelet, banks.local  Pulling image "containous/whoami"
      Normal  Pulled     85s   kubelet, banks.local  Successfully pulled image "containous/whoami"
      Normal  Created    85s   kubelet, banks.local  Created container whoami
      Normal  Started    85s   kubelet, banks.local  Started container whoami
      Normal  Pulled     85s   kubelet, banks.local  Container image "docker.io/istio/proxyv2:1.4.2" already present on machine
      Normal  Created    84s   kubelet, banks.local  Created container istio-proxy
      Normal  Started    84s   kubelet, banks.local  Started container istio-proxy
    ```

We can now create a `Service` and `Ingress` for the deployment so that we can access it externally.

```yaml
# file: ~/istio/whoami-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: whoami
  labels:
    app: whoami
spec:
  ports:
  - protocol: TCP
    port: 80
    name: http
  selector:
    app: whoami
```

create it with `kubectl apply -f ~/istio/whoami-svc.yaml`

```yaml
# file: ~/istio/whoami-ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  labels:
    app: whoami
  annotations:
    kubernetes.io/ingress.class: istio
    # cert-manager.io/issuer: "letsencrypt"
    # external-dns.alpha.kubernetes.io/hostname: whoami.jamesveitch.dev
  name: whoami-ingress
spec:
  rules:
    - host: whoami.jamesveitch.dev
      http:
        paths:
          - path: /
            backend:
              serviceName: whoami
              servicePort: 80
```

# TLS via Cert-Manager
Setup a secret to hold our cloudflare api key (dns validation)

```bash
kubectl create secret generic cloudflare-apikey-secret --from-literal=apikey=MYKEYFROMCLOUDA
```