## Banks (compute node)
### Basic housekeeping
First of all install the latest Ubuntu 18.04 LTS release and copy across your ssh key and then follow some simple hardening steps `#TODO: link to ansible`.

We'll configure our hostname too in order to reflect whatever the `fqdn` of our server is going to be. In my case this will be `banks.local` as it will just be an internal address. This is used by kubernetes later on.

```bash
sudo hostnamectl set-hostname banks.local
```

### Setup Wireguard (VPN)
See [Wireguard vs OpenVPN on a local Gigabit Network](https://snikt.net/blog/2018/12/13/wireguard-vs-openvpn-on-a-local-gigabit-network/) for a performance comparison. I've gone with Wireguard over OpenVPN based on it being incorporated into the Linux Kernel and increased performance versus OpenVPN.

#### Install Wireguard
```bash
sudo add-apt-repository -y ppa:wireguard/wireguard
sudo apt-get install -y wireguard
sudo modprobe wireguard  # activate kernal module
```

???+ check "Check Kernel Module"
    To check if the module is loaded use `lsmod | grep wireguard`. You should see something like the below.

    ```bash
    root@banks:~# lsmod | grep wireguard
    wireguard             212992  0
    ip6_udp_tunnel         16384  1 wireguard
    udp_tunnel             16384  1 wireguard
    ```

#### Keys
You will need to generate a key-pair for every peer (device) that is connected, including things like mobile phones etc. The iOS WireGuard client allow you to generate the keys on the device itself (if you want).

```bash
# Generate public/private keypair
cd /etc/wireguard
umask 077
wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey
```

#### Configure
We need to create a network interface now for the wireguard VPN. Common convention is to use `wg0` as a name for this. In addition we also need to choose a `subnet` for the VPN addresses. As I've got a `192.168.0.1/24` configuration at home I'll use `10.10.0.1/24` for the VPN.

Note the highlighted IP address we assign to each node here. It will need to be incremented for each to provide a unique address.

```bash hl_lines="4 5"
# file: /etc/wireguard/wg0.conf
[Interface]
PrivateKey = {{PRIVATE_KEY}}
Address = 10.10.0.1/24
Address = fd86:ea04:1111::1/64
SaveConfig = true
PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o {{ETH0}} -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o {{ETH0}} -j MASQUERADE
PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o {{ETH0}} -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o {{ETH0}} -j MASQUERADE
ListenPort = 51820
```

Now to keepo things DRY we'll run the following to replace the placeholder text with the actual contents of our server's private key we generated earlier. 

```bash
sudo sed -i.bak 's/{{PRIVATE_KEY}}/'$(sudo cat /etc/wireguard/privatekey)'/' /etc/wireguard/wg0.conf
```

We also need to replace the `{{ETH0}}` placeholder with the name of our existing primary network interface. A quick one-liner for this is `ip -4 route | grep default | awk '{print $5}'` which, on my server, gives `bond0` as the answer (as I'm running LACP across multiple bonded physical interfaces).

```bash
sudo sed -i.bak 's/{{ETH0}}/'$(ip -4 route | grep default | awk '{print $5}')'/g'  /etc/wireguard/wg0.conf
```

Enable forwarding of packets in the host kernel.


```bash
sudo tee /etc/sysctl.conf << EOF
net.ipv4.ip_forward=1
net.ipv6.conf.all.forwarding=1
EOF
sudo sysctl -p
```

Finally we can start the `wg0` interface.

```bash
wg-quick up wg0
```

Hopefully you'll see something like the below output.

```bash
root@banks:/etc/wireguard# wg-quick up wg0
[#] ip link add wg0 type wireguard
[#] wg setconf wg0 /dev/fd/63
[#] ip -4 address add 10.10.0.1/24 dev wg0
[#] ip -6 address add fd86:ea04:1111::1/64 dev wg0
[#] ip link set mtu 1420 up dev wg0
[#] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o bond0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o bond0 -j MASQUERADE
```

??? check "Checking status"
    The check the status of wireguard run the `wg` command.

    ```bash
    root@banks:/etc/wireguard# wg
    interface: wg0
        public key: I6ZHsLe44SHNH44xE86AI0VEnm8CfzrQUrxSCJVjAEw=
        private key: (hidden)
        listening port: 51820
    ```

    In addition, we should now have an additional `route` appear for our VPN subnet.

    ```bash hl_lines="3"
    root@banks:/etc/wireguard# ip route
    default via 192.168.0.1 dev bond0 proto dhcp src 192.168.0.94 metric 100 
    10.10.0.0/24 dev wg0 proto kernel scope link src 10.10.0.1 
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
    ...
    ```

#### Systemd service
Assuming the above works we can now enable the `wg0` interface on boot.

```bash
sudo systemctl enable wg-quick@wg0.service
sudo systemctl daemon-reload
```

You can then manually start `sudo service wg-quick@wg0 start` and check status `service wg-quick@wg0 status` of the service.

### Enable Avahi (Discovery) on VPN and LAN
As per [Wikipedia](https://en.wikipedia.org/wiki/Avahi_%28software%29).

>Avahi is a free zero-configuration networking (zeroconf) implementation, including a system for multicast DNS/DNS-SD service discovery. It is licensed under the GNU Lesser General Public License (LGPL).
>
>Avahi is a system which enables programs to publish and discover services and hosts running on a local network. For example, a user can plug a computer into a network and have Avahi automatically advertise the network services running on its machine, facilitating user access to those services.

We will setup our nodes to publish themselves on both the LAN (so can be accessed via their hostnames) and VPN.

```bash
export DEBIAN_FRONTEND=noninteractive  # from docker
sudo apt-get update -y
sudo apt-get -qq install -y avahi-daemon avahi-utils
```

The main configuration is held in `/etc/avahi/avahi-daemon.conf`. We want to modify the following line (to limit which interfaces we advertise on).

This will be useful for when we want to only enable it on *internal* interfaces (e.g. our Cloud node shouldn't try and broadcast across the in)

```bash
#file: /etc/avahi/avahi-daemon.conf
allow-interfaces=bond0, wg0, docker0

# If we're using an internal domain then leave as `local` below, else change to tld
domain-name=local
```

Reload / restart the daemon with `sudo systemctl daemon-reload && sudo systemctl restart avahi-daemon.service`