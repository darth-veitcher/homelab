# Configuring Kubernetes
In [part 1](001.configuring.physical.nodes.md) we setup and configured the physical nodes we are going to use for our kubernetes cluster, ensuring they could communicate between each other using a secure VPN.

We'll now:

* install kubernetes
* implement a CNI (network solution for pod communication)
* add a user and give them the ability to run commands on the cluster

## Install Kubernetes
### Docker
As Kubernetes is an orchestration layer we will need to install a container runtime eninge. The below commands will install a compatible version of Docker.

???+ info "Kubernetes and Docker versions"
    It's worth being aware that Kubernetes only supports specific versions of docker and, as a result, you should check for compatability in their changelog. At time of writing the latest stable version of Kubernetes was 1.16. The [CHANGELOG-1.16](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#dependencies) shows that they have `validated` the following docker versions:

    >* The list of validated docker versions remains unchanged.
    >* The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09

```bash
export K8S_DOCKER_VERSION=18.09
sudo apt-get update
sudo apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository \
   "deb https://download.docker.com/linux/$(. /etc/os-release; echo "$ID") \
   $(lsb_release -cs) \
   stable"
sudo apt-get update && sudo apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep ${K8S_DOCKER_VERSION} | head -1 | awk '{print $3}')
```

### Kubernetes
With docker now setup as a runtime we'll install Kubernetes.

```bash
sudo apt-get update && sudo apt-get install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo tee /etc/apt/sources.list.d/kubernetes.list <<EOF
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
```

#### Pre-flight fixes
Before intialising the cluster we need to change the following:

* modify the `cgroup` driver from `cgroupfs` to `systemd`
* disable `swap`

```bash
# Setup daemon.
sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

sudo mkdir -p /etc/systemd/system/docker.service.d

# Restart docker.
sudo systemctl daemon-reload
sudo systemctl restart docker
```

The swap should be disabled both for the current session with `sudo swapoff -a` and then in `/etc/fstab` (just add a comment `#` at the start of the line) so that this persists across reboots.

#### Initialise
Initialise the node. We'll use our [VPN address](001.configuring.physical.nodes.md#setup-wireguard-vpn) here as the address to listen on.

```bash
# Bind to VPN address
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address $(ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}')
```

You'll see a load of scrolling log text followed by the following indicating success and giving some next step instructions.

??? "Success"

    ```bash
    Your Kubernetes control-plane has initialized successfully!

    To start using your cluster, you need to run the following as a regular user:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

    You should now deploy a pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/

    Then you can join any number of worker nodes by running the following on each as root:

    kubeadm join 10.10.0.1:6443 --token 64we2d.5jzsjzpa0ysqzagl \
        --discovery-token-ca-cert-hash sha256:9d1dda6163e0e539588e0209f06b37d209d230a373b0167ea5f881cd60537178
    ```

Give your user the ability to run both kubectl and sudo.

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

echo "$USER ALL=(ALL:ALL) NOPASSWD:ALL" | sudo tee -a /etc/sudoers > /dev/null
```

To test this works we can run a couple of kubernetes commands.

```bash
$ kubectl get nodes
NAME          STATUS     ROLES    AGE     VERSION
banks.local   NotReady   master   5m29s   v1.16.3
```

The node will show as `NotReady` until we complete the following below.

Navigating to the [addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/) link provided above will show that we've got some options available. First we will focus on the `Networking and Network Policy` where we will use Canal as a CNI (networking solution between pods). Canal unites Flannel and Calico, providing networking and network policy.

```bash
# Role-based access control (RBAC)
# Kubernetes API datastore with flannel networking:
# https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/integration#role-based-access-control-rbac

kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-flannel.yaml

# Installing Calico for policy and flannel for networking
# Installing with the Kubernetes API datastore (recommended)
# We can install directly as we're using the pod CIDR 10.244.0.0/16
# https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel

kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/canal.yaml
```

## Remove the Control plane node isolation taint
The official [docs](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#control-plane-node-isolation) highlight:

>By default, your cluster will not schedule pods on the control-plane node for security reasons.

This will be a problem if we only have one node running (e.g. homelab development) so we will remove the taint that prevents this.

```bash
kubectl taint nodes --all node-role.kubernetes.io/master-
```

This will remove the node-role.kubernetes.io/master taint from any nodes that have it, including the control-plane node, meaning that the scheduler will then be able to schedule pods everywhere.

```bash
$ kubectl get nodes
NAME          STATUS   ROLES    AGE     VERSION
banks.local   Ready    master   5m31s   v1.16.3
```

???+ danger "Wiping your Kubernetes installation"
    On the off-chance that you ever want to completely uninstall kubernetes and associated resources you can run the commands below to purge from the system.
    ```bash
    kubeadm reset; \
    sudo apt-get purge kubeadm kubectl kubelet kubernetes-cni kube*; \
    sudo apt-get autoremove; \
    rm -rf ~/.kube; \
    sudo rm -rf /etc/kubernetes; \
    sudo rm -rf /var/lib/etcd
    ```
    \* *off-chance being the polite way of saying "this happens quite a lot when learning"...*


??? abstract "Testing a Kubernetes application deployment"
    Whilst the below is optional it's highly recommended to quickly test that our initial Kubernetes installation has been successful.

    Get a list of available nodes

    ```bash
    adminlocal@banks:~$ kubectl get nodes
    NAME    STATUS   ROLES    AGE   VERSION
    banks   Ready    master   43m   v1.16.3
    ```

    Run a single pod deployment and expose it

    ```bash
    kubectl run helloworld --image=containous/whoami --port=80
    kubectl expose deployment helloworld --type=NodePort
    ```

    We'll get an output now which shows the port mappings

    ```bash
    adminlocal@banks:~$ kubectl get svc
    NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
    helloworld   NodePort    10.98.3.69   <none>        8080:31491/TCP   14s
    kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP          58m
    ```

    And quickly check the status of the pod

    ```bash
    adminlocal@banks:~$ kubectl get pods
    NAME                          READY   STATUS    RESTARTS   AGE
    helloworld-68f956cfd8-dxdfb   1/1     Running   0          23m
    ```

    ???+ danger "Pod stuck on 'Pending'"
        If your pod, per the above, is stuck on `pending` for a long period of time check whether you've [removed the the control plane node taint](#remove-the-control-plane-node-isolation-taint). Without this, the scheduler will look for somewhere to run the pod but will fail if you've only got one physical node.

        ```bash
        adminlocal@banks:~$ kubectl get pods
        NAME                          READY   STATUS    RESTARTS   AGE
        helloworld-68f956cfd8-dxdfb   0/1     Pending   0          15m
        ```

    As we've used `NodePort` above our internal port of `8080` for the application has been mapped to port `31491` on the node automatically by the scheduler. A description of the service yields soime useful bits of information.

    ```bash hl_lines="5 11"
    adminlocal@banks:~$ kubectl describe services helloworld
    Name:                     helloworld
    Namespace:                default
    Labels:                   run=helloworld
    Annotations:              <none>
    Selector:                 run=helloworld
    Type:                     NodePort
    IP:                       10.98.3.69
    Port:                     <unset>  8080/TCP
    TargetPort:               8080/TCP
    NodePort:                 <unset>  31491/TCP
    Endpoints:                10.244.0.4:8080
    Session Affinity:         None
    External Traffic Policy:  Cluster
    Events:                   <none>
    ```

    As detailed in [Use a Service to Access an Application in a Cluster](https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/) we can list the pods that are running this application by using the `selector` highlighted above.


    ```bash
    adminlocal@banks:~$ kubectl get pods --selector="run=helloworld" --output=wide
    NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE    NOMINATED NODE   READINESS GATES
    helloworld-68f956cfd8-dxdfb   1/1     Running   0          60m   10.244.0.4   banks   <none>           <none>
    ```

    Now to find the (available only on our internal network for the moment) IP we need to run `kubectl describe node banks`. This returns a huge amount of information but the most important part being `InternalIP`.

    ```bash hl_lines="9"
    ...
    Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
    ----             ------  -----------------                 ------------------                ------                       -------
    MemoryPressure   False   Mon, 18 Nov 2019 22:56:21 +0000   Mon, 18 Nov 2019 20:55:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
    DiskPressure     False   Mon, 18 Nov 2019 22:56:21 +0000   Mon, 18 Nov 2019 20:55:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
    PIDPressure      False   Mon, 18 Nov 2019 22:56:21 +0000   Mon, 18 Nov 2019 20:55:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
    Ready            True    Mon, 18 Nov 2019 22:56:21 +0000   Mon, 18 Nov 2019 20:56:17 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
    Addresses:
    InternalIP:  192.168.0.95
    Hostname:    banks
    Capacity:
    cpu:                16
    ephemeral-storage:  114295980Ki
    hugepages-2Mi:      0
    ...
    ```

    Finally we can navigate to http://192.168.0.95:31491 and see a response:

    ```bash
    adminlocal@banks:~$ curl http://192.168.0.95:31491
    CLIENT VALUES:
    client_address=192.168.0.95
    command=GET
    real path=/
    query=nil
    request_version=1.1
    request_uri=http://192.168.0.95:8080/

    SERVER VALUES:
    server_version=nginx: 1.10.0 - lua: 10001

    HEADERS RECEIVED:
    accept=*/*
    host=192.168.0.95:31491
    user-agent=curl/7.58.0
    BODY:
    -no body in request-
    ```

    And let's teardown the application deployment and service.

    ```bash
    kubectl delete services helloworld
    kubectl delete deployment helloworld
    ```