# CEPH Storage with Rook
In [part 2]() we installed Kubernetes and setup a user (in my case `adminlocal`) on our cluster with the ability to run administrative kubernetes commands.

etcd (key/value store), Rook, Promethues and Vault are all examples of technologies we will be using in our cluster and are deployed using [Kubernetes Operators](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/). In this section we'll be deploying the [Rook](https://rook.io) storage orchestrator with [Ceph](http://ceph.com) as a storage provider. 

## Rook
[Rook](https://rook.io) allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider.

More information on the architecure can be found in the [docs](https://rook.github.io/docs/rook/master/ceph-storage.html)

![Rook Architecture](https://rook.github.io/docs/rook/master/media/rook-architecture.png)

![Rook Kubernetes](https://rook.github.io/docs/rook/master/media/kubernetes.png)

## Ceph
Ceph provides three types of storage:

* `object`: compatible with S3 API
* `file`: files and directories (incl. NFS); and
* `block`: replicate a hard drive

There a 4 key components of the architecture to be aware of (shown above in the Rook diagram):

* `Monitor` (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication
* `Manager` daemon: keeps track of state in cluster and metrics
* `OSDs` (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage.
* `MSDs` (Metatdata Server): concerned with filesystem storage type only

Under the hood everything is stored as an object in logical storage pools.

## Installation and Setup
### Deployment of the Rook operator
See the Rook [quickstart](https://rook.io/docs/rook/v1.1/ceph-quickstart.html)

We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple `mon` components onto a single node. Similar to when we [removed the the control plane node taint](01.infrastructure/02.kubernetes/00.configuring.kubernetes.md#remove-the-control-plane-node-isolation-taint) Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes).

```bash
# create a working directory
mkdir -p ~/rook && \
cd ~/rook

# download the sample files
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/common.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/operator.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/cluster.yaml; \
wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/toolbox.yaml

# modify the cluster spec
#   - allow multiple mons per node
sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml
```

In addition, because we've setup our encrypted data storage to be mounted at `/data/${BLKID}` we will edit the default storage options to remove the `useAllDevices` selection and, instead, specify the directories. See the [docs](https://rook.io/docs/rook/v1.1/ceph-cluster-crd.html#storage-configuration-cluster-wide-directories) for more details.

```bash
#   - set default storage location and remove default `useAllDevices: true`
sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml
sed -i.bak 's/deviceFilter:/deviceFilter: data-*' cluster.yaml
```

With these configurations now downloaded we'll apply them in the following order.

```bash
kubectl create -f ~/rook/common.yaml; \
kubectl create -f ~/rook/operator.yaml
```

???+ warning "Verify the `rook-ceph-operator` is in the `Running` state"
    Use `kubectl -n rook-ceph get pod` to check we have a running state.
    ```bash
    root@banks:~# kubectl -n rook-ceph get pod
    NAME                                            READY   STATUS    RESTARTS   AGE
    ...
    rook-ceph-operator-c8ff6447d-tbh5c              1/1     Running   0          6m18s
    ```

### Create the Rook cluster
Assuming the operator looks ok we can now create the cluster
```bash
kubectl create -f ~/rook/cluster.yaml
```

To verify the state of the cluster we will connect to the [Rook Toolbox](https://rook.io/docs/rook/v1.1/ceph-toolbox.html)

```bash
kubectl create -f ~/rook/toolbox.yaml
```

Wait for the toolbox pod to enter a `running` state:

```bash
kubectl -n rook-ceph get pod -l "app=rook-ceph-tools"
```

Once the rook-ceph-tools pod is running, you can connect to it with:
```
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
```

When inside the toolbox run `ceph status`

???+ info "ceph status"
    ```bash
    [root@banks /]# ceph status
    cluster:
        id:     e39fdcf2-0c6a-4625-a57b-91914cafa634
        health: HEALTH_OK
    
    services:
        mon: 3 daemons, quorum a,b,c (age 55s)
        mgr: a(active, since 12s)
        osd: 0 osds: 0 up, 0 in
    
    data:
        pools:   0 pools, 0 pgs
        objects: 0 objects, 0 B
        usage:   0 B used, 0 B / 0 B avail
        pgs:     
    ```

    * All mons should be in quorum
    * A mgr should be active
    * At least one OSD should be active
    * If the health is not HEALTH_OK, the warnings or errors should be investigated

When you are done with the toolbox, you can remove the deployment:
```
kubectl -n rook-ceph delete deployment rook-ceph-tools
```

??? tip "If you want to delete the cluster and start again..."
    Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse.
    ```bash
    kubectl delete -f toolbox.yaml; \
    kubectl delete -f cluster.yaml; \
    kubectl delete -f operator.yaml; \
    kubectl delete -f common.yaml
    ```

### Configure storage
