{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Darth-Veitcher's Homelab This repository contains docker details, notes and general setup musings around configuring my (overkill) homelab end-to-end. This has been deliberately done the hard way (i.e. handcrafted from scratch) wherever possible so that I understand what's going on under the hood. I'm not a big fan of black boxes. You'll undoubtedly find quicker and easier getting started with Kubernetes guides elsewhere online (personally I'd recommend looking at some of the excellent posts by Alex Ellis ) but I wanted something fully featured. This document should not feel like the below... This setup is opinionated and features the following: Hybrid cloud setup with nodes both on-premise (bare metal) and cloud (dedicated servers) VPN links for secure access between nodes Redundant Ceph storage Monitoring via Prometheus, visualisation with Grafana TLS certificates via LetsEncrypt Integrated Identity and Access Management (IDAM), supporting multiple protocols (incl. Oauth, OIDC and LDAP) via Keycloak High level requirements, roadmap and table of contents Reference Architecture (appendix) Configuring physical nodes VPN between nodes Basic hardening Configuring kubernetes Networking and Network Policy via Canal Service Discovery via CoreDNS Ceph storage via Rook Block storage for pods Shared filesystem Object storage with LDAP for authentication ; and with Vault for secure key management With dashboard Enabled object gateway management via SSO Monitoring with Prometheus and Grafana Cloudflare integration with external-dns TLS certificates with Cert-Manager Self-signed Root CA for internal services Let's Encrypt for external services Identity and Access Management with OIDC OpenLDAP as directory service KeyCloak/Dex as identity provider Multi-factor auth for key admin users OIDC Forward Auth for additional fine grained RBAC Secure VPN access for users Integrated with PKI Integrated with IDAM, Keycloak+OpenLDAP Services Stack: Developer Docker registry Gitea","title":"Introduction"},{"location":"#darth-veitchers-homelab","text":"This repository contains docker details, notes and general setup musings around configuring my (overkill) homelab end-to-end. This has been deliberately done the hard way (i.e. handcrafted from scratch) wherever possible so that I understand what's going on under the hood. I'm not a big fan of black boxes. You'll undoubtedly find quicker and easier getting started with Kubernetes guides elsewhere online (personally I'd recommend looking at some of the excellent posts by Alex Ellis ) but I wanted something fully featured. This document should not feel like the below... This setup is opinionated and features the following: Hybrid cloud setup with nodes both on-premise (bare metal) and cloud (dedicated servers) VPN links for secure access between nodes Redundant Ceph storage Monitoring via Prometheus, visualisation with Grafana TLS certificates via LetsEncrypt Integrated Identity and Access Management (IDAM), supporting multiple protocols (incl. Oauth, OIDC and LDAP) via Keycloak","title":"Darth-Veitcher's Homelab"},{"location":"#high-level-requirements-roadmap-and-table-of-contents","text":"Reference Architecture (appendix) Configuring physical nodes VPN between nodes Basic hardening Configuring kubernetes Networking and Network Policy via Canal Service Discovery via CoreDNS Ceph storage via Rook Block storage for pods Shared filesystem Object storage with LDAP for authentication ; and with Vault for secure key management With dashboard Enabled object gateway management via SSO Monitoring with Prometheus and Grafana Cloudflare integration with external-dns TLS certificates with Cert-Manager Self-signed Root CA for internal services Let's Encrypt for external services Identity and Access Management with OIDC OpenLDAP as directory service KeyCloak/Dex as identity provider Multi-factor auth for key admin users OIDC Forward Auth for additional fine grained RBAC Secure VPN access for users Integrated with PKI Integrated with IDAM, Keycloak+OpenLDAP Services Stack: Developer Docker registry Gitea","title":"High level requirements, roadmap and table of contents"},{"location":"01.infrastructure/01.hosts/00.configuring.physical.nodes/","text":"Physical Nodes I'm going to start with a single node install on a server at home and then, slowly, add additional nodes to create a cluster. I'll taint the first node though so that it can run standalone without needing further nodes to operate. My final configuration will look roughly like this: Homelab Server 1: Dell R610, Primary Compute, asimov Server 2: Dell R710, Secondary Compute, banks Server 3: Custom Build, Primary LAN Storage, clarke Mini PC: Custom Build x86, LoadBalancer between master nodes for HA (later step), hamilton Intel Celeron J1900 quad core, 8GB RAM, 32GB SSD, x4 LAN Cloud Dedicated Server 1: Kimsufi, Ingress Node, donaldson Dedicated Server 2: Kimsufi, Compute Node, eesmith Between the Homelab and Cloud I'll run a VPN such that traffic is encrypted and we can bypass the public internet and any associated NAT / routing issues by having the nodes discover each other using VPN subnet addresses. We'll start with a single node on-prem and then expand and add to this.","title":"Host Configuration"},{"location":"01.infrastructure/01.hosts/00.configuring.physical.nodes/#physical-nodes","text":"I'm going to start with a single node install on a server at home and then, slowly, add additional nodes to create a cluster. I'll taint the first node though so that it can run standalone without needing further nodes to operate. My final configuration will look roughly like this: Homelab Server 1: Dell R610, Primary Compute, asimov Server 2: Dell R710, Secondary Compute, banks Server 3: Custom Build, Primary LAN Storage, clarke Mini PC: Custom Build x86, LoadBalancer between master nodes for HA (later step), hamilton Intel Celeron J1900 quad core, 8GB RAM, 32GB SSD, x4 LAN Cloud Dedicated Server 1: Kimsufi, Ingress Node, donaldson Dedicated Server 2: Kimsufi, Compute Node, eesmith Between the Homelab and Cloud I'll run a VPN such that traffic is encrypted and we can bypass the public internet and any associated NAT / routing issues by having the nodes discover each other using VPN subnet addresses. We'll start with a single node on-prem and then expand and add to this.","title":"Physical Nodes"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/","text":"This is an optional setup if you want to configure a direct wireguard VPN link between nodes instead of using a CNI plugin. Setup Wireguard (VPN) See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. I've gone with Wireguard over OpenVPN based on it being incorporated into the Linux Kernel and increased performance versus OpenVPN. Install Wireguard sudo add-apt-repository -y ppa:wireguard/wireguard ; \\ sudo apt-get install -y wireguard ; \\ sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. root@banks:~# lsmod | grep wireguard wireguard 212992 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Keys You will need to generate a key-pair for every peer (device) that is connected, including things like mobile phones etc. The iOS WireGuard client allow you to generate the keys on the device itself (if you want). # Generate public/private keypair cd /etc/wireguard umask 077 wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey Configure We need to create a network interface now for the wireguard VPN. Common convention is to use wg0 as a name for this. In addition we also need to choose a subnet for the VPN addresses. As I've got a 192.168.0.1/24 configuration at home I'll use 10.10.0.1/24 for the VPN. Note the highlighted IP address we assign to each node here. It will need to be incremented for each to provide a unique address. # file: /etc/wireguard/wg0.conf [ Interface ] PrivateKey = {{ PRIVATE_KEY }} Address = 10 .10.0.1/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT ; iptables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -A FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT ; iptables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -D FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ListenPort = 51820 Now to keepo things DRY we'll run the following to replace the placeholder text with the actual contents of our server's private key we generated earlier. sudo sed -i.bak 's/{{PRIVATE_KEY}}/' $( sudo cat /etc/wireguard/privatekey ) '/' /etc/wireguard/wg0.conf We also need to replace the {{ETH0}} placeholder with the name of our existing primary network interface. A quick one-liner for this is ip -4 route | grep default | awk '{print $5}' which, on my server, gives bond0 as the answer (as I'm running LACP across multiple bonded physical interfaces). sudo sed -i.bak 's/{{ETH0}}/' $( ip -4 route | grep default | awk '{print $5}' ) '/g' /etc/wireguard/wg0.conf Enable forwarding of packets in the host kernel. sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p Finally we can start the wg0 interface. wg-quick up wg0 Hopefully you'll see something like the below output. root@banks:/etc/wireguard# wg-quick up wg0 [ #] ip link add wg0 type wireguard [ #] wg setconf wg0 /dev/fd/63 [ #] ip -4 address add 10.10.0.1/24 dev wg0 [ #] ip -6 address add fd86:ea04:1111::1/64 dev wg0 [ #] ip link set mtu 1420 up dev wg0 [ #] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o bond0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o bond0 -j MASQUERADE Checking status The check the status of wireguard run the wg command. root@banks:/etc/wireguard# wg interface: wg0 public key: I6ZHsLe44SHNH44xE86AI0VEnm8CfzrQUrxSCJVjAEw = private key: ( hidden ) listening port: 51820 In addition, we should now have an additional route appear for our VPN subnet. root@banks:/etc/wireguard# ip route default via 192 .168.0.1 dev bond0 proto dhcp src 192 .168.0.94 metric 100 10 .10.0.0/24 dev wg0 proto kernel scope link src 10 .10.0.1 172 .17.0.0/16 dev docker0 proto kernel scope link src 172 .17.0.1 linkdown ... Systemd service Assuming the above works we can now enable the wg0 interface on boot. sudo systemctl enable wg-quick@wg0.service sudo systemctl daemon-reload You can then manually start sudo service wg-quick@wg0 start and check status service wg-quick@wg0 status of the service.","title":"00.wireguard.vpn"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#setup-wireguard-vpn","text":"See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. I've gone with Wireguard over OpenVPN based on it being incorporated into the Linux Kernel and increased performance versus OpenVPN.","title":"Setup Wireguard (VPN)"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#install-wireguard","text":"sudo add-apt-repository -y ppa:wireguard/wireguard ; \\ sudo apt-get install -y wireguard ; \\ sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. root@banks:~# lsmod | grep wireguard wireguard 212992 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard","title":"Install Wireguard"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#keys","text":"You will need to generate a key-pair for every peer (device) that is connected, including things like mobile phones etc. The iOS WireGuard client allow you to generate the keys on the device itself (if you want). # Generate public/private keypair cd /etc/wireguard umask 077 wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey","title":"Keys"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#configure","text":"We need to create a network interface now for the wireguard VPN. Common convention is to use wg0 as a name for this. In addition we also need to choose a subnet for the VPN addresses. As I've got a 192.168.0.1/24 configuration at home I'll use 10.10.0.1/24 for the VPN. Note the highlighted IP address we assign to each node here. It will need to be incremented for each to provide a unique address. # file: /etc/wireguard/wg0.conf [ Interface ] PrivateKey = {{ PRIVATE_KEY }} Address = 10 .10.0.1/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT ; iptables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -A FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT ; iptables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -D FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ListenPort = 51820 Now to keepo things DRY we'll run the following to replace the placeholder text with the actual contents of our server's private key we generated earlier. sudo sed -i.bak 's/{{PRIVATE_KEY}}/' $( sudo cat /etc/wireguard/privatekey ) '/' /etc/wireguard/wg0.conf We also need to replace the {{ETH0}} placeholder with the name of our existing primary network interface. A quick one-liner for this is ip -4 route | grep default | awk '{print $5}' which, on my server, gives bond0 as the answer (as I'm running LACP across multiple bonded physical interfaces). sudo sed -i.bak 's/{{ETH0}}/' $( ip -4 route | grep default | awk '{print $5}' ) '/g' /etc/wireguard/wg0.conf Enable forwarding of packets in the host kernel. sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p Finally we can start the wg0 interface. wg-quick up wg0 Hopefully you'll see something like the below output. root@banks:/etc/wireguard# wg-quick up wg0 [ #] ip link add wg0 type wireguard [ #] wg setconf wg0 /dev/fd/63 [ #] ip -4 address add 10.10.0.1/24 dev wg0 [ #] ip -6 address add fd86:ea04:1111::1/64 dev wg0 [ #] ip link set mtu 1420 up dev wg0 [ #] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o bond0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o bond0 -j MASQUERADE Checking status The check the status of wireguard run the wg command. root@banks:/etc/wireguard# wg interface: wg0 public key: I6ZHsLe44SHNH44xE86AI0VEnm8CfzrQUrxSCJVjAEw = private key: ( hidden ) listening port: 51820 In addition, we should now have an additional route appear for our VPN subnet. root@banks:/etc/wireguard# ip route default via 192 .168.0.1 dev bond0 proto dhcp src 192 .168.0.94 metric 100 10 .10.0.0/24 dev wg0 proto kernel scope link src 10 .10.0.1 172 .17.0.0/16 dev docker0 proto kernel scope link src 172 .17.0.1 linkdown ...","title":"Configure"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#systemd-service","text":"Assuming the above works we can now enable the wg0 interface on boot. sudo systemctl enable wg-quick@wg0.service sudo systemctl daemon-reload You can then manually start sudo service wg-quick@wg0 start and check status service wg-quick@wg0 status of the service.","title":"Systemd service"},{"location":"01.infrastructure/01.hosts/01.banks/","text":"Basic housekeeping First of all install the latest Ubuntu 18.04 LTS release and copy across your ssh key and then follow some simple hardening steps #TODO: link to ansible . Allow sudo without password (optional) You can allow your user to execute sudo commands without needing a password prompt for ease. echo \" $USER ALL=(ALL:ALL) NOPASSWD:ALL\" | sudo tee -a /etc/sudoers > /dev/null We'll configure our hostname too in order to reflect whatever the fqdn of our server is going to be. In my case this will be banks.local as it will just be an internal address. This is used by kubernetes later on. sudo hostnamectl set-hostname banks.local Install wireguard See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. As wireguard is now incorporated into the linux kernel we can install it on the host nodes and then kubernetes network plugins such as wormhole or kilo can then use the kernel module to configure a dynamic VPN mesh between nodes. sudo add-apt-repository -y ppa:wireguard/wireguard ; \\ sudo apt-get install -y wireguard ; \\ sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. $ lsmod | grep wireguard wireguard 208896 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Enable forwarding of packets in the host kernel (there might be a slight delay in reconnecting if doing this over ssh). sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p Enable Avahi (Discovery) on VPN and LAN As per Wikipedia . Avahi is a free zero-configuration networking (zeroconf) implementation, including a system for multicast DNS/DNS-SD service discovery. It is licensed under the GNU Lesser General Public License (LGPL). Avahi is a system which enables programs to publish and discover services and hosts running on a local network. For example, a user can plug a computer into a network and have Avahi automatically advertise the network services running on its machine, facilitating user access to those services. We will setup our nodes to publish themselves on both the LAN (so can be accessed via their hostnames) and VPN (optional). # from docker export DEBIAN_FRONTEND = noninteractive ; \\ sudo apt-get update -y ; \\ sudo apt-get -qq install -y avahi-daemon avahi-utils The main configuration is held in /etc/avahi/avahi-daemon.conf . We can modify the allow-interfaces line (to limit which interfaces we advertise on). This will be useful for when we want to only enable it on internal interfaces (e.g. our Cloud node shouldn't try and broadcast across the internet). We'll leave this for now. # file: /etc/avahi/avahi-daemon.conf allow-interfaces = bond0, wg0, docker0 # If we're using an internal domain then leave as `local` below, else change to tld domain-name = local Reload / restart the daemon with sudo systemctl daemon-reload && sudo systemctl restart avahi-daemon.service Network status After a vanilla install above, the network configuration and topology looks like the below. Network status $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 6 links listed. $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 192 .168.0.1 0 .0.0.0 UG 100 0 0 bond0 192 .168.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 bond0 192 .168.0.1 0 .0.0.0 255 .255.255.255 UH 100 0 0 bond0","title":"Compute (Banks)"},{"location":"01.infrastructure/01.hosts/01.banks/#basic-housekeeping","text":"First of all install the latest Ubuntu 18.04 LTS release and copy across your ssh key and then follow some simple hardening steps #TODO: link to ansible . Allow sudo without password (optional) You can allow your user to execute sudo commands without needing a password prompt for ease. echo \" $USER ALL=(ALL:ALL) NOPASSWD:ALL\" | sudo tee -a /etc/sudoers > /dev/null We'll configure our hostname too in order to reflect whatever the fqdn of our server is going to be. In my case this will be banks.local as it will just be an internal address. This is used by kubernetes later on. sudo hostnamectl set-hostname banks.local","title":"Basic housekeeping"},{"location":"01.infrastructure/01.hosts/01.banks/#install-wireguard","text":"See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. As wireguard is now incorporated into the linux kernel we can install it on the host nodes and then kubernetes network plugins such as wormhole or kilo can then use the kernel module to configure a dynamic VPN mesh between nodes. sudo add-apt-repository -y ppa:wireguard/wireguard ; \\ sudo apt-get install -y wireguard ; \\ sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. $ lsmod | grep wireguard wireguard 208896 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Enable forwarding of packets in the host kernel (there might be a slight delay in reconnecting if doing this over ssh). sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p","title":"Install wireguard"},{"location":"01.infrastructure/01.hosts/01.banks/#enable-avahi-discovery-on-vpn-and-lan","text":"As per Wikipedia . Avahi is a free zero-configuration networking (zeroconf) implementation, including a system for multicast DNS/DNS-SD service discovery. It is licensed under the GNU Lesser General Public License (LGPL). Avahi is a system which enables programs to publish and discover services and hosts running on a local network. For example, a user can plug a computer into a network and have Avahi automatically advertise the network services running on its machine, facilitating user access to those services. We will setup our nodes to publish themselves on both the LAN (so can be accessed via their hostnames) and VPN (optional). # from docker export DEBIAN_FRONTEND = noninteractive ; \\ sudo apt-get update -y ; \\ sudo apt-get -qq install -y avahi-daemon avahi-utils The main configuration is held in /etc/avahi/avahi-daemon.conf . We can modify the allow-interfaces line (to limit which interfaces we advertise on). This will be useful for when we want to only enable it on internal interfaces (e.g. our Cloud node shouldn't try and broadcast across the internet). We'll leave this for now. # file: /etc/avahi/avahi-daemon.conf allow-interfaces = bond0, wg0, docker0 # If we're using an internal domain then leave as `local` below, else change to tld domain-name = local Reload / restart the daemon with sudo systemctl daemon-reload && sudo systemctl restart avahi-daemon.service","title":"Enable Avahi (Discovery) on VPN and LAN"},{"location":"01.infrastructure/01.hosts/01.banks/#network-status","text":"After a vanilla install above, the network configuration and topology looks like the below. Network status $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 6 links listed. $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 192 .168.0.1 0 .0.0.0 UG 100 0 0 bond0 192 .168.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 bond0 192 .168.0.1 0 .0.0.0 255 .255.255.255 UH 100 0 0 bond0","title":"Network status"},{"location":"01.infrastructure/01.hosts/02.clarke/","text":"I've got an old self-built frankenstein storage server that runs a lot of disks in it. Again, I've installed Ubuntu 18.04 LTS on it and will add as a master node to the cluster to surface up file/block/object storage to workloads. NB: I'm going to use the storage server as a master due to the fact it's one of the device which should always be on and running to support other workloads. Specs For reference, here's the specs and parts list. Setup storage As I'll be using Ceph I'll leave all the disks as standalone JBOD (**J**ust a **B**unch **O**f **D**isks, no RAID) and perform fulldisk encryption such that they have to be unlocked if removed from the server with a key that's held on a separate, removeable, usb stick. Finding the relevant disks is done with lsblk . example lsblk I've expanded the lsblk command below to add some additional output options to help. serial : disk serial number (can be found on the physical disk) kname : internal kernel device name wwn : unique storage identifier (can be found on the physical disk, seems like not universally implemented by manufacturers) hotplug : removable or hotplug device (usb, pcmcia, ...) rota : rotational device (i.e. is it spinning rust hdd or ssd | nvme ) $ lsblk -o +size,serial,kname,wwn,hotplug,rota NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT SIZE SERIAL KNAME WWN HOTPLUG ROTA sda 8 :0 0 2 .7T 0 disk 2 .7T WD-WMC4N0K53DHU sda 0x50014ee6b0533e3d 0 1 sdb 8 :16 0 953 .9G 0 disk 953 .9G 9591205808031 sdb 0 0 sdc 8 :32 0 117 .4G 0 disk 117 .4G 142433405344 sdc 0x5001b44c354645a0 0 0 sdd 8 :48 0 953 .9G 0 disk 953 .9G 9591205808034 sdd 0 0 sde 8 :64 0 2 .7T 0 disk 2 .7T WD-WCC4N6APEFS4 sde 0x50014ee2605ef68e 0 1 sdf 8 :80 1 29 .3G 0 disk 29 .3G 4C530000290829118252 sdf 1 1 \u2514\u2500sdf1 8 :81 1 29 .3G 0 part /mnt/key 29 .3G sdf1 1 1 nvme0n1 259 :0 0 119 .2G 0 disk 119 .2G 9191210606695 nvme0n1 eui.0100000000000000 0 0 \u251c\u2500nvme0n1p1 259 :1 0 512M 0 part /boot/efi 512M nvme0n1p1 eui.0100000000000000 0 0 \u2514\u2500nvme0n1p2 259 :2 0 118 .8G 0 part 118 .8G nvme0n1p2 eui.0100000000000000 0 0 \u251c\u2500clarke--vg-root 253 :0 0 117 .8G 0 lvm / 117 .8G dm-0 0 0 \u2514\u2500clarke--vg-swap_1 253 :1 0 980M 0 lvm 980M dm-1 0 0 Based on the above output we can see I have: one hotplug device ( sdf ) which is the usb with the key on it, mounted to /mnt/key two rotational devices ( sde , sda ) with 2.7TB of capacity three non-rotational (i.e. ssd ) devices with varying capacities ( sdb , sdc , sdd ) one device ( nvme ) which contains the OS As a way of illustration her are pictures of the physcial devices in sda and sdb . ??? example \" sda 3TB Western Digital Green !WD ??? example \" sdb KingSpec 1TB SSD !KS Create a KeyFile There a a number of different ways to do this. I'd also advise that you perform this on your trusted development machine (and back up the resulting file somewhere...) as opposed to doing all of this on the target host. # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = ~/.secretkey bs = 1 count = 4096 chmod 0400 ~/.secretkey Copy that file onto a FAT formatted USB drive. It probably doesn't matter too much what filesystem you use but that has the most common support across Linux, MacOS and Windows. Now, in the target host, insert the USB and mount it to /mnt/key as read only . sudo mkdir -p /mnt/key sudo mount -t vfat -o defaults,ro /dev/sdf1 /mnt/key Check this has worked by listing the contents and then attempting to modify the keyfile. $ head -n 1 /mnt/key/.secretkey M???j [ \u0335s?%?< ; ?fwefwehwqn??pZ?^o?v? | 2 ?f\u01c4 ) ?#?%??x*?@.?? ??j????/?2?NC? ] ? { ? } thmrtj675?i??CZ & ???S?t324t?X?:?R?*?fl98??g?qxcv?n6?loi0 ; ? $ echo 'test modify' >> /mnt/key/.secretkey -bash: /mnt/key/.secretkey: Read-only file system Make this persist across reboots by modifying fstab . Replace the UUID with the output from blkid . # Get the UUID of disk $ blkid /dev/sdf1 -o value -s UUID FFCA-07F9 Now add into fstab so it's mounted every boot. # file: /etc/fstab # Key UUID = FFCA-07F9 /mnt/key vfat defaults,ro 0 1 Configure disks Rook and Ceph ideally work with blank disks and then take over the management of them, filesystem etc. Whilst we could allow Ceph to perform the encryption of the devices automatically I'd like to manage that myself with the physcial key. As a result we'll use cryptsetup first on our disks to create a LUKS volume that we then later provide to Ceph as the target device using a recent patch for devicePathFilter in Rook. NB: There's a script which will do this in an automated fashion in the files section of the repo. The below just covers it in order to explain the steps and process. Encrypt with LUKS # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset the disk sudo wipefs -af /dev/ $d # Encrypt the whole disk (using the keyfile) sudo cryptsetup luksFormat -q -s 512 -c aes-xts-plain64 -d /mnt/key/.secretkey /dev/ $d Auto-mount encrypted devices at boot We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the blkid of the encrypted disk export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid -s UUID -o value /dev/ $d ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-<uuid> and unlocks using the secretkey sudo tee -a /etc/crypttab <<EOF data-${BLKID} UUID=${BLKID} /mnt/key/.secretkey luks,retry=1,timeout=180 EOF Whilst we could now add the decrypted /dev/mapper/data-${BLKID} into fstab and mount it somewhere we don't need to for Rook and Ceph, they'll take over from here. Using a USB as a KeyFile As detailed Unlock LUKS Encrypted Volumes at Boot With a USB Key There is a bit of a catch here. /etc/crypttab is processed before /etc/fstab . So /mnt/key will not be available at the right time. What we need to do is create a /etc/default/cryptdisks file with the associated mount that needs to be present before processing /etc/crypttab . sudo tee -a /etc/default/cryptdisks <<EOF CRYPTDISKS_MOUNT='/mnt/key' EOF Completing To confirm the above is all working run a sudo shutdown -r now in order to prove the devices are decrypted by the key successfully. Networking As I'm using a switch that supports link aggregation (LACP) and a server with multiple network interface controllers (NICs) I'll modify the network configuration to take advantage of this. Create the file /etc/netplan/config.yaml with the following in it and then apply the configuration with sudo netplan apply , followed by a reboot. Modify as necessary for your setup. # /etc/netplan/config.yaml # sudo netplan apply network : version : 2 ethernets : eports : match : name : enp* optional : true bonds : bond0 : dhcp4 : true interfaces : [ eports ] # addresses: [192.168.0.111/24] # gateway4: 192.168.0.1 # nameservers: # search: [local] # addresses: [8.8.8.8, 8.8.4.4] parameters : mode : 802.3ad lacp-rate : fast transmit-hash-policy : layer2 Testing network throughput Using iperf3 you can test that this bonding is working as expected. mini : MacMini, single NIC asimov : Dell R710, 4xNICs clarke : Storage Server, 5xNICs With transfers from clarke \u2192 mini we get the following: [ ID ] Interval Transfer Bandwidth Retr [ 4 ] 0 .00-10.00 sec 112 MBytes 94 .2 Mbits/sec 64 sender [ 4 ] 0 .00-10.00 sec 112 MBytes 94 .1 Mbits/sec receiver With transfers between clarke \u2192 asimov we get a substantially higher throughput: [ ID ] Interval Transfer Bandwidth Retr [ 4 ] 0 .00-10.00 sec 1 .00 GBytes 862 Mbits/sec 0 sender [ 4 ] 0 .00-10.00 sec 1 .00 GBytes 862 Mbits/sec receiver Based on the above performance of 862 Mbits/sec I'm more likely to get limited by Disk I/O now.","title":"Storage (Clarke)"},{"location":"01.infrastructure/01.hosts/02.clarke/#specs","text":"For reference, here's the specs and parts list.","title":"Specs"},{"location":"01.infrastructure/01.hosts/02.clarke/#setup-storage","text":"As I'll be using Ceph I'll leave all the disks as standalone JBOD (**J**ust a **B**unch **O**f **D**isks, no RAID) and perform fulldisk encryption such that they have to be unlocked if removed from the server with a key that's held on a separate, removeable, usb stick. Finding the relevant disks is done with lsblk . example lsblk I've expanded the lsblk command below to add some additional output options to help. serial : disk serial number (can be found on the physical disk) kname : internal kernel device name wwn : unique storage identifier (can be found on the physical disk, seems like not universally implemented by manufacturers) hotplug : removable or hotplug device (usb, pcmcia, ...) rota : rotational device (i.e. is it spinning rust hdd or ssd | nvme ) $ lsblk -o +size,serial,kname,wwn,hotplug,rota NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT SIZE SERIAL KNAME WWN HOTPLUG ROTA sda 8 :0 0 2 .7T 0 disk 2 .7T WD-WMC4N0K53DHU sda 0x50014ee6b0533e3d 0 1 sdb 8 :16 0 953 .9G 0 disk 953 .9G 9591205808031 sdb 0 0 sdc 8 :32 0 117 .4G 0 disk 117 .4G 142433405344 sdc 0x5001b44c354645a0 0 0 sdd 8 :48 0 953 .9G 0 disk 953 .9G 9591205808034 sdd 0 0 sde 8 :64 0 2 .7T 0 disk 2 .7T WD-WCC4N6APEFS4 sde 0x50014ee2605ef68e 0 1 sdf 8 :80 1 29 .3G 0 disk 29 .3G 4C530000290829118252 sdf 1 1 \u2514\u2500sdf1 8 :81 1 29 .3G 0 part /mnt/key 29 .3G sdf1 1 1 nvme0n1 259 :0 0 119 .2G 0 disk 119 .2G 9191210606695 nvme0n1 eui.0100000000000000 0 0 \u251c\u2500nvme0n1p1 259 :1 0 512M 0 part /boot/efi 512M nvme0n1p1 eui.0100000000000000 0 0 \u2514\u2500nvme0n1p2 259 :2 0 118 .8G 0 part 118 .8G nvme0n1p2 eui.0100000000000000 0 0 \u251c\u2500clarke--vg-root 253 :0 0 117 .8G 0 lvm / 117 .8G dm-0 0 0 \u2514\u2500clarke--vg-swap_1 253 :1 0 980M 0 lvm 980M dm-1 0 0 Based on the above output we can see I have: one hotplug device ( sdf ) which is the usb with the key on it, mounted to /mnt/key two rotational devices ( sde , sda ) with 2.7TB of capacity three non-rotational (i.e. ssd ) devices with varying capacities ( sdb , sdc , sdd ) one device ( nvme ) which contains the OS As a way of illustration her are pictures of the physcial devices in sda and sdb . ??? example \" sda 3TB Western Digital Green !WD ??? example \" sdb KingSpec 1TB SSD !KS","title":"Setup storage"},{"location":"01.infrastructure/01.hosts/02.clarke/#create-a-keyfile","text":"There a a number of different ways to do this. I'd also advise that you perform this on your trusted development machine (and back up the resulting file somewhere...) as opposed to doing all of this on the target host. # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = ~/.secretkey bs = 1 count = 4096 chmod 0400 ~/.secretkey Copy that file onto a FAT formatted USB drive. It probably doesn't matter too much what filesystem you use but that has the most common support across Linux, MacOS and Windows. Now, in the target host, insert the USB and mount it to /mnt/key as read only . sudo mkdir -p /mnt/key sudo mount -t vfat -o defaults,ro /dev/sdf1 /mnt/key Check this has worked by listing the contents and then attempting to modify the keyfile. $ head -n 1 /mnt/key/.secretkey M???j [ \u0335s?%?< ; ?fwefwehwqn??pZ?^o?v? | 2 ?f\u01c4 ) ?#?%??x*?@.?? ??j????/?2?NC? ] ? { ? } thmrtj675?i??CZ & ???S?t324t?X?:?R?*?fl98??g?qxcv?n6?loi0 ; ? $ echo 'test modify' >> /mnt/key/.secretkey -bash: /mnt/key/.secretkey: Read-only file system Make this persist across reboots by modifying fstab . Replace the UUID with the output from blkid . # Get the UUID of disk $ blkid /dev/sdf1 -o value -s UUID FFCA-07F9 Now add into fstab so it's mounted every boot. # file: /etc/fstab # Key UUID = FFCA-07F9 /mnt/key vfat defaults,ro 0 1","title":"Create a KeyFile"},{"location":"01.infrastructure/01.hosts/02.clarke/#configure-disks","text":"Rook and Ceph ideally work with blank disks and then take over the management of them, filesystem etc. Whilst we could allow Ceph to perform the encryption of the devices automatically I'd like to manage that myself with the physcial key. As a result we'll use cryptsetup first on our disks to create a LUKS volume that we then later provide to Ceph as the target device using a recent patch for devicePathFilter in Rook. NB: There's a script which will do this in an automated fashion in the files section of the repo. The below just covers it in order to explain the steps and process.","title":"Configure disks"},{"location":"01.infrastructure/01.hosts/02.clarke/#encrypt-with-luks","text":"# Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset the disk sudo wipefs -af /dev/ $d # Encrypt the whole disk (using the keyfile) sudo cryptsetup luksFormat -q -s 512 -c aes-xts-plain64 -d /mnt/key/.secretkey /dev/ $d","title":"Encrypt with LUKS"},{"location":"01.infrastructure/01.hosts/02.clarke/#auto-mount-encrypted-devices-at-boot","text":"We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the blkid of the encrypted disk export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid -s UUID -o value /dev/ $d ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-<uuid> and unlocks using the secretkey sudo tee -a /etc/crypttab <<EOF data-${BLKID} UUID=${BLKID} /mnt/key/.secretkey luks,retry=1,timeout=180 EOF Whilst we could now add the decrypted /dev/mapper/data-${BLKID} into fstab and mount it somewhere we don't need to for Rook and Ceph, they'll take over from here.","title":"Auto-mount encrypted devices at boot"},{"location":"01.infrastructure/01.hosts/02.clarke/#using-a-usb-as-a-keyfile","text":"As detailed Unlock LUKS Encrypted Volumes at Boot With a USB Key There is a bit of a catch here. /etc/crypttab is processed before /etc/fstab . So /mnt/key will not be available at the right time. What we need to do is create a /etc/default/cryptdisks file with the associated mount that needs to be present before processing /etc/crypttab . sudo tee -a /etc/default/cryptdisks <<EOF CRYPTDISKS_MOUNT='/mnt/key' EOF","title":"Using a USB as a KeyFile"},{"location":"01.infrastructure/01.hosts/02.clarke/#completing","text":"To confirm the above is all working run a sudo shutdown -r now in order to prove the devices are decrypted by the key successfully.","title":"Completing"},{"location":"01.infrastructure/01.hosts/02.clarke/#networking","text":"As I'm using a switch that supports link aggregation (LACP) and a server with multiple network interface controllers (NICs) I'll modify the network configuration to take advantage of this. Create the file /etc/netplan/config.yaml with the following in it and then apply the configuration with sudo netplan apply , followed by a reboot. Modify as necessary for your setup. # /etc/netplan/config.yaml # sudo netplan apply network : version : 2 ethernets : eports : match : name : enp* optional : true bonds : bond0 : dhcp4 : true interfaces : [ eports ] # addresses: [192.168.0.111/24] # gateway4: 192.168.0.1 # nameservers: # search: [local] # addresses: [8.8.8.8, 8.8.4.4] parameters : mode : 802.3ad lacp-rate : fast transmit-hash-policy : layer2 Testing network throughput Using iperf3 you can test that this bonding is working as expected. mini : MacMini, single NIC asimov : Dell R710, 4xNICs clarke : Storage Server, 5xNICs With transfers from clarke \u2192 mini we get the following: [ ID ] Interval Transfer Bandwidth Retr [ 4 ] 0 .00-10.00 sec 112 MBytes 94 .2 Mbits/sec 64 sender [ 4 ] 0 .00-10.00 sec 112 MBytes 94 .1 Mbits/sec receiver With transfers between clarke \u2192 asimov we get a substantially higher throughput: [ ID ] Interval Transfer Bandwidth Retr [ 4 ] 0 .00-10.00 sec 1 .00 GBytes 862 Mbits/sec 0 sender [ 4 ] 0 .00-10.00 sec 1 .00 GBytes 862 Mbits/sec receiver Based on the above performance of 862 Mbits/sec I'm more likely to get limited by Disk I/O now.","title":"Networking"},{"location":"01.infrastructure/01.hosts/03.donaldson/","text":"For the Kubernetes ingress node I'm going to use a cheap (<$5 per month) dedicated server from Kimsufi which has unmetered traffic and 100mb bandwidth. This will allow me to have a static IP4 address to point my DNS records at whilst then using the internal wireguard VPN to route traffic from here back home / elsewhere in a secure fashion. Boot into rescue We need to set our cloud node to boot into a LiveCD rescue image so that we can then login and install CoreOS to our main disk. Kimsufi Control Panel Login to the kimsufi control panel and select Netboot from the options against your server. Step through te options provided and select Rescue \u2192 rescue64-pro . Now restart the server and wait for an email confirming your temporary ssh login credentials. Install CoreOS Installation SSH into the box with the temporary credentials and use the CoreOS installer script to overwrite and nuke current installation by OVH. Make sure we've transferred across our ignition file with ssh key and hostname etc. wget https://raw.github.com/coreos/init/master/bin/coreos-install ; \\ chmod +x coreos-install ; \\ sudo ./coreos-install -d /dev/sda -i ignition.json Once you see a Success! CoreOS Container Linux stable 2303.3.0 is installed on /dev/sda we can reboot (after changing the boot option in the control panel). Kimsufi Control Panel Now go back to the control panel and change the boot to Hard disk before then rebooting. Configure Kubernetes Follow the existing instructions, remembering to change for your hostname and also use kimsufi as region when labelling the node. Node stuck on \"Not Ready\" If the node, even after applying the CNI, is still stuck on Not Ready check the following. output from systemctl status kubelet output from kubectl describe node <nodename> Chances are you have the same problem I ended up with originally of CoreOS being unable to find the correct CNI configuration file. Unable to update cni config: no networks found in /etc/cni/net.d If you're seeing this error in the output from systemctl then the fix is to manually drop in the necessary configuration based on the CNI selected. The way to find this is to actually do the install properly somewhere else and then see which files are written to that folder... I ended up running an ubuntu vm for this purpose. So follow the instructions here with the Ubuntu tabs. If you just want the configs I've put copies of the most common ones here in the repo. Also https://docs.projectcalico.org/v3.11/reference/faq#are-the-calico-manifests-compatible-with-coreos export OLD = /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds export NEW = /var/lib/kubelet/volumeplugins/nodeagent~uds sed -i -e \"s? $OLD ? $NEW ?g\" ~/calico/calico.yaml Calico: The hard way Because reasons... CoreOS doesn't seem to like Calico or allow the default configuration manifests to install to either /etc/cni/net.d or to /opt/cni/bin . # run as root sudo -s We will manually add the necessary binaries follwing the guidance in Install the plugin . export CALICO_VERSION = v3.11.0 curl -L -o /opt/cni/bin/calico https://github.com/projectcalico/cni-plugin/releases/download/ ${ CALICO_VERSION } /calico-amd64 chmod 755 /opt/cni/bin/calico curl -L -o /opt/cni/bin/calico-ipam https://github.com/projectcalico/cni-plugin/releases/download/ ${ CALICO_VERSION } /calico-ipam-amd64 chmod 755 /opt/cni/bin/calico-ipam Create the config directory mkdir -p /etc/cni/net.d/ Now manually create our config sudo tee /etc/cni/net.d/10-calico.conflist <<EOF { \"name\": \"k8s-pod-network\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"calico\", \"log_level\": \"info\", \"datastore_type\": \"kubernetes\", \"nodename\": \"test\", \"mtu\": 1440, \"ipam\": { \"type\": \"calico-ipam\" }, \"policy\": { \"type\": \"k8s\" }, \"kubernetes\": { \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\" } }, { \"type\": \"portmap\", \"snat\": true, \"capabilities\": {\"portMappings\": true} } ] } EOF","title":"Compute (Donaldson)"},{"location":"01.infrastructure/01.hosts/03.donaldson/#boot-into-rescue","text":"We need to set our cloud node to boot into a LiveCD rescue image so that we can then login and install CoreOS to our main disk.","title":"Boot into rescue"},{"location":"01.infrastructure/01.hosts/03.donaldson/#kimsufi-control-panel","text":"Login to the kimsufi control panel and select Netboot from the options against your server. Step through te options provided and select Rescue \u2192 rescue64-pro . Now restart the server and wait for an email confirming your temporary ssh login credentials.","title":"Kimsufi Control Panel"},{"location":"01.infrastructure/01.hosts/03.donaldson/#install-coreos","text":"","title":"Install CoreOS"},{"location":"01.infrastructure/01.hosts/03.donaldson/#installation","text":"SSH into the box with the temporary credentials and use the CoreOS installer script to overwrite and nuke current installation by OVH. Make sure we've transferred across our ignition file with ssh key and hostname etc. wget https://raw.github.com/coreos/init/master/bin/coreos-install ; \\ chmod +x coreos-install ; \\ sudo ./coreos-install -d /dev/sda -i ignition.json Once you see a Success! CoreOS Container Linux stable 2303.3.0 is installed on /dev/sda we can reboot (after changing the boot option in the control panel).","title":"Installation"},{"location":"01.infrastructure/01.hosts/03.donaldson/#kimsufi-control-panel_1","text":"Now go back to the control panel and change the boot to Hard disk before then rebooting.","title":"Kimsufi Control Panel"},{"location":"01.infrastructure/01.hosts/03.donaldson/#configure-kubernetes","text":"Follow the existing instructions, remembering to change for your hostname and also use kimsufi as region when labelling the node.","title":"Configure Kubernetes"},{"location":"01.infrastructure/01.hosts/03.donaldson/#node-stuck-on-not-ready","text":"If the node, even after applying the CNI, is still stuck on Not Ready check the following. output from systemctl status kubelet output from kubectl describe node <nodename> Chances are you have the same problem I ended up with originally of CoreOS being unable to find the correct CNI configuration file. Unable to update cni config: no networks found in /etc/cni/net.d If you're seeing this error in the output from systemctl then the fix is to manually drop in the necessary configuration based on the CNI selected. The way to find this is to actually do the install properly somewhere else and then see which files are written to that folder... I ended up running an ubuntu vm for this purpose. So follow the instructions here with the Ubuntu tabs. If you just want the configs I've put copies of the most common ones here in the repo. Also https://docs.projectcalico.org/v3.11/reference/faq#are-the-calico-manifests-compatible-with-coreos export OLD = /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds export NEW = /var/lib/kubelet/volumeplugins/nodeagent~uds sed -i -e \"s? $OLD ? $NEW ?g\" ~/calico/calico.yaml","title":"Node stuck on \"Not Ready\""},{"location":"01.infrastructure/01.hosts/03.donaldson/#calico-the-hard-way","text":"Because reasons... CoreOS doesn't seem to like Calico or allow the default configuration manifests to install to either /etc/cni/net.d or to /opt/cni/bin . # run as root sudo -s We will manually add the necessary binaries follwing the guidance in Install the plugin . export CALICO_VERSION = v3.11.0 curl -L -o /opt/cni/bin/calico https://github.com/projectcalico/cni-plugin/releases/download/ ${ CALICO_VERSION } /calico-amd64 chmod 755 /opt/cni/bin/calico curl -L -o /opt/cni/bin/calico-ipam https://github.com/projectcalico/cni-plugin/releases/download/ ${ CALICO_VERSION } /calico-ipam-amd64 chmod 755 /opt/cni/bin/calico-ipam Create the config directory mkdir -p /etc/cni/net.d/ Now manually create our config sudo tee /etc/cni/net.d/10-calico.conflist <<EOF { \"name\": \"k8s-pod-network\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"calico\", \"log_level\": \"info\", \"datastore_type\": \"kubernetes\", \"nodename\": \"test\", \"mtu\": 1440, \"ipam\": { \"type\": \"calico-ipam\" }, \"policy\": { \"type\": \"k8s\" }, \"kubernetes\": { \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\" } }, { \"type\": \"portmap\", \"snat\": true, \"capabilities\": {\"portMappings\": true} } ] } EOF","title":"Calico: The hard way"},{"location":"01.infrastructure/01.hosts/04.hamilton/","text":"","title":"Ingress (Hamilton)"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/","text":"In part 1 we setup and configured the physical nodes we are going to use for our kubernetes cluster, ensuring they could communicate between each other using a secure VPN. We'll now: install kubernetes implement a CNI (network solution for pod communication) add a load balancer so that services can obtain external IP addresses give your user them the ability to run commands on the cluster Install Kubernetes Docker As Kubernetes is an orchestration layer we will need to install a container runtime eninge. The below commands will install a compatible version of Docker. Kubernetes and Docker versions It's worth being aware that Kubernetes only supports specific versions of docker and, as a result, you should check for compatability in their changelog. At time of writing the latest stable version of Kubernetes was 1.16. The CHANGELOG-1.16 shows that they have validated the following docker versions: The list of validated docker versions remains unchanged. The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09 export K8S_DOCKER_VERSION = 18 .09 CoreOS # Not applicable Ubuntu sudo apt-get update ; \\ sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ p7zip-full \\ software-properties-common ; \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - ; \\ sudo add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" ; \\ sudo apt-get update && sudo apt-get install -y docker-ce = $( apt-cache madison docker-ce | grep ${ K8S_DOCKER_VERSION } | head -1 | awk '{print $3}' ) Network status With docker installed the network configuration and topology now looks like this. $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 7 links listed. $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 192 .168.0.1 0 .0.0.0 UG 100 0 0 bond0 172 .17.0.0 0 .0.0.0 255 .255.0.0 U 0 0 0 docker0 192 .168.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 bond0 192 .168.0.1 0 .0.0.0 255 .255.255.255 UH 100 0 0 bond0 Kubernetes With docker now setup as a runtime we'll install Kubernetes as well as jq (which will come in handy later). CoreOS # Install CNI plugins export CNI_VERSION = \"v0.8.2\" mkdir -p /opt/cni/bin ; \\ curl -L \"https://github.com/containernetworking/plugins/releases/download/ ${ CNI_VERSION } /cni-plugins-linux-amd64- ${ CNI_VERSION } .tgz\" | tar -C /opt/cni/bin -xz # Install crictl export CRICTL_VERSION = \"v1.16.0\" mkdir -p /opt/bin ; \\ curl -L \"https://github.com/kubernetes-sigs/cri-tools/releases/download/ ${ CRICTL_VERSION } /crictl- ${ CRICTL_VERSION } -linux-amd64.tar.gz\" | tar -C /opt/bin -xz # Install kubeadm, kubelet, kubectl and add a kubelet systemd service export RELEASE = \" $( curl -sSL https://dl.k8s.io/release/stable.txt ) \" mkdir -p /opt/bin ; \\ cd /opt/bin ; \\ curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/ ${ RELEASE } /bin/linux/amd64/ { kubeadm,kubelet,kubectl } ; \\ chmod +x { kubeadm,kubelet,kubectl } curl -sSL \"https://raw.githubusercontent.com/kubernetes/kubernetes/ ${ RELEASE } /build/debs/kubelet.service\" | sed \"s:/usr/bin:/opt/bin:g\" > /etc/systemd/system/kubelet.service ; \\ mkdir -p /etc/systemd/system/kubelet.service.d ; \\ curl -sSL \"https://raw.githubusercontent.com/kubernetes/kubernetes/ ${ RELEASE } /build/debs/10-kubeadm.conf\" | sed \"s:/usr/bin:/opt/bin:g\" > /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Enable and start kubelet systemctl enable --now kubelet Ubuntu sudo apt-get update && sudo apt-get install -y apt-transport-https ; \\ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - ; \\ sudo tee /etc/apt/sources.list.d/kubernetes.list <<EOF deb http://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl jq Pre-flight fixes Before intialising the cluster we need to change the following: modify the cgroup driver from cgroupfs to systemd disable swap enable the docker service CoreOS sudo systemctl enable --now docker.service # Setup docker daemon. sudo tee /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF CoreOS sudo systemctl daemon-reload sudo systemctl restart docker Ubuntu # Docker service sudo mkdir -p /etc/systemd/system/docker.service.d sudo systemctl daemon-reload sudo systemctl restart docker # The swap should be disabled both for the current session sudo swapoff -a # Now edit the `/etc/fstab` file to add a comment so it persists # across restarts sudo nano /etc/fstab Initialise Initialise the node Warning Ensure you can ping your hostname before proceeding as otherwise the initialisation will fail. If not, edit your /etc/hosts file and add an entry for 127.0.1.1 donaldson.local . sudo kubeadm config images pull ; \\ sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 Listening on a specific address If you wanted to you can set the api for kubernetes to listen only on a specific address. This is useful if, for instance, you have a VPN connection between nodes. An example of how to initialise this is below. In this instance we're getting the IPv4 address of the wireguard interface. # Bind to VPN address sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) You'll see a load of scrolling log text followed by the following indicating success and giving some next step instructions. Success Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192 .168.0.101:6443 --token 64we2d.5jzsjzpa0ysqzagl \\ --discovery-token-ca-cert-hash sha256:9d1dda6163e0e539588e0209f06b37d209d230a373b0167ea5f881cd60537178 Give your user the ability to run kubectl . mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config To test this works we can run a couple of kubernetes commands. $ kubectl get nodes NAME STATUS ROLES AGE VERSION banks.local NotReady master 15s v1.17.0 The node will show as NotReady until we add a container network interface (CNI) in the next step. Remove the Control plane node isolation taint The official docs highlight: By default, your cluster will not schedule pods on the control-plane node for security reasons. This will be a problem if we only have one node running (e.g. homelab development) so we will remove the taint that prevents this. kubectl taint nodes --all node-role.kubernetes.io/master- This will remove the node-role.kubernetes.io/master taint from any nodes that have it, including the control-plane node, meaning that the scheduler will then be able to schedule pods everywhere. Add node labels For later on we will add a topology.kubernetes.io/region label to the node as this is used to indicate failure domains for DR and also assists with the setup of VPN links. For more informatio see Running in multiple zones and Well-Known Labels, Annotations and Taints in the official kubernetes docs. kubectl label node banks.local topology.kubernetes.io/region = house ; \\ kubectl label node banks.local topology.kubernetes.io/zone = mancave ; \\ kubectl label node banks.local topology.rook.io/rack = dellboy Wiping your Kubernetes installation On the off-chance that you ever want to completely uninstall kubernetes and associated resources you can run the commands below to purge from the system. kubeadm reset ; \\ sudo apt-get purge -y kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get autoremove -y ; \\ rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd * off-chance being the polite way of saying \"this happens quite a lot when learning\"... Network status With kubernetes installed the network configuration and topology still look the same until we apply a CNI for the pods to communicate with each other.","title":"Install Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#install-kubernetes","text":"","title":"Install Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#docker","text":"As Kubernetes is an orchestration layer we will need to install a container runtime eninge. The below commands will install a compatible version of Docker. Kubernetes and Docker versions It's worth being aware that Kubernetes only supports specific versions of docker and, as a result, you should check for compatability in their changelog. At time of writing the latest stable version of Kubernetes was 1.16. The CHANGELOG-1.16 shows that they have validated the following docker versions: The list of validated docker versions remains unchanged. The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09 export K8S_DOCKER_VERSION = 18 .09 CoreOS # Not applicable Ubuntu sudo apt-get update ; \\ sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ p7zip-full \\ software-properties-common ; \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - ; \\ sudo add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" ; \\ sudo apt-get update && sudo apt-get install -y docker-ce = $( apt-cache madison docker-ce | grep ${ K8S_DOCKER_VERSION } | head -1 | awk '{print $3}' ) Network status With docker installed the network configuration and topology now looks like this. $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 7 links listed. $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 192 .168.0.1 0 .0.0.0 UG 100 0 0 bond0 172 .17.0.0 0 .0.0.0 255 .255.0.0 U 0 0 0 docker0 192 .168.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 bond0 192 .168.0.1 0 .0.0.0 255 .255.255.255 UH 100 0 0 bond0","title":"Docker"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#kubernetes","text":"With docker now setup as a runtime we'll install Kubernetes as well as jq (which will come in handy later). CoreOS # Install CNI plugins export CNI_VERSION = \"v0.8.2\" mkdir -p /opt/cni/bin ; \\ curl -L \"https://github.com/containernetworking/plugins/releases/download/ ${ CNI_VERSION } /cni-plugins-linux-amd64- ${ CNI_VERSION } .tgz\" | tar -C /opt/cni/bin -xz # Install crictl export CRICTL_VERSION = \"v1.16.0\" mkdir -p /opt/bin ; \\ curl -L \"https://github.com/kubernetes-sigs/cri-tools/releases/download/ ${ CRICTL_VERSION } /crictl- ${ CRICTL_VERSION } -linux-amd64.tar.gz\" | tar -C /opt/bin -xz # Install kubeadm, kubelet, kubectl and add a kubelet systemd service export RELEASE = \" $( curl -sSL https://dl.k8s.io/release/stable.txt ) \" mkdir -p /opt/bin ; \\ cd /opt/bin ; \\ curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/ ${ RELEASE } /bin/linux/amd64/ { kubeadm,kubelet,kubectl } ; \\ chmod +x { kubeadm,kubelet,kubectl } curl -sSL \"https://raw.githubusercontent.com/kubernetes/kubernetes/ ${ RELEASE } /build/debs/kubelet.service\" | sed \"s:/usr/bin:/opt/bin:g\" > /etc/systemd/system/kubelet.service ; \\ mkdir -p /etc/systemd/system/kubelet.service.d ; \\ curl -sSL \"https://raw.githubusercontent.com/kubernetes/kubernetes/ ${ RELEASE } /build/debs/10-kubeadm.conf\" | sed \"s:/usr/bin:/opt/bin:g\" > /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Enable and start kubelet systemctl enable --now kubelet Ubuntu sudo apt-get update && sudo apt-get install -y apt-transport-https ; \\ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - ; \\ sudo tee /etc/apt/sources.list.d/kubernetes.list <<EOF deb http://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl jq","title":"Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#pre-flight-fixes","text":"Before intialising the cluster we need to change the following: modify the cgroup driver from cgroupfs to systemd disable swap enable the docker service CoreOS sudo systemctl enable --now docker.service # Setup docker daemon. sudo tee /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF CoreOS sudo systemctl daemon-reload sudo systemctl restart docker Ubuntu # Docker service sudo mkdir -p /etc/systemd/system/docker.service.d sudo systemctl daemon-reload sudo systemctl restart docker # The swap should be disabled both for the current session sudo swapoff -a # Now edit the `/etc/fstab` file to add a comment so it persists # across restarts sudo nano /etc/fstab","title":"Pre-flight fixes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#initialise","text":"Initialise the node Warning Ensure you can ping your hostname before proceeding as otherwise the initialisation will fail. If not, edit your /etc/hosts file and add an entry for 127.0.1.1 donaldson.local . sudo kubeadm config images pull ; \\ sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 Listening on a specific address If you wanted to you can set the api for kubernetes to listen only on a specific address. This is useful if, for instance, you have a VPN connection between nodes. An example of how to initialise this is below. In this instance we're getting the IPv4 address of the wireguard interface. # Bind to VPN address sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) You'll see a load of scrolling log text followed by the following indicating success and giving some next step instructions. Success Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192 .168.0.101:6443 --token 64we2d.5jzsjzpa0ysqzagl \\ --discovery-token-ca-cert-hash sha256:9d1dda6163e0e539588e0209f06b37d209d230a373b0167ea5f881cd60537178 Give your user the ability to run kubectl . mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config To test this works we can run a couple of kubernetes commands. $ kubectl get nodes NAME STATUS ROLES AGE VERSION banks.local NotReady master 15s v1.17.0 The node will show as NotReady until we add a container network interface (CNI) in the next step.","title":"Initialise"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#remove-the-control-plane-node-isolation-taint","text":"The official docs highlight: By default, your cluster will not schedule pods on the control-plane node for security reasons. This will be a problem if we only have one node running (e.g. homelab development) so we will remove the taint that prevents this. kubectl taint nodes --all node-role.kubernetes.io/master- This will remove the node-role.kubernetes.io/master taint from any nodes that have it, including the control-plane node, meaning that the scheduler will then be able to schedule pods everywhere.","title":"Remove the Control plane node isolation taint"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#add-node-labels","text":"For later on we will add a topology.kubernetes.io/region label to the node as this is used to indicate failure domains for DR and also assists with the setup of VPN links. For more informatio see Running in multiple zones and Well-Known Labels, Annotations and Taints in the official kubernetes docs. kubectl label node banks.local topology.kubernetes.io/region = house ; \\ kubectl label node banks.local topology.kubernetes.io/zone = mancave ; \\ kubectl label node banks.local topology.rook.io/rack = dellboy Wiping your Kubernetes installation On the off-chance that you ever want to completely uninstall kubernetes and associated resources you can run the commands below to purge from the system. kubeadm reset ; \\ sudo apt-get purge -y kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get autoremove -y ; \\ rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd * off-chance being the polite way of saying \"this happens quite a lot when learning\"... Network status With kubernetes installed the network configuration and topology still look the same until we apply a CNI for the pods to communicate with each other.","title":"Add node labels"},{"location":"01.infrastructure/02.kubernetes/01.cni/","text":"Navigating to the addons link provided above will show that we've got some options available. First we will focus on the Networking and Network Policy where we will use Canal as a CNI (networking solution between pods). Canal unites Flannel and Calico, providing networking and network policy. Canal Most of these manifests can be found in the official GitHub repo if required. # Role-based access control (RBAC) # Kubernetes API datastore with flannel networking: # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/integration#role-based-access-control-rbac kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-flannel.yaml # Installing Calico for policy and flannel for networking # Installing with the Kubernetes API datastore (recommended) # We can install directly as we're using the pod CIDR 10.244.0.0/16 # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/canal.yaml Network Interfaces Checking the available network interfaces should now show something similar to the below. You'll have a number of cali* interfaces, each one per pod. $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 8 cali9cce5775874 ether degraded unmanaged 11 cali8bf6cbe9a38 ether degraded unmanaged 12 flannel.1 ether routable unmanaged 13 cali4010097f9ae ether degraded unmanaged 14 calia9e20d251e6 ether degraded unmanaged Pure Calico mkdir -p ~/calico ; \\ cd ~/calico ; \\ curl https://docs.projectcalico.org/v3.11/manifests/calico.yaml -O export POD_CIDR = 10 .244.0.0/16 ; \\ sed -i -e \"s?192.168.0.0/16? $POD_CIDR ?g\" calico.yaml kubectl apply -f ~/calico/calico.yaml Network Interfaces Checking the available network interfaces should now show something similar to the below. You'll have a number of cali* interfaces, each one per pod. The main difference is a tunl0 as opposed to flannel.1 . $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 8 caliba1d20a20a6 ether degraded unmanaged 11 tunl0 tunnel routable unmanaged 12 cali6b7a2c35969 ether degraded unmanaged 13 cali7c0e5e55068 ether degraded unmanaged 14 calid6b03898a0b ether degraded unmanaged 32 cali887554ea8bf ether degraded unmanaged","title":"CNI"},{"location":"01.infrastructure/02.kubernetes/01.cni/#canal","text":"Most of these manifests can be found in the official GitHub repo if required. # Role-based access control (RBAC) # Kubernetes API datastore with flannel networking: # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/integration#role-based-access-control-rbac kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-flannel.yaml # Installing Calico for policy and flannel for networking # Installing with the Kubernetes API datastore (recommended) # We can install directly as we're using the pod CIDR 10.244.0.0/16 # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/canal.yaml Network Interfaces Checking the available network interfaces should now show something similar to the below. You'll have a number of cali* interfaces, each one per pod. $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 8 cali9cce5775874 ether degraded unmanaged 11 cali8bf6cbe9a38 ether degraded unmanaged 12 flannel.1 ether routable unmanaged 13 cali4010097f9ae ether degraded unmanaged 14 calia9e20d251e6 ether degraded unmanaged","title":"Canal"},{"location":"01.infrastructure/02.kubernetes/01.cni/#pure-calico","text":"mkdir -p ~/calico ; \\ cd ~/calico ; \\ curl https://docs.projectcalico.org/v3.11/manifests/calico.yaml -O export POD_CIDR = 10 .244.0.0/16 ; \\ sed -i -e \"s?192.168.0.0/16? $POD_CIDR ?g\" calico.yaml kubectl apply -f ~/calico/calico.yaml Network Interfaces Checking the available network interfaces should now show something similar to the below. You'll have a number of cali* interfaces, each one per pod. The main difference is a tunl0 as opposed to flannel.1 . $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 8 caliba1d20a20a6 ether degraded unmanaged 11 tunl0 tunnel routable unmanaged 12 cali6b7a2c35969 ether degraded unmanaged 13 cali7c0e5e55068 ether degraded unmanaged 14 calid6b03898a0b ether degraded unmanaged 32 cali887554ea8bf ether degraded unmanaged","title":"Pure Calico"},{"location":"01.infrastructure/02.kubernetes/02.metallb/","text":"When you run Kubernetes services on supported cloud providers (GCP, AWS, Azure) obtaining an externally adressable endpoint for those you'd like to expose is abstracted away behind a call to a LoadBalancer in their infrastructre. This provisions an IP address and routes it to your service. As we're running on-prem or in our own cloud configuration we don't have this luxury. Luckily, someone at Google also likes bare metal... and so they created MetalLB as an alternative implementation of this functionality. MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols. Install MetalLB per the docs export METALLB_VERSION = v0.8.3 mkdir -p ~/metallb ; \\ cd ~/metallb wget https://raw.githubusercontent.com/google/metallb/ ${ METALLB_VERSION } /manifests/metallb.yaml ; \\ kubectl apply -f ~/metallb/metallb.yaml Create a configmap that allocates a valid address range on your local network which MetalLB will then hand out. # file: ~/metallb/config.yaml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 192.168.0.200-192.168.0.250 Apply this now with kubectl apply -f ~/metallb/config.yaml . Sample deployment We'll now deploy a simple whoami container and tell MetalLB to give it an external IP address from the cluster internal network. Further information on Service objects in kubernetes can be found in the official docs , specifically the Publishing Services (ServiceTypes) section. # file: ~/metallb/example/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : type : LoadBalancer selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Run kubectl apply -f ~/metallb/example/whoami.yaml to create the deployment and associated service. You should now be able to see a service whoami of type LoadBalancer being created and having an EXTERNAL-IP allocated to it by MetalLB. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 13h whoami LoadBalancer 10 .96.99.249 192 .168.0.201 80 :31314/TCP 8m47s You can query this as well with curl to confirm. $ curl 192 .168.0.201 Hostname: whoami-deployment-5b4bb9c787-fmb77 IP: 127 .0.0.1 IP: 192 .168.204.231 RemoteAddr: 192 .168.0.102:20516 GET / HTTP/1.1 Host: 192 .168.0.201 User-Agent: curl/7.58.0 Accept: */* The interesting pieces above are the IPs in the hops. IP: 127.0.0.1 : localhost IP: 192.168.204.231 : an address on the subnet of the Calico CNI tunl0 . You could access the pod directly using this as well if on the same network (pod to pod communication). RemoteAddr: 192.168.0.102:20516 : the client making the request (us, on the master node using curl). Host: 192.168.0.201 : the address allocated to the service by MetalLB, now accessible from anyone on the LAN. You can see some of this by running a describe on the pod with kubectl describe pod -l 'app=whoami' . This will show both the Node and assigned IP as well as the associated CNI annotation that allows Calico to route packets. Name : whoami-deployment-5b4bb9c787-fmb77 Namespace : default Priority : 0 Node : banks.local/172.17.0.1 Start Time : Thu, 12 Dec 2019 11:49:16 +0000 Labels : app=whoami pod-template-hash=5b4bb9c787 Annotations : cni.projectcalico.org/podIP : 192.168.204.231/32 Status : Running IP : 192.168.204.231 IPs : IP : 192.168.204.231 Controlled By : ReplicaSet/whoami-deployment-5b4bb9c787 Containers : whoami : Container ID : docker://5966c73d17c87df726b83db12a8227008dcfbccd4b8bd5cd0fda48bca153f415 Image : containous/whoami Image ID : docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b Port : 80/TCP Host Port : 0/TCP State : Running Started : Thu, 12 Dec 2019 11:49:19 +0000 Ready : True Restart Count : 0 Environment : <none> Mounts : /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro) Conditions : Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes : default-token-k9gvj : Type : Secret (a volume populated by a Secret) SecretName : default-token-k9gvj Optional : false QoS Class : BestEffort Node-Selectors : <none> Tolerations : node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 10m default-scheduler Successfully assigned default/whoami-deployment-5b4bb9c787-fmb77 to banks.local Normal Pulling 10m kubelet, banks.local Pulling image \"containous/whoami\" Normal Pulled 10m kubelet, banks.local Successfully pulled image \"containous/whoami\" Normal Created 10m kubelet, banks.local Created container whoami Normal Started 10m kubelet, banks.local Started container whoami Reviewing the service with kubectl describe svc whoami gives similarly useful information. Name : whoami Namespace : default Labels : <none> Annotations : kubectl.kubernetes.io/last-applied-configuration : { \"apiVersion\" : \"v1\" , \"kind\" : \"Service\" , \"metadata\" :{ \"annotations\" :{}, \"name\" : \"whoami\" , \"namespace\" : \"default\" }, \"spec\" :{ \"ports\" :[{ \"port\" : 80 , \"proto... Selector: app=whoami Type: LoadBalancer IP: 10.96.99.249 LoadBalancer Ingress: 192.168.0.201 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 31314/TCP Endpoints: 192.168.204.231:80 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 15m metallb-controller Assigned IP \" 192.168.0.201\" Normal nodeAssigned 15m metallb-speaker announcing from node \"banks.local\" Tear down Remove the deployment and example service with kubectl delete -f ~/metallb/example/whoami.yaml","title":"Load Balancer"},{"location":"01.infrastructure/02.kubernetes/02.metallb/#sample-deployment","text":"We'll now deploy a simple whoami container and tell MetalLB to give it an external IP address from the cluster internal network. Further information on Service objects in kubernetes can be found in the official docs , specifically the Publishing Services (ServiceTypes) section. # file: ~/metallb/example/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : type : LoadBalancer selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Run kubectl apply -f ~/metallb/example/whoami.yaml to create the deployment and associated service. You should now be able to see a service whoami of type LoadBalancer being created and having an EXTERNAL-IP allocated to it by MetalLB. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 13h whoami LoadBalancer 10 .96.99.249 192 .168.0.201 80 :31314/TCP 8m47s You can query this as well with curl to confirm. $ curl 192 .168.0.201 Hostname: whoami-deployment-5b4bb9c787-fmb77 IP: 127 .0.0.1 IP: 192 .168.204.231 RemoteAddr: 192 .168.0.102:20516 GET / HTTP/1.1 Host: 192 .168.0.201 User-Agent: curl/7.58.0 Accept: */* The interesting pieces above are the IPs in the hops. IP: 127.0.0.1 : localhost IP: 192.168.204.231 : an address on the subnet of the Calico CNI tunl0 . You could access the pod directly using this as well if on the same network (pod to pod communication). RemoteAddr: 192.168.0.102:20516 : the client making the request (us, on the master node using curl). Host: 192.168.0.201 : the address allocated to the service by MetalLB, now accessible from anyone on the LAN. You can see some of this by running a describe on the pod with kubectl describe pod -l 'app=whoami' . This will show both the Node and assigned IP as well as the associated CNI annotation that allows Calico to route packets. Name : whoami-deployment-5b4bb9c787-fmb77 Namespace : default Priority : 0 Node : banks.local/172.17.0.1 Start Time : Thu, 12 Dec 2019 11:49:16 +0000 Labels : app=whoami pod-template-hash=5b4bb9c787 Annotations : cni.projectcalico.org/podIP : 192.168.204.231/32 Status : Running IP : 192.168.204.231 IPs : IP : 192.168.204.231 Controlled By : ReplicaSet/whoami-deployment-5b4bb9c787 Containers : whoami : Container ID : docker://5966c73d17c87df726b83db12a8227008dcfbccd4b8bd5cd0fda48bca153f415 Image : containous/whoami Image ID : docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b Port : 80/TCP Host Port : 0/TCP State : Running Started : Thu, 12 Dec 2019 11:49:19 +0000 Ready : True Restart Count : 0 Environment : <none> Mounts : /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro) Conditions : Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes : default-token-k9gvj : Type : Secret (a volume populated by a Secret) SecretName : default-token-k9gvj Optional : false QoS Class : BestEffort Node-Selectors : <none> Tolerations : node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 10m default-scheduler Successfully assigned default/whoami-deployment-5b4bb9c787-fmb77 to banks.local Normal Pulling 10m kubelet, banks.local Pulling image \"containous/whoami\" Normal Pulled 10m kubelet, banks.local Successfully pulled image \"containous/whoami\" Normal Created 10m kubelet, banks.local Created container whoami Normal Started 10m kubelet, banks.local Started container whoami Reviewing the service with kubectl describe svc whoami gives similarly useful information. Name : whoami Namespace : default Labels : <none> Annotations : kubectl.kubernetes.io/last-applied-configuration : { \"apiVersion\" : \"v1\" , \"kind\" : \"Service\" , \"metadata\" :{ \"annotations\" :{}, \"name\" : \"whoami\" , \"namespace\" : \"default\" }, \"spec\" :{ \"ports\" :[{ \"port\" : 80 , \"proto... Selector: app=whoami Type: LoadBalancer IP: 10.96.99.249 LoadBalancer Ingress: 192.168.0.201 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 31314/TCP Endpoints: 192.168.204.231:80 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 15m metallb-controller Assigned IP \" 192.168.0.201\" Normal nodeAssigned 15m metallb-speaker announcing from node \"banks.local\"","title":"Sample deployment"},{"location":"01.infrastructure/02.kubernetes/02.metallb/#tear-down","text":"Remove the deployment and example service with kubectl delete -f ~/metallb/example/whoami.yaml","title":"Tear down"},{"location":"01.infrastructure/02.kubernetes/03.ingress/","text":"As described in the official kubernetes docs an Ingress object manages external access to services in a cluster, typically HTTP. They can provide load balancing, SSL termination and name-based virtual hosting. The easiest way to get setup with one is via Helm and we'll use the Nginx ingress for the moment. Install helm CoreOS export HELM_VERSION = v3.0.2 wget -O \"helm.tar.gz\" https://get.helm.sh/helm- ${ HELM_VERSION } -linux-amd64.tar.gz tar -xvzf helm.tar.gz chmod +x linux-amd64/helm sudo mv linux-amd64/helm /opt/bin/ rm -rf helm.tar.gz linux-amd64 Ubuntu # Live on the edge curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Add the stable repo helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Now install the ingress. helm install my-ingress stable/nginx-ingress \\ --set controller.kind = DaemonSet \\ --set controller.service.type = LoadBalancer \\ --set controller.hostNetwork = true To check that all pods are running. $ kubectl get pod -l 'app=nginx-ingress' NAME READY STATUS RESTARTS AGE my-ingress-nginx-ingress-controller-n7mt6 1 /1 Running 0 3m17s my-ingress-nginx-ingress-default-backend-7469774fb6-wlxjh 1 /1 Running 0 3m17s As we deployed the ingress with both DaemonSet and hostNetwork=true these controllers will exist on every node in the cluster and be listening on the host ports of 80 and 443 . Editing one will show this. $ kubectl edit pod my-ingress-nginx-ingress-controller-n7mt6 ... affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchFields: - key: metadata.name operator: In values: - banks.local ... ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP - containerPort: 443 hostPort: 443 name: https protocol: TCP Deploy sample application Let's try to expose an application now via this ingress controlller. mkdir -p ~/ingress ; \\ cd ~/ingress First, create the deployment and associated service. Notice how we don't use LoadBalancer as the type. # file: ~/ingress/example/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : # type: LoadBalancer selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Apply with kubectl apply -f ~/ingress/example/whoami.yaml . We can now configure the ingress controller to route requests to this service. We'll use localhost for the moment to avoid any DNS setting requirements and just prove the routing works. As you can see from the comment this should change to the fqdn of the service ultimately. # file: ~/ingress/example/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami-ingress namespace : default annotations : kubernete.io/ingress.class : nginx spec : rules : - host : localhost # whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 Apply with kubectl apply -f ~/ingress/example/whoami-ingress.yaml . Now, running a curl from the node should give you something similar to the following. $ curl localhost Hostname: whoami-deployment-5b4bb9c787-zsv2h IP: 127 .0.0.1 IP: 10 .244.204.200 RemoteAddr: 192 .168.0.104:45478 GET / HTTP/1.1 Host: localhost User-Agent: curl/7.58.0 Accept: */* X-Forwarded-For: 127 .0.0.1 X-Forwarded-Host: localhost X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 127 .0.0.1 X-Request-Id: 678701531703cebe524d5803dd28d08b X-Scheme: http Teardown Remove the ingress resource and whoami application. kubectl delete -f ~/ingress/example/whoami-ingress.yaml ; \\ kubectl delete -f ~/ingress/example/whoami.yaml","title":"Ingress"},{"location":"01.infrastructure/02.kubernetes/03.ingress/#deploy-sample-application","text":"Let's try to expose an application now via this ingress controlller. mkdir -p ~/ingress ; \\ cd ~/ingress First, create the deployment and associated service. Notice how we don't use LoadBalancer as the type. # file: ~/ingress/example/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : # type: LoadBalancer selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Apply with kubectl apply -f ~/ingress/example/whoami.yaml . We can now configure the ingress controller to route requests to this service. We'll use localhost for the moment to avoid any DNS setting requirements and just prove the routing works. As you can see from the comment this should change to the fqdn of the service ultimately. # file: ~/ingress/example/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami-ingress namespace : default annotations : kubernete.io/ingress.class : nginx spec : rules : - host : localhost # whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 Apply with kubectl apply -f ~/ingress/example/whoami-ingress.yaml . Now, running a curl from the node should give you something similar to the following. $ curl localhost Hostname: whoami-deployment-5b4bb9c787-zsv2h IP: 127 .0.0.1 IP: 10 .244.204.200 RemoteAddr: 192 .168.0.104:45478 GET / HTTP/1.1 Host: localhost User-Agent: curl/7.58.0 Accept: */* X-Forwarded-For: 127 .0.0.1 X-Forwarded-Host: localhost X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 127 .0.0.1 X-Request-Id: 678701531703cebe524d5803dd28d08b X-Scheme: http","title":"Deploy sample application"},{"location":"01.infrastructure/02.kubernetes/03.ingress/#teardown","text":"Remove the ingress resource and whoami application. kubectl delete -f ~/ingress/example/whoami-ingress.yaml ; \\ kubectl delete -f ~/ingress/example/whoami.yaml","title":"Teardown"},{"location":"01.infrastructure/03.certificates/00.cert.manager/","text":"We're going to use the OpenSource Cert-Manager from JetStack to automate TLS within the cluster. cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources. It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry. The quickstart on Kubernetes guide is a good place to start. Wherever possible we'll look to use the same proposed default settings. Install with regular manifests With helm currently going through some significant changes with the move from v2 to v3 we'll stick with deploying cert-manager with regular manifests for now. Now download and apply the latest manifest. mkdir ~/cert-manager ; \\ cd ~/cert-manager export CERT_MANAGER_VERSION = v0.12.0 ; \\ wget https://github.com/jetstack/cert-manager/releases/download/ ${ CERT_MANAGER_VERSION } /cert-manager.yaml ; kubectl create -f ~/cert-manager/cert-manager.yaml Verify the Installation $ kubectl -n cert-manager get pods NAME READY STATUS RESTARTS AGE cert-manager-5c47f46f57-ldb5v 1 /1 Running 0 39s cert-manager-cainjector-6659d6844d-25mqs 1 /1 Running 0 39s cert-manager-webhook-547567b88f-kvw28 1 /1 Running 0 39s We should have three pods running cert-manager , cert-manager-cainjector , and cert-manager-webhook . DNS With an ingress now available, login to your DNS provider (in my case Cloudflare) and point an A record for whoami to the IP address of the first node. Wildcard DNS An alternative option (which is what I've gone for below) is to point your root A record at the IP address (e.g. jamesveitch.dev) and then add a * wildcard CNAME entry which points at the root . This way any arbitrary subdomain (e.g. myapp.jamesveitch.dev) that isn't specifically found as a standalone entry will route straight to wherever the root is pointed. The disadvantage of this is that anything not specifically highlighted as a standalone entry will not be proxied through their CDN (see the cloud). We will fix this later though with ExternalDNS on Kubernetes. For now it's fine. NB: I'm using a CNAME above for the root which points to an anondns.net address. This is so I can use a dynamic IP from home. We'll replace this with an A record and static IP later when we turn on the cloud node. If you want to stick with a single node at home though you can use my AnonDNS updater docker image to keep your home IP registered for free with anondns. We'll now deploy a simple whoami container and tell the ingress node how to route to it. # file: ~/cert-manager/example/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Create the container deployment and service with kubectl create -f ~/cert-manager/example/whoami.yaml . In order to route traffic to this container though we now need to create an ingress resource. # file: ~/cert-manager/example/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami annotations : kubernete.io/ingress.class : nginx spec : rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 Apply this with kubectl apply -f ~/cert-manager/example/whoami-ingress.yaml . (Note we use apply instead of create so we can edit for the TLS later). To check this is working we can curl the address. $ curl whoami.jamesveitch.dev Hostname: whoami-deployment-5b4bb9c787-xfkm2 IP: 127 .0.0.1 IP: 10 .244.0.104 RemoteAddr: 192 .168.0.99:19850 GET / HTTP/1.1 Host: whoami.jamesveitch.dev User-Agent: curl/7.58.0 Accept: */* X-Forwarded-For: 82 .19.212.223 X-Forwarded-Host: whoami.jamesveitch.dev X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 82 .19.212.223 X-Request-Id: 0747f4ec40d196f54151e48014f50383 X-Scheme: http Cloudflare CDN If you've used a manually created entry for whoami (as opposed to the wildcard) you can click the cloud icon in the Cloudflare DNS page to use their proxy/CDN. This will give you a slightly different output to the above with a couple of additional header keys indicating you're using their CDN. ... Cdn-Loop: cloudflare ... X-Original-Forwarded-For: 82 .19.222.223 Configuring an Issuer As per the official architecture diagram below Kubernetes has the concept of Issuers . Once issued, certificates are then stored in Kubernetes secrets. We're particularly interested in LetsEncrypt and Vault . We'll start off using the http challenge (which is generic for all providers) and then, later, move to dns for Cloudflare . # file: ~/cert-manager/letsencrypt-staging.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL and email address for ACME registration server : https://acme-staging-v02.api.letsencrypt.org/directory email : lol@cats.com # Name of the secret to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging-key solvers : # Enable HTTP01 validations - http01 : ingress : class : nginx kubectl create -f ~/cert-manager/letsencrypt-staging.yaml Obtaining a Certificate for an app We now need to create the equivalent of a certificate request for the whoami container. The highlighted line shows we're asking CertManager to use the staging issuer we configured earlier. We need to specify it's a cluster issuer (as opposed to a local namespace). # file: ~/cert-manager/example/whoami-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : whoami namespace : default spec : # Secret names are always required. secretName : whoami-jamesveitch-dev-tls duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : whoami.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - whoami.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt-staging # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io Request the certificate with a kubectl apply -f ~/cert-manager/whoami-certificate.yaml and then check it's been obtained. $ kubectl get certificates NAME READY SECRET AGE whoami True whoami-jamesveitch-dev-tls 28s We'll modify the ingress now to uncomment the following lines so that nginx knows to use the ssl cert. We'll also (optional) provide multiple routes to connect to the application via: whoami.jamesveitch.dev (using the CDN) as a proxied DNS record; or jamesveitch.dev/whoami (using the apex). # file: ~/cert-manager/example/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami annotations : kubernete.io/ingress.class : nginx nginx.ingress.kubernetes.io/rewrite-target : / cert-manager.io/issuer : \"letsencrypt-staging\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - secretName : whoami-jamesveitch-dev-tls hosts : - whoami.jamesveitch.dev rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 - host : jamesveitch.dev http : paths : - path : /whoami backend : serviceName : whoami servicePort : 80 Then apply these changes with a kubectl apply -f ~/cert-manager/whoami-ingress.yaml You may need to clear your cache but, navigating to the website should now redirect you automatically to https and present you with a LetsEncrypt certificate (albeit an untrusted one from the staging server) as opposed to the previous Kubernetes Ingress Controller Fake Certificate . # basic http gets a permanent redirect to https $ curl whoami.jamesveitch.dev <html> <head><title>308 Permanent Redirect</title></head> <body> <center><h1>308 Permanent Redirect</h1></center> <hr><center>openresty/1.15.8.2</center> </body> </html> Cloudflare CDN and too many redirects If you use the cloudflare proxy/cdn in your DNS then you can run into a too many redirects issue as a result of your TLS/SSL settings. This is because unencrypted traffic will enter the CDN; bounce around a number of nodes; and then get redirected straight back again through the same process to be encrypted. This need to be Full at a minimum. Once you're using production LetsEncrypt you could upgrade this further to Full (strict) if wanted. As with most things in life there's a good StackOverflow answer that explains in a bit more detail. Off : No visitors will be able to view your site over HTTPS; they will be redirected to HTTP. Flexible SSL : You cannot configure HTTPS support on your origin, even with a certificate that is not valid for your site. Visitors will be able to access your site over HTTPS, but connections to your origin will be made over HTTP. Note: You may encounter a redirect loop with some origin configurations. Full SSL : Your origin supports HTTPS, but the certificate installed does not match your domain or is self-signed. Cloudflare will connect to your origin over HTTPS, but will not validate the certificate. Full (strict) : Your origin has a valid certificate (not expired and signed by a trusted CA or Cloudflare Origin CA) installed. Cloudflare will connect over HTTPS and verify the cert on each request. # show https $ curl --insecure https://whoami.jamesveitch.dev Hostname: whoami-deployment-5b4bb9c787-bspd6 IP: 127 .0.0.1 IP: 10 .244.204.204 RemoteAddr: 192 .168.0.104:45694 GET / HTTP/1.1 Host: whoami.jamesveitch.dev User-Agent: curl/7.58.0 Accept: */* Accept-Encoding: gzip Cdn-Loop: cloudflare Cf-Connecting-Ip: 82 .19.222.223 Cf-Ipcountry: GB Cf-Ray: 5479b0729dc3ce53-LHR Cf-Visitor: { \"scheme\" : \"https\" } X-Forwarded-For: 162 .158.154.251 X-Forwarded-Host: whoami.jamesveitch.dev X-Forwarded-Port: 443 X-Forwarded-Proto: https X-Original-Forwarded-For: 82 .19.222.223 X-Real-Ip: 162 .158.154.251 X-Request-Id: 226c8aebc0f065303e9ece36b1433e5b X-Scheme: https LetsEncrypt Production Server With everything now setup and working we'll replace our staging implementation of LetsEncrypt with their production service such that our certificates are trusted by default in browsers. We need to create a new Issuer for LetsEncrypt and then change both the cert-request and ingress configurations. # file: ~/cert-manager/letsencrypt.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt namespace : default spec : acme : # The ACME server URL and email address for ACME registration server : https://acme-v02.api.letsencrypt.org/directory email : lol@cats.com # Name of the secret to store the ACME account private key privateKeySecretRef : name : letsencrypt-key solvers : # Enable HTTP01 validations - http01 : ingress : class : nginx kubectl create -f ~/cert-manager/letsencrypt.yaml We'll now need to modify the original certificate request to use the production service and then delete the staging certificate so it can be recreated. # file: ~/cert-manager/whoami-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : whoami namespace : default spec : # Secret names are always required. secretName : whoami-jamesveitch-dev-tls duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : whoami.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - whoami.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io kubectl delete certificate whoami ; \\ kubectl apply -f ~/cert-manager/whoami-certificate.yaml ; \\ watch -n 1 kubectl get certificate whoami NB: If you're using the Cloudflare CDN you might see their certificate instead.","title":"TLS Certificates Management"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#install-with-regular-manifests","text":"With helm currently going through some significant changes with the move from v2 to v3 we'll stick with deploying cert-manager with regular manifests for now. Now download and apply the latest manifest. mkdir ~/cert-manager ; \\ cd ~/cert-manager export CERT_MANAGER_VERSION = v0.12.0 ; \\ wget https://github.com/jetstack/cert-manager/releases/download/ ${ CERT_MANAGER_VERSION } /cert-manager.yaml ; kubectl create -f ~/cert-manager/cert-manager.yaml","title":"Install with regular manifests"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#verify-the-installation","text":"$ kubectl -n cert-manager get pods NAME READY STATUS RESTARTS AGE cert-manager-5c47f46f57-ldb5v 1 /1 Running 0 39s cert-manager-cainjector-6659d6844d-25mqs 1 /1 Running 0 39s cert-manager-webhook-547567b88f-kvw28 1 /1 Running 0 39s We should have three pods running cert-manager , cert-manager-cainjector , and cert-manager-webhook .","title":"Verify the Installation"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#dns","text":"With an ingress now available, login to your DNS provider (in my case Cloudflare) and point an A record for whoami to the IP address of the first node. Wildcard DNS An alternative option (which is what I've gone for below) is to point your root A record at the IP address (e.g. jamesveitch.dev) and then add a * wildcard CNAME entry which points at the root . This way any arbitrary subdomain (e.g. myapp.jamesveitch.dev) that isn't specifically found as a standalone entry will route straight to wherever the root is pointed. The disadvantage of this is that anything not specifically highlighted as a standalone entry will not be proxied through their CDN (see the cloud). We will fix this later though with ExternalDNS on Kubernetes. For now it's fine. NB: I'm using a CNAME above for the root which points to an anondns.net address. This is so I can use a dynamic IP from home. We'll replace this with an A record and static IP later when we turn on the cloud node. If you want to stick with a single node at home though you can use my AnonDNS updater docker image to keep your home IP registered for free with anondns. We'll now deploy a simple whoami container and tell the ingress node how to route to it. # file: ~/cert-manager/example/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Create the container deployment and service with kubectl create -f ~/cert-manager/example/whoami.yaml . In order to route traffic to this container though we now need to create an ingress resource. # file: ~/cert-manager/example/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami annotations : kubernete.io/ingress.class : nginx spec : rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 Apply this with kubectl apply -f ~/cert-manager/example/whoami-ingress.yaml . (Note we use apply instead of create so we can edit for the TLS later). To check this is working we can curl the address. $ curl whoami.jamesveitch.dev Hostname: whoami-deployment-5b4bb9c787-xfkm2 IP: 127 .0.0.1 IP: 10 .244.0.104 RemoteAddr: 192 .168.0.99:19850 GET / HTTP/1.1 Host: whoami.jamesveitch.dev User-Agent: curl/7.58.0 Accept: */* X-Forwarded-For: 82 .19.212.223 X-Forwarded-Host: whoami.jamesveitch.dev X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 82 .19.212.223 X-Request-Id: 0747f4ec40d196f54151e48014f50383 X-Scheme: http Cloudflare CDN If you've used a manually created entry for whoami (as opposed to the wildcard) you can click the cloud icon in the Cloudflare DNS page to use their proxy/CDN. This will give you a slightly different output to the above with a couple of additional header keys indicating you're using their CDN. ... Cdn-Loop: cloudflare ... X-Original-Forwarded-For: 82 .19.222.223","title":"DNS"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#configuring-an-issuer","text":"As per the official architecture diagram below Kubernetes has the concept of Issuers . Once issued, certificates are then stored in Kubernetes secrets. We're particularly interested in LetsEncrypt and Vault . We'll start off using the http challenge (which is generic for all providers) and then, later, move to dns for Cloudflare . # file: ~/cert-manager/letsencrypt-staging.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL and email address for ACME registration server : https://acme-staging-v02.api.letsencrypt.org/directory email : lol@cats.com # Name of the secret to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging-key solvers : # Enable HTTP01 validations - http01 : ingress : class : nginx kubectl create -f ~/cert-manager/letsencrypt-staging.yaml","title":"Configuring an Issuer"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#obtaining-a-certificate-for-an-app","text":"We now need to create the equivalent of a certificate request for the whoami container. The highlighted line shows we're asking CertManager to use the staging issuer we configured earlier. We need to specify it's a cluster issuer (as opposed to a local namespace). # file: ~/cert-manager/example/whoami-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : whoami namespace : default spec : # Secret names are always required. secretName : whoami-jamesveitch-dev-tls duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : whoami.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - whoami.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt-staging # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io Request the certificate with a kubectl apply -f ~/cert-manager/whoami-certificate.yaml and then check it's been obtained. $ kubectl get certificates NAME READY SECRET AGE whoami True whoami-jamesveitch-dev-tls 28s We'll modify the ingress now to uncomment the following lines so that nginx knows to use the ssl cert. We'll also (optional) provide multiple routes to connect to the application via: whoami.jamesveitch.dev (using the CDN) as a proxied DNS record; or jamesveitch.dev/whoami (using the apex). # file: ~/cert-manager/example/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami annotations : kubernete.io/ingress.class : nginx nginx.ingress.kubernetes.io/rewrite-target : / cert-manager.io/issuer : \"letsencrypt-staging\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - secretName : whoami-jamesveitch-dev-tls hosts : - whoami.jamesveitch.dev rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 - host : jamesveitch.dev http : paths : - path : /whoami backend : serviceName : whoami servicePort : 80 Then apply these changes with a kubectl apply -f ~/cert-manager/whoami-ingress.yaml You may need to clear your cache but, navigating to the website should now redirect you automatically to https and present you with a LetsEncrypt certificate (albeit an untrusted one from the staging server) as opposed to the previous Kubernetes Ingress Controller Fake Certificate . # basic http gets a permanent redirect to https $ curl whoami.jamesveitch.dev <html> <head><title>308 Permanent Redirect</title></head> <body> <center><h1>308 Permanent Redirect</h1></center> <hr><center>openresty/1.15.8.2</center> </body> </html> Cloudflare CDN and too many redirects If you use the cloudflare proxy/cdn in your DNS then you can run into a too many redirects issue as a result of your TLS/SSL settings. This is because unencrypted traffic will enter the CDN; bounce around a number of nodes; and then get redirected straight back again through the same process to be encrypted. This need to be Full at a minimum. Once you're using production LetsEncrypt you could upgrade this further to Full (strict) if wanted. As with most things in life there's a good StackOverflow answer that explains in a bit more detail. Off : No visitors will be able to view your site over HTTPS; they will be redirected to HTTP. Flexible SSL : You cannot configure HTTPS support on your origin, even with a certificate that is not valid for your site. Visitors will be able to access your site over HTTPS, but connections to your origin will be made over HTTP. Note: You may encounter a redirect loop with some origin configurations. Full SSL : Your origin supports HTTPS, but the certificate installed does not match your domain or is self-signed. Cloudflare will connect to your origin over HTTPS, but will not validate the certificate. Full (strict) : Your origin has a valid certificate (not expired and signed by a trusted CA or Cloudflare Origin CA) installed. Cloudflare will connect over HTTPS and verify the cert on each request. # show https $ curl --insecure https://whoami.jamesveitch.dev Hostname: whoami-deployment-5b4bb9c787-bspd6 IP: 127 .0.0.1 IP: 10 .244.204.204 RemoteAddr: 192 .168.0.104:45694 GET / HTTP/1.1 Host: whoami.jamesveitch.dev User-Agent: curl/7.58.0 Accept: */* Accept-Encoding: gzip Cdn-Loop: cloudflare Cf-Connecting-Ip: 82 .19.222.223 Cf-Ipcountry: GB Cf-Ray: 5479b0729dc3ce53-LHR Cf-Visitor: { \"scheme\" : \"https\" } X-Forwarded-For: 162 .158.154.251 X-Forwarded-Host: whoami.jamesveitch.dev X-Forwarded-Port: 443 X-Forwarded-Proto: https X-Original-Forwarded-For: 82 .19.222.223 X-Real-Ip: 162 .158.154.251 X-Request-Id: 226c8aebc0f065303e9ece36b1433e5b X-Scheme: https","title":"Obtaining a Certificate for an app"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#letsencrypt-production-server","text":"With everything now setup and working we'll replace our staging implementation of LetsEncrypt with their production service such that our certificates are trusted by default in browsers. We need to create a new Issuer for LetsEncrypt and then change both the cert-request and ingress configurations. # file: ~/cert-manager/letsencrypt.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt namespace : default spec : acme : # The ACME server URL and email address for ACME registration server : https://acme-v02.api.letsencrypt.org/directory email : lol@cats.com # Name of the secret to store the ACME account private key privateKeySecretRef : name : letsencrypt-key solvers : # Enable HTTP01 validations - http01 : ingress : class : nginx kubectl create -f ~/cert-manager/letsencrypt.yaml We'll now need to modify the original certificate request to use the production service and then delete the staging certificate so it can be recreated. # file: ~/cert-manager/whoami-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : whoami namespace : default spec : # Secret names are always required. secretName : whoami-jamesveitch-dev-tls duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : whoami.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - whoami.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io kubectl delete certificate whoami ; \\ kubectl apply -f ~/cert-manager/whoami-certificate.yaml ; \\ watch -n 1 kubectl get certificate whoami NB: If you're using the Cloudflare CDN you might see their certificate instead.","title":"LetsEncrypt Production Server"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/","text":"In part 2 we installed Kubernetes and setup a user (in my case adminlocal ) on our cluster with the ability to run administrative kubernetes commands. etcd (key/value store), Rook, Promethues and Vault are all examples of technologies we will be using in our cluster and are deployed using Kubernetes Operators . In this section we'll be deploying the Rook storage orchestrator with Ceph as a storage provider. Important This is not the final configuration I've used for my own storage server but is a good way to generically start with your own cluster setup. I suggest running through this first and then looking back at what I've done in the following areas. This section is all valid, I have though specifcially tailored my storage and cluster definitions to take into account requirements for lifecycle management (using cache-tiering) and device classes as well as local encryption (using devicePathFilter to specify the LUKS devices): * Setting up the physical host ( clarke ) * Specific manifests I've used: * Cluster CRD * Cache Tierieing Storage Pools Components Rook Rook allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider. More information on the architecure can be found in the docs Ceph Ceph provides three types of storage: object : compatible with S3 API file : files and directories (incl. NFS); and block : replicate a hard drive There a 4 key components of the architecture to be aware of (shown above in the Rook diagram): Monitor (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication Manager daemon: keeps track of state in cluster and metrics OSDs (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage. MSDs (Metatdata Server): concerned with filesystem storage type only Under the hood everything is stored as an object in logical storage pools. Installation and Setup Deployment of the Rook operator See the Rook quickstart We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple mon components onto a single node. Similar to when we removed the the control plane node taint Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes). # create a working directory mkdir -p ~/rook && \\ cd ~/rook # download the sample files # all of these can be found here: https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph export ROOK_VERSION = 1 .2 wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/common.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/operator.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/cluster.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/toolbox.yaml # modify the cluster spec # - allow multiple mons per node sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml In addition, because we've setup our encrypted data storage to be mounted at /data/${BLKID} we will edit the default storage options to remove the useAllDevices selection and, instead, specify the directories. See the docs for more details. # set default storage location and remove default `useAllDevices: true` sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml # any devices starting with 'sd' (but not sda as that's our root filesystem) sed -i.bak 's/deviceFilter:/deviceFilter: ^sd[^a]/' cluster.yaml # encrypt them with LUKS # see conversation https://github.com/rook/rook/issues/923#issuecomment-557651052 sed -i.bak 's/# encryptedDevice: \"true\"/encryptedDevice: \"true\"/' cluster.yaml Wiping disks for usage Because Rook will fail if it finds an existing \"in use\" filesystem or disk we need to wipe the disks in the host which we want to use. For example. In use disks below (from a previous cluster). in use $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk \u2514\u2500ceph--04176411--2495--4944--abae--4ac6cedd8b46-osd--data--20a17864--c8d4--48f7--8aa0--e9d393fbb9e4 253 :1 0 1 .8T 0 lvm sdc 8 :32 0 1 .8T 0 disk \u2514\u2500ceph--38ed1893--7746--477a--a8c8--dae66f8360e5-osd--data--0940f2cf--d1c2--489f--9c8a--31d75d759be8 253 :0 0 1 .8T 0 lvm sdd 8 :48 0 2T 0 disk sde 8 :64 0 2T 0 disk sr0 11 :0 1 1024M 0 rom We can delete and wipe the disk of partition maps. # replicate the rook commands (and use regex to exclude sda like the manifest) export DISKS = $( lsblk --all --noheadings --list --output KNAME | grep sd [ ^a ] ) ; \\ for d in $DISKS ; do \\ export DISK = /dev/ $d ; \\ sudo wipefs $DISK ; \\ sudo vgremove -y $( sudo pvscan | grep $DISK | awk '{print $4}' ) ; \\ sudo dd if = /dev/zero of = $DISK bs = 512 count = 10 ; \\ done Now, if appropriate, remove the LVM with the appropriate combination of pvremove , lvremove , vgremove etc. $ sudo vgremove -y $( sudo pvscan | grep $DISK | awk '{print $4}' ) Logical volume \"osd-data-20a17864-c8d4-48f7-8aa0-e9d393fbb9e4\" successfully removed Volume group \"ceph-04176411-2495-4944-abae-4ac6cedd8b46\" successfully removed With these configurations now downloaded we'll apply them in the following order. kubectl create -f ~/rook/common.yaml ; \\ kubectl create -f ~/rook/operator.yaml Verify the rook-ceph-operator is in the Running state Use kubectl -n rook-ceph get pod to check we have a running state. root@banks:~# kubectl -n rook-ceph get pod NAME READY STATUS RESTARTS AGE ... rook-ceph-operator-c8ff6447d-tbh5c 1 /1 Running 0 6m18s Create the Rook cluster Assuming the operator looks ok we can now create the cluster kubectl create -f ~/rook/cluster.yaml Watch cluster creation In order to monitor the cluster being created we can run watch kubectl -n rook-ceph get pod . The operator will take care of the orchestration but you should notice some phases of: a detect-version pod being created first (and then disappearing); followed by a series of mon pods being spun up (expect to see 3 of them by default a , b , c ); followed by an osd-prepare-<node> pod for each of our nodes which is where the ceph volumes are being created on the disks we've supplied. NAME READY STATUS RESTARTS AGE csi-cephfsplugin-provisioner-56c8b7ddf4-tlnxm 4 /4 Running 0 6m29s csi-cephfsplugin-provisioner-56c8b7ddf4-vbwvr 4 /4 Running 0 6m29s csi-cephfsplugin-z5zxb 3 /3 Running 0 6m29s csi-rbdplugin-2pppd 3 /3 Running 0 6m29s csi-rbdplugin-provisioner-6ff4dd4b94-24q9d 5 /5 Running 0 6m29s csi-rbdplugin-provisioner-6ff4dd4b94-m9f9x 5 /5 Running 0 6m29s rook-ceph-crashcollector-clarke-85b98f894d-z6r86 1 /1 Running 0 5m17s rook-ceph-mgr-a-5574b9fb9b-glfkb 1 /1 Running 0 5m17s rook-ceph-mon-a-679b66554d-6qbg7 1 /1 Running 0 6m2s rook-ceph-mon-b-84bf565bdc-49dwd 1 /1 Running 0 5m47s rook-ceph-mon-c-65997d4bf4-q2krn 1 /1 Running 0 5m32s rook-ceph-operator-6d8fb9498b-45ffm 1 /1 Running 0 7m18s rook-ceph-osd-prepare-clarke-xx557 1 /1 Running 0 5m1s rook-discover-xnkx4 1 /1 Running 0 6m46s Once this osd-prepare pod has completed we should have a cluster available. If you want you can check the logs for this pod with a kubectl -n rook-ceph logs rook-ceph-osd-prepare-clarke-xx557 . NB: This whole process can take a few minutes depending on the size of the disks you've selected. With ~6TB of HDDs and ~2TB of SSDs across 5 physical disks my prepare pod took 12mins and, start to finish, the whole cluster spin up (including the prepare) took 14mins. To verify the state of the cluster we will connect to the Rook Toolbox kubectl create -f ~/rook/toolbox.yaml Wait for the toolbox pod to enter a running state: kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" Once the rook-ceph-tools pod is running, you can connect to it with: kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash When inside the toolbox run ceph status after setting a custom prompt so we don't forget where we are. export PS1 = \"ceph-toolbox# \" ceph status [ root@banks / ] # ceph status cluster: id: 06da5ebc-d2f3-4366-a51c-db759d8bc664 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c ( age 2m ) mgr: a ( active, since 102s ) osd: 2 osds: 2 up ( since 33s ) , 2 in ( since 33s ) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2 .0 GiB used, 3 .6 TiB / 3 .6 TiB avail pgs: All mons should be in quorum A mgr should be active At least one OSD should be active If the health is not HEALTH_OK, the warnings or errors should be investigated Toubleshooting: Not all OSDs (disks) are created The task to prepare a disk can vary in duration based on it's size and a number of other factors. Start off by checking that the prepare has actually finished. $ watch kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-banks.local-dlvk7 0/1 Completed 0 2m31s If this doesn't show Completed then it's still performing the tasks. Wait for it to complete and then go back into the toolbox and check the ceph status output again. Troubleshooting: [errno 2] NB: You might get an error unable to get monitor info from DNS SRV with service name: ceph-mon or [errno 2] error connecting to the cluster when running ceph status in the toolbox if you've typed all of the above commands very quickly. This is usually because the cluster is still starting and waiting for all the monitors to come up and establish connections. Go get a cup of tea / wait a couple of minutes and try again. In the cluster.yaml spec the default number of mon instances is 3 . As a result if you don't have three of these pods running then your cluster is still initialising. You can run kubectl -n rook-ceph logs -l \"app=rook-ceph-operator\" to see an output of the logs from the operator and search for mons running . As you can see below it took mine around a minute to initialise all 3. To see what monitors you have run kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" . $ kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" NAME READY STATUS RESTARTS AGE rook-ceph-mon-a-5d677b5849-t4xct 1 /1 Running 0 82s rook-ceph-mon-b-6cfbcf8db4-7cwxp 1 /1 Running 0 66s rook-ceph-mon-c-8f858c585-c9z5b 1 /1 Running 0 50s When you are done with the toolbox, you can remove the deployment: kubectl -n rook-ceph delete deployment rook-ceph-tools If you want to delete the cluster and start again... Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse. There are some additional pointers here in the docs. kubectl delete -f ~/rook ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/* Dashboard and Storage We now have a cluster running but no configured storage or an ability to review status (other than logging into the toolbox).","title":"Install the Cluster"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#components","text":"","title":"Components"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#rook","text":"Rook allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider. More information on the architecure can be found in the docs","title":"Rook"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#ceph","text":"Ceph provides three types of storage: object : compatible with S3 API file : files and directories (incl. NFS); and block : replicate a hard drive There a 4 key components of the architecture to be aware of (shown above in the Rook diagram): Monitor (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication Manager daemon: keeps track of state in cluster and metrics OSDs (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage. MSDs (Metatdata Server): concerned with filesystem storage type only Under the hood everything is stored as an object in logical storage pools.","title":"Ceph"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#deployment-of-the-rook-operator","text":"See the Rook quickstart We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple mon components onto a single node. Similar to when we removed the the control plane node taint Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes). # create a working directory mkdir -p ~/rook && \\ cd ~/rook # download the sample files # all of these can be found here: https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph export ROOK_VERSION = 1 .2 wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/common.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/operator.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/cluster.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/toolbox.yaml # modify the cluster spec # - allow multiple mons per node sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml In addition, because we've setup our encrypted data storage to be mounted at /data/${BLKID} we will edit the default storage options to remove the useAllDevices selection and, instead, specify the directories. See the docs for more details. # set default storage location and remove default `useAllDevices: true` sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml # any devices starting with 'sd' (but not sda as that's our root filesystem) sed -i.bak 's/deviceFilter:/deviceFilter: ^sd[^a]/' cluster.yaml # encrypt them with LUKS # see conversation https://github.com/rook/rook/issues/923#issuecomment-557651052 sed -i.bak 's/# encryptedDevice: \"true\"/encryptedDevice: \"true\"/' cluster.yaml Wiping disks for usage Because Rook will fail if it finds an existing \"in use\" filesystem or disk we need to wipe the disks in the host which we want to use. For example. In use disks below (from a previous cluster). in use $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk \u2514\u2500ceph--04176411--2495--4944--abae--4ac6cedd8b46-osd--data--20a17864--c8d4--48f7--8aa0--e9d393fbb9e4 253 :1 0 1 .8T 0 lvm sdc 8 :32 0 1 .8T 0 disk \u2514\u2500ceph--38ed1893--7746--477a--a8c8--dae66f8360e5-osd--data--0940f2cf--d1c2--489f--9c8a--31d75d759be8 253 :0 0 1 .8T 0 lvm sdd 8 :48 0 2T 0 disk sde 8 :64 0 2T 0 disk sr0 11 :0 1 1024M 0 rom We can delete and wipe the disk of partition maps. # replicate the rook commands (and use regex to exclude sda like the manifest) export DISKS = $( lsblk --all --noheadings --list --output KNAME | grep sd [ ^a ] ) ; \\ for d in $DISKS ; do \\ export DISK = /dev/ $d ; \\ sudo wipefs $DISK ; \\ sudo vgremove -y $( sudo pvscan | grep $DISK | awk '{print $4}' ) ; \\ sudo dd if = /dev/zero of = $DISK bs = 512 count = 10 ; \\ done Now, if appropriate, remove the LVM with the appropriate combination of pvremove , lvremove , vgremove etc. $ sudo vgremove -y $( sudo pvscan | grep $DISK | awk '{print $4}' ) Logical volume \"osd-data-20a17864-c8d4-48f7-8aa0-e9d393fbb9e4\" successfully removed Volume group \"ceph-04176411-2495-4944-abae-4ac6cedd8b46\" successfully removed With these configurations now downloaded we'll apply them in the following order. kubectl create -f ~/rook/common.yaml ; \\ kubectl create -f ~/rook/operator.yaml Verify the rook-ceph-operator is in the Running state Use kubectl -n rook-ceph get pod to check we have a running state. root@banks:~# kubectl -n rook-ceph get pod NAME READY STATUS RESTARTS AGE ... rook-ceph-operator-c8ff6447d-tbh5c 1 /1 Running 0 6m18s","title":"Deployment of the Rook operator"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#create-the-rook-cluster","text":"Assuming the operator looks ok we can now create the cluster kubectl create -f ~/rook/cluster.yaml Watch cluster creation In order to monitor the cluster being created we can run watch kubectl -n rook-ceph get pod . The operator will take care of the orchestration but you should notice some phases of: a detect-version pod being created first (and then disappearing); followed by a series of mon pods being spun up (expect to see 3 of them by default a , b , c ); followed by an osd-prepare-<node> pod for each of our nodes which is where the ceph volumes are being created on the disks we've supplied. NAME READY STATUS RESTARTS AGE csi-cephfsplugin-provisioner-56c8b7ddf4-tlnxm 4 /4 Running 0 6m29s csi-cephfsplugin-provisioner-56c8b7ddf4-vbwvr 4 /4 Running 0 6m29s csi-cephfsplugin-z5zxb 3 /3 Running 0 6m29s csi-rbdplugin-2pppd 3 /3 Running 0 6m29s csi-rbdplugin-provisioner-6ff4dd4b94-24q9d 5 /5 Running 0 6m29s csi-rbdplugin-provisioner-6ff4dd4b94-m9f9x 5 /5 Running 0 6m29s rook-ceph-crashcollector-clarke-85b98f894d-z6r86 1 /1 Running 0 5m17s rook-ceph-mgr-a-5574b9fb9b-glfkb 1 /1 Running 0 5m17s rook-ceph-mon-a-679b66554d-6qbg7 1 /1 Running 0 6m2s rook-ceph-mon-b-84bf565bdc-49dwd 1 /1 Running 0 5m47s rook-ceph-mon-c-65997d4bf4-q2krn 1 /1 Running 0 5m32s rook-ceph-operator-6d8fb9498b-45ffm 1 /1 Running 0 7m18s rook-ceph-osd-prepare-clarke-xx557 1 /1 Running 0 5m1s rook-discover-xnkx4 1 /1 Running 0 6m46s Once this osd-prepare pod has completed we should have a cluster available. If you want you can check the logs for this pod with a kubectl -n rook-ceph logs rook-ceph-osd-prepare-clarke-xx557 . NB: This whole process can take a few minutes depending on the size of the disks you've selected. With ~6TB of HDDs and ~2TB of SSDs across 5 physical disks my prepare pod took 12mins and, start to finish, the whole cluster spin up (including the prepare) took 14mins. To verify the state of the cluster we will connect to the Rook Toolbox kubectl create -f ~/rook/toolbox.yaml Wait for the toolbox pod to enter a running state: kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" Once the rook-ceph-tools pod is running, you can connect to it with: kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash When inside the toolbox run ceph status after setting a custom prompt so we don't forget where we are. export PS1 = \"ceph-toolbox# \" ceph status [ root@banks / ] # ceph status cluster: id: 06da5ebc-d2f3-4366-a51c-db759d8bc664 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c ( age 2m ) mgr: a ( active, since 102s ) osd: 2 osds: 2 up ( since 33s ) , 2 in ( since 33s ) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2 .0 GiB used, 3 .6 TiB / 3 .6 TiB avail pgs: All mons should be in quorum A mgr should be active At least one OSD should be active If the health is not HEALTH_OK, the warnings or errors should be investigated","title":"Create the Rook cluster"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#toubleshooting-not-all-osds-disks-are-created","text":"The task to prepare a disk can vary in duration based on it's size and a number of other factors. Start off by checking that the prepare has actually finished. $ watch kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-banks.local-dlvk7 0/1 Completed 0 2m31s If this doesn't show Completed then it's still performing the tasks. Wait for it to complete and then go back into the toolbox and check the ceph status output again.","title":"Toubleshooting: Not all OSDs (disks) are created"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#troubleshooting-errno-2","text":"NB: You might get an error unable to get monitor info from DNS SRV with service name: ceph-mon or [errno 2] error connecting to the cluster when running ceph status in the toolbox if you've typed all of the above commands very quickly. This is usually because the cluster is still starting and waiting for all the monitors to come up and establish connections. Go get a cup of tea / wait a couple of minutes and try again. In the cluster.yaml spec the default number of mon instances is 3 . As a result if you don't have three of these pods running then your cluster is still initialising. You can run kubectl -n rook-ceph logs -l \"app=rook-ceph-operator\" to see an output of the logs from the operator and search for mons running . As you can see below it took mine around a minute to initialise all 3. To see what monitors you have run kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" . $ kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" NAME READY STATUS RESTARTS AGE rook-ceph-mon-a-5d677b5849-t4xct 1 /1 Running 0 82s rook-ceph-mon-b-6cfbcf8db4-7cwxp 1 /1 Running 0 66s rook-ceph-mon-c-8f858c585-c9z5b 1 /1 Running 0 50s When you are done with the toolbox, you can remove the deployment: kubectl -n rook-ceph delete deployment rook-ceph-tools If you want to delete the cluster and start again... Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse. There are some additional pointers here in the docs. kubectl delete -f ~/rook ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Troubleshooting: [errno 2]"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#dashboard-and-storage","text":"We now have a cluster running but no configured storage or an ability to review status (other than logging into the toolbox).","title":"Dashboard and Storage"},{"location":"01.infrastructure/04.storage/01.dashboard/","text":"Ceph comes with it's own dashboard for management purposes. Let's check the dashboard status. For full details see the docs root@banks:~/rook# kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-cephfsplugin-metrics ClusterIP 10 .107.29.251 <none> 8080 /TCP,8081/TCP 14m csi-rbdplugin-metrics ClusterIP 10 .102.151.196 <none> 8080 /TCP,8081/TCP 14m rook-ceph-mgr ClusterIP 10 .110.58.77 <none> 9283 /TCP 12m rook-ceph-mgr-dashboard ClusterIP 10 .104.71.31 <none> 8443 /TCP 12m rook-ceph-mon-a ClusterIP 10 .101.139.217 <none> 6789 /TCP,3300/TCP 13m rook-ceph-mon-b ClusterIP 10 .109.239.104 <none> 6789 /TCP,3300/TCP 13m rook-ceph-mon-c ClusterIP 10 .99.14.79 <none> 6789 /TCP,3300/TCP 13m So it looks like our rook-ceph-mgr-dashboard is running on port 8843 . Describing the service indicates it's not yet exposed via a NodePort etc. so we can't access it external to the cluster. root@banks:~/rook# kubectl -n rook-ceph describe services rook-ceph-mgr-dashboard Name: rook-ceph-mgr-dashboard Namespace: rook-ceph Labels: app = rook-ceph-mgr rook_cluster = rook-ceph Annotations: <none> Selector: app = rook-ceph-mgr,rook_cluster = rook-ceph Type: ClusterIP IP: 10 .104.71.31 Port: https-dashboard 8443 /TCP TargetPort: 8443 /TCP Endpoints: 10 .244.0.80:8443 Session Affinity: None Events: <none> You now have options for how to expose this: Ingress NodePort NodePort We'll enable this via a NodePort now. export ROOK_VERSION = 1 .2 cd ~/rook wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/dashboard-external-https.yaml kubectl create -f ~/rook/dashboard-external-https.yaml You will see the new service rook-ceph-mgr-dashboard-external-https created: $ kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-cephfsplugin-metrics ClusterIP 10 .102.185.98 <none> 8080 /TCP,8081/TCP 18h csi-rbdplugin-metrics ClusterIP 10 .108.114.119 <none> 8080 /TCP,8081/TCP 18h rook-ceph-mgr ClusterIP 10 .107.152.33 <none> 9283 /TCP 18h rook-ceph-mgr-dashboard ClusterIP 10 .105.220.219 <none> 8443 /TCP 18h rook-ceph-mgr-dashboard-external-https NodePort 10 .98.207.115 <none> 8443 :30995/TCP 9s rook-ceph-mon-a ClusterIP 10 .101.90.100 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-b ClusterIP 10 .107.14.178 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-c ClusterIP 10 .101.91.134 <none> 6789 /TCP,3300/TCP 18h In this example, port 30995 will be opened to expose port 8443 from the ceph-mgr pod . Find the ip address of the node or, if you've used Avahi as per the physcial node installation you could probably access the node directly using the hostname (Kubernetes will listen on that fqdn). Finding the IP address So this isn't that obvious... We've exposed our service via a NodePort but now need to find the IP address of the actual node in order to hit the required port to see the service... $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-mgr\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-mgr-a-59cc7fb98-7wfxf 1 /1 Running 0 19h 10 .244.0.112 banks <none> <none> Our service is running on the banks node. Let's find the IP of that then. $ kubectl describe node banks | grep InternalIP InternalIP: 192 .168.0.95 Or you could find the IP address with: kubectl get node banks -o jsonpath = '{.status.addresses[0].address}' Now you can enter the URL in your browser as https://192.168.0.95:30995 or https://banks.local:30995 and the dashboard will appear. Credentials As described in the docs Rook creates a default user named admin and generates a secret called rook-ceph-dashboard-admin-password in the namespace where rook is running. To retrieve the generated password, you can run the following: kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo This is a good thing to see... Ingress Given all the work we went through to give our services proper external access with TLS certificates earlier it would be a shame to waste this...! Let's teardown the NodePort. kubectl delete -f ~/rook/dashboard-external-https.yaml The docs have an example using an ingress controller we will borrow from. Quote If you have a cluster with an nginx Ingress Controller and a Certificate Manager (e.g. cert-manager) then you can create an Ingress like the one below. This example achieves four things: Exposes the dashboard on the Internet (using an reverse proxy) Issues an valid TLS Certificate for the specified domain name (using ACME) Tells the reverse proxy that the dashboard itself uses HTTPS Tells the reverse proxy that the dashboard itself does not have a valid certificate (it is self-signed) We need to modify the example though as some of the syntax and extensions used are out of date versus our version of kubernetes and we want to use our specific TLS settings. # file: ~/rook/dashboard-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rook-ceph-mgr-dashboard namespace : rook-ceph annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTPS\" nginx.ingress.kubernetes.io/server-snippet : | proxy_ssl_verify off; spec : tls : - hosts : - rook-ceph.jamesveitch.dev secretName : rook-ceph.jamesveitch.dev rules : - host : rook-ceph.jamesveitch.dev http : paths : - path : / backend : serviceName : rook-ceph-mgr-dashboard servicePort : https-dashboard --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : rook-ceph namespace : rook-ceph spec : secretName : rook-ceph.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : rook-ceph.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - rook-ceph.jamesveitch.dev issuerRef : name : letsencrypt-staging kind : ClusterIssuer group : cert-manager.io --- apiVersion : v1 kind : Service metadata : name : rook-ceph-mgr-dashboard-external-https namespace : rook-ceph labels : app : rook-ceph-mgr rook_cluster : rook-ceph spec : ports : - name : dashboard port : 8443 protocol : TCP targetPort : 8443 selector : app : rook-ceph-mgr rook_cluster : rook-ceph sessionAffinity : None Once you've confirmed this works with openssl we can change the issuer for the production LetsEncrypt issuer. Check Certificate with OpenSSL With the above manifest we should have a Fake LE certificate authority from the LetsEncrypt staging servers. $ echo | openssl s_client -connect rook-ceph.jamesveitch.dev:443 2 > & 1 | sed --quiet '/CN/p' depth = 1 CN = Fake LE Intermediate X1 0 s:CN = rook-ceph.jamesveitch.dev i:CN = Fake LE Intermediate X1 1 s:CN = Fake LE Intermediate X1 i:CN = Fake LE Root X1 subject = CN = rook-ceph.jamesveitch.dev issuer = CN = Fake LE Intermediate X1 kubectl -n rook-ceph delete certificate rook-ceph ; \\ sed -i.bak 's/letsencrypt-staging/letsencrypt/g' ~/rook/dashboard-ingress-https.yaml ; \\ kubectl apply -f ~/rook/dashboard-ingress-https.yaml Assuming this all works you should be able to navigate straight to the dashboard with your external dns settings.","title":"Setup Dashboard"},{"location":"01.infrastructure/04.storage/01.dashboard/#nodeport","text":"We'll enable this via a NodePort now. export ROOK_VERSION = 1 .2 cd ~/rook wget https://raw.githubusercontent.com/rook/rook/release- ${ ROOK_VERSION } /cluster/examples/kubernetes/ceph/dashboard-external-https.yaml kubectl create -f ~/rook/dashboard-external-https.yaml You will see the new service rook-ceph-mgr-dashboard-external-https created: $ kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-cephfsplugin-metrics ClusterIP 10 .102.185.98 <none> 8080 /TCP,8081/TCP 18h csi-rbdplugin-metrics ClusterIP 10 .108.114.119 <none> 8080 /TCP,8081/TCP 18h rook-ceph-mgr ClusterIP 10 .107.152.33 <none> 9283 /TCP 18h rook-ceph-mgr-dashboard ClusterIP 10 .105.220.219 <none> 8443 /TCP 18h rook-ceph-mgr-dashboard-external-https NodePort 10 .98.207.115 <none> 8443 :30995/TCP 9s rook-ceph-mon-a ClusterIP 10 .101.90.100 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-b ClusterIP 10 .107.14.178 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-c ClusterIP 10 .101.91.134 <none> 6789 /TCP,3300/TCP 18h In this example, port 30995 will be opened to expose port 8443 from the ceph-mgr pod . Find the ip address of the node or, if you've used Avahi as per the physcial node installation you could probably access the node directly using the hostname (Kubernetes will listen on that fqdn). Finding the IP address So this isn't that obvious... We've exposed our service via a NodePort but now need to find the IP address of the actual node in order to hit the required port to see the service... $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-mgr\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-mgr-a-59cc7fb98-7wfxf 1 /1 Running 0 19h 10 .244.0.112 banks <none> <none> Our service is running on the banks node. Let's find the IP of that then. $ kubectl describe node banks | grep InternalIP InternalIP: 192 .168.0.95 Or you could find the IP address with: kubectl get node banks -o jsonpath = '{.status.addresses[0].address}' Now you can enter the URL in your browser as https://192.168.0.95:30995 or https://banks.local:30995 and the dashboard will appear.","title":"NodePort"},{"location":"01.infrastructure/04.storage/01.dashboard/#credentials","text":"As described in the docs Rook creates a default user named admin and generates a secret called rook-ceph-dashboard-admin-password in the namespace where rook is running. To retrieve the generated password, you can run the following: kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo This is a good thing to see...","title":"Credentials"},{"location":"01.infrastructure/04.storage/01.dashboard/#ingress","text":"Given all the work we went through to give our services proper external access with TLS certificates earlier it would be a shame to waste this...! Let's teardown the NodePort. kubectl delete -f ~/rook/dashboard-external-https.yaml The docs have an example using an ingress controller we will borrow from. Quote If you have a cluster with an nginx Ingress Controller and a Certificate Manager (e.g. cert-manager) then you can create an Ingress like the one below. This example achieves four things: Exposes the dashboard on the Internet (using an reverse proxy) Issues an valid TLS Certificate for the specified domain name (using ACME) Tells the reverse proxy that the dashboard itself uses HTTPS Tells the reverse proxy that the dashboard itself does not have a valid certificate (it is self-signed) We need to modify the example though as some of the syntax and extensions used are out of date versus our version of kubernetes and we want to use our specific TLS settings. # file: ~/rook/dashboard-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rook-ceph-mgr-dashboard namespace : rook-ceph annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTPS\" nginx.ingress.kubernetes.io/server-snippet : | proxy_ssl_verify off; spec : tls : - hosts : - rook-ceph.jamesveitch.dev secretName : rook-ceph.jamesveitch.dev rules : - host : rook-ceph.jamesveitch.dev http : paths : - path : / backend : serviceName : rook-ceph-mgr-dashboard servicePort : https-dashboard --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : rook-ceph namespace : rook-ceph spec : secretName : rook-ceph.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : rook-ceph.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - rook-ceph.jamesveitch.dev issuerRef : name : letsencrypt-staging kind : ClusterIssuer group : cert-manager.io --- apiVersion : v1 kind : Service metadata : name : rook-ceph-mgr-dashboard-external-https namespace : rook-ceph labels : app : rook-ceph-mgr rook_cluster : rook-ceph spec : ports : - name : dashboard port : 8443 protocol : TCP targetPort : 8443 selector : app : rook-ceph-mgr rook_cluster : rook-ceph sessionAffinity : None Once you've confirmed this works with openssl we can change the issuer for the production LetsEncrypt issuer. Check Certificate with OpenSSL With the above manifest we should have a Fake LE certificate authority from the LetsEncrypt staging servers. $ echo | openssl s_client -connect rook-ceph.jamesveitch.dev:443 2 > & 1 | sed --quiet '/CN/p' depth = 1 CN = Fake LE Intermediate X1 0 s:CN = rook-ceph.jamesveitch.dev i:CN = Fake LE Intermediate X1 1 s:CN = Fake LE Intermediate X1 i:CN = Fake LE Root X1 subject = CN = rook-ceph.jamesveitch.dev issuer = CN = Fake LE Intermediate X1 kubectl -n rook-ceph delete certificate rook-ceph ; \\ sed -i.bak 's/letsencrypt-staging/letsencrypt/g' ~/rook/dashboard-ingress-https.yaml ; \\ kubectl apply -f ~/rook/dashboard-ingress-https.yaml Assuming this all works you should be able to navigate straight to the dashboard with your external dns settings.","title":"Ingress"},{"location":"01.infrastructure/04.storage/02.storage/","text":"We now have Ceph running and managed via Rook on our Kubernetes cluster (with a nice graphical dashboard) but don't yet have any storage configured. There are some good documented examples we can walk through to get started.","title":"Storage"},{"location":"01.infrastructure/04.storage/0200.block/","text":"As per the docs : Block storage allows a single pod to mount storage We need to create both a StorageClass and a CephBlockPool in order to use black storage on our cluster. mkdir -p ~/rook/storage cd ~/rook/storage wget -O \"storageclass-rbd.yaml\" https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' storageclass-rbd.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' storageclass-rbd.yaml ; \\ kubectl create -f ~/rook/storage/storageclass-rbd.yaml Test this new storage out with a Wordpress installation (including MySQL) which requires the usage of Volume and PersistentVolumeClaim . mkdir -p ~/rook/examples cd ~/rook/examples wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/wordpress.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/mysql.yaml kubectl create -f ~/rook/examples/mysql.yaml ; \\ kubectl create -f ~/rook/examples/wordpress.yaml To review the volumes that have been created run kubectl get pvc $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-29aa4aba-4029-487d-8e7e-b2eb08400382 20Gi RWO rook-ceph-block 74s wp-pv-claim Bound pvc-7251e045-a174-49e1-9393-fbd8dbcfeaa6 20Gi RWO rook-ceph-block 73s Those PVCs can also be seen in the ceph dashboard under Block >> Images Once the pods for Wordpress and MySQL are running get the cluster IP for the wordpress app and navigate to it. You should be presented with the installation wizard. We'll tear this down now before proceeding (but leave the storage class for later usage). $ kubectl delete -f wordpress.yaml service \"wordpress\" deleted persistentvolumeclaim \"wp-pv-claim\" deleted deployment.apps \"wordpress\" deleted $ kubectl delete -f mysql.yaml service \"wordpress-mysql\" deleted persistentvolumeclaim \"mysql-pv-claim\" deleted deployment.apps \"wordpress-mysql\" deleted # kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool # kubectl delete storageclass rook-ceph-block Setting as the default storage class Rather than needing to specify the storage everytime we deploy a new application we can tell kubernetes to, by default, use this storage if not given. Edit the ~/rook/storage/storageclass-rbd.yaml manifest and add both an annotation and the ability to expand the volume dynamically (added in kubernetes v1.16 ). # file: ~/rook/storage/storageclass-rbd.yaml apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block annotations : storageclass.kubernetes.io/is-default-class : \"true\" provisioner : rook-ceph.rbd.csi.ceph.com ... allowVolumeExpansion : true Now apply the modifications with kubectl apply -f ~/rook/storage/storageclass-rbd.yaml","title":"Block"},{"location":"01.infrastructure/04.storage/0200.block/#setting-as-the-default-storage-class","text":"Rather than needing to specify the storage everytime we deploy a new application we can tell kubernetes to, by default, use this storage if not given. Edit the ~/rook/storage/storageclass-rbd.yaml manifest and add both an annotation and the ability to expand the volume dynamically (added in kubernetes v1.16 ). # file: ~/rook/storage/storageclass-rbd.yaml apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block annotations : storageclass.kubernetes.io/is-default-class : \"true\" provisioner : rook-ceph.rbd.csi.ceph.com ... allowVolumeExpansion : true Now apply the modifications with kubectl apply -f ~/rook/storage/storageclass-rbd.yaml","title":"Setting as the default storage class"},{"location":"01.infrastructure/04.storage/0201.filesystem/","text":"As per the docs A shared file system can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared filesystem. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/filesystem.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' filesystem.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' filesystem.yaml ; \\ kubectl create -f ~/rook/storage/filesystem.yaml Wait for the mds pods to start adminlocal@banks:~/rook/storage$ kubectl -n rook-ceph get pod -l app = rook-ceph-mds NAME READY STATUS RESTARTS AGE rook-ceph-mds-myfs-a-84ccb448b5-9jbds 1 /1 Running 0 21s rook-ceph-mds-myfs-b-7d85c48b4c-72q9d 0 /1 Pending 0 20s NB: Because there is a podAntiAffinity spec in the filesystem.yaml placement section you may only see one pod running if we have a single node cluster. This is fine. Running a ceph status via the toolbox will reinforce this. # ceph status ... services: mon: 3 daemons, quorum a,b,c ( age 3d ) mgr: a ( active, since 3d ) mds: myfs:1 { 0 = myfs-a = up:active } osd: 2 osds: 2 up ( since 3d ) , 2 in ( since 3d ) rgw: 1 daemon active ( my.store.a ) ... As with other storage options, we need to create a StorageClass for a Filesystem. This will use the CSI Driver (which is the preferred driver going forward for K8s 1.13 and newer). cd ~/rook/storage wget -O \"storageclass-cephfs.yaml\" https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml kubectl create -f ~/rook/storage/storageclass-cephfs.yaml Consume the Shared File System: K8s Registry Sample We'll deploy a private docker registry that uses this shared filesystem via a PersistentVolumeClaim . cd ~/rook/ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml kubectl create -f ~/rook/kube-registry.yaml Configure registry See Github docs for further details. mkdir -p ~/registry cd ~/registry Now create a service.yaml file. # file: ~/registry/service.yaml apiVersion: v1 kind: Service metadata: name: kube-registry namespace: kube-system labels: k8s-app: kube-registry-upstream kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"KubeRegistry\" spec: selector: k8s-app: kube-registry ports: - name: registry port: 5000 protocol: TCP Apply this with kubectl create -f service.yaml . With the service created we'll use a DaemonSet to deploy a pod onto every node in the cluster (so that Dokcer sees it as localhost ). # file: ~/registry/daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-registry-proxy namespace: kube-system labels: k8s-app: kube-registry-proxy kubernetes.io/cluster-service: \"true\" version: v0.4 spec: selector: matchLabels: name: kube-registry template: metadata: labels: k8s-app: kube-registry-proxy kubernetes.io/name: \"kube-registry-proxy\" kubernetes.io/cluster-service: \"true\" version: v0.4 name: kube-registry spec: containers: - name: kube-registry-proxy image: gcr.io/google_containers/kube-registry-proxy:0.4 resources: limits: cpu: 100m memory: 50Mi env: - name: REGISTRY_HOST value: kube-registry.kube-system.svc.cluster.local - name: REGISTRY_PORT value: \"5000\" ports: - name: registry containerPort: 80 hostPort: 5000 Apply with a kubectl create -f ~/registry/daemonset.yaml and then check for completion of pods. $ kubectl -n kube-system get pod -l 'name=kube-registry' NAME READY STATUS RESTARTS AGE kube-registry-proxy-vtd56 1 /1 Running 0 5m34s We can check the registry has been deployed by running curl localhost:5000/image and expecting a 404 response. $ curl localhost:5000/image 404 page not found Push an image to the registry As per Docker docs we will push a small docker alpine image to our new local private repository. sudo docker pull alpine sudo docker images | grep alpine | grep latest sudo docker tag 965ea09ff2eb 127 .0.0.1:5000/alpine sudo docker push 127 .0.0.1:5000/alpine Mount the filesystem in toolbox to confirm # Create the directory mkdir /tmp/registry # Detect the mon endpoints and the user secret for the connection mon_endpoints = $( grep mon_host /etc/ceph/ceph.conf | awk '{print $3}' ) my_secret = $( grep key /etc/ceph/keyring | awk '{print $3}' ) # Mount the file system mount -t ceph -o mds_namespace = myfs,name = admin,secret = $my_secret $mon_endpoints :/ /tmp/registry # See your mounted file system df -h With the filesystem mounted we'll confirm there's an alpine repository now after our push above. # find /tmp/registry -name \"alpine\" /tmp/registry/volumes/csi/csi-vol-77b79f13-11ee-11ea-9848-7a3d11f24466/docker/registry/v2/repositories/alpine Teardown kubectl delete -f ~/registry/daemonset.yaml ; \\ kubectl delete -f ~/registry/service.yaml ; \\ kubectl delete -f ~/rook/kube-registry.yaml To delete the filesystem components and backing data, delete the Filesystem CRD. Warning: Data will be deleted kubectl -n rook-ceph delete cephfilesystem myfs","title":"Filesystem"},{"location":"01.infrastructure/04.storage/0201.filesystem/#consume-the-shared-file-system-k8s-registry-sample","text":"We'll deploy a private docker registry that uses this shared filesystem via a PersistentVolumeClaim . cd ~/rook/ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml kubectl create -f ~/rook/kube-registry.yaml","title":"Consume the Shared File System: K8s Registry Sample"},{"location":"01.infrastructure/04.storage/0201.filesystem/#configure-registry","text":"See Github docs for further details. mkdir -p ~/registry cd ~/registry Now create a service.yaml file. # file: ~/registry/service.yaml apiVersion: v1 kind: Service metadata: name: kube-registry namespace: kube-system labels: k8s-app: kube-registry-upstream kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"KubeRegistry\" spec: selector: k8s-app: kube-registry ports: - name: registry port: 5000 protocol: TCP Apply this with kubectl create -f service.yaml . With the service created we'll use a DaemonSet to deploy a pod onto every node in the cluster (so that Dokcer sees it as localhost ). # file: ~/registry/daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-registry-proxy namespace: kube-system labels: k8s-app: kube-registry-proxy kubernetes.io/cluster-service: \"true\" version: v0.4 spec: selector: matchLabels: name: kube-registry template: metadata: labels: k8s-app: kube-registry-proxy kubernetes.io/name: \"kube-registry-proxy\" kubernetes.io/cluster-service: \"true\" version: v0.4 name: kube-registry spec: containers: - name: kube-registry-proxy image: gcr.io/google_containers/kube-registry-proxy:0.4 resources: limits: cpu: 100m memory: 50Mi env: - name: REGISTRY_HOST value: kube-registry.kube-system.svc.cluster.local - name: REGISTRY_PORT value: \"5000\" ports: - name: registry containerPort: 80 hostPort: 5000 Apply with a kubectl create -f ~/registry/daemonset.yaml and then check for completion of pods. $ kubectl -n kube-system get pod -l 'name=kube-registry' NAME READY STATUS RESTARTS AGE kube-registry-proxy-vtd56 1 /1 Running 0 5m34s We can check the registry has been deployed by running curl localhost:5000/image and expecting a 404 response. $ curl localhost:5000/image 404 page not found","title":"Configure registry"},{"location":"01.infrastructure/04.storage/0201.filesystem/#push-an-image-to-the-registry","text":"As per Docker docs we will push a small docker alpine image to our new local private repository. sudo docker pull alpine sudo docker images | grep alpine | grep latest sudo docker tag 965ea09ff2eb 127 .0.0.1:5000/alpine sudo docker push 127 .0.0.1:5000/alpine","title":"Push an image to the registry"},{"location":"01.infrastructure/04.storage/0201.filesystem/#mount-the-filesystem-in-toolbox-to-confirm","text":"# Create the directory mkdir /tmp/registry # Detect the mon endpoints and the user secret for the connection mon_endpoints = $( grep mon_host /etc/ceph/ceph.conf | awk '{print $3}' ) my_secret = $( grep key /etc/ceph/keyring | awk '{print $3}' ) # Mount the file system mount -t ceph -o mds_namespace = myfs,name = admin,secret = $my_secret $mon_endpoints :/ /tmp/registry # See your mounted file system df -h With the filesystem mounted we'll confirm there's an alpine repository now after our push above. # find /tmp/registry -name \"alpine\" /tmp/registry/volumes/csi/csi-vol-77b79f13-11ee-11ea-9848-7a3d11f24466/docker/registry/v2/repositories/alpine","title":"Mount the filesystem in toolbox to confirm"},{"location":"01.infrastructure/04.storage/0201.filesystem/#teardown","text":"kubectl delete -f ~/registry/daemonset.yaml ; \\ kubectl delete -f ~/registry/service.yaml ; \\ kubectl delete -f ~/rook/kube-registry.yaml To delete the filesystem components and backing data, delete the Filesystem CRD. Warning: Data will be deleted kubectl -n rook-ceph delete cephfilesystem myfs","title":"Teardown"},{"location":"01.infrastructure/04.storage/0202.object/","text":"As per the docs Object storage exposes an S3 API to the storage cluster for applications to put and get data. We'll first create a CephObjectStore followed by a StorageClass for the bucket. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-ec.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' object-ec.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' object-ec.yaml ; \\ kubectl create -f ~/rook/storage/object/object-ec.yaml Check that the object store is configured and a rgw pod has started. $ kubectl -n rook-ceph get pod -l app = rook-ceph-rgw NAME READY STATUS RESTARTS AGE rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 27s Enable dashboard for the Object Gateway As per the docs we need to specifically enable access to the object gateway for it to be registered in the Ceph dashboard. # Connect to the toolbox first kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash # Create a system user radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system Make note of the keys # radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system { \"user_id\" : \"666\" , \"display_name\" : \"dashboard\" , \"email\" : \"\" , \"suspended\" : 0 , \"max_buckets\" : 1000 , \"subusers\" : [] , \"keys\" : [ { \"user\" : \"666\" , \"access_key\" : \"MUNSZSY7LF2E202MW1H6\" , \"secret_key\" : \"OF1za2LvibBpYjb6mw0umYDePfBkzfWSRNMeIwL0\" } ] , \"swift_keys\" : [] , \"caps\" : [] , \"op_mask\" : \"read, write, delete\" , \"system\" : \"true\" , \"default_placement\" : \"\" , \"default_storage_class\" : \"\" , \"placement_tags\" : [] , \"bucket_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"user_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"temp_url_keys\" : [] , \"type\" : \"rgw\" , \"mfa_ids\" : [] } Get the access_key and secret_access_key radosgw-admin user info --uid = 666 Now apply these credentials to the dashboard ceph dashboard set-rgw-api-access-key <access_key> ceph dashboard set-rgw-api-secret-key <secret_key> Set the host. You can get the service details with kubectl -n rook-ceph describe svc -l \"app=rook-ceph-rgw\" # use the format `service`.`namespace` as per docs # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ ceph dashboard set-rgw-api-host rook-ceph-rgw-my-store.rook-ceph ceph dashboard set-rgw-api-port 80 Create a bucket With an object store configured we can create a bucket. A bucket is created by defining a storage class and then registering an associated claim. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml kubectl create -f ~/rook/storage/object/storageclass-bucket-delete.yaml ; \\ kubectl create -f ~/rook/storage/object/object-bucket-claim-delete.yaml We should now see something like this when navigating on the dashboard to Object Gateway >> Buckets Enable external access Much like the Ceph Dashboard we want to expose the bucket to services that potentially live outside of the cluster. As with the dashboard we can either use a NodePort or Ingress to do this. NodePort We'll create a new service for external access. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/rgw-external.yaml kubectl create -f rgw-external.yaml We should now have a service running listening on a NodePort $ kubectl get svc -n rook-ceph -l 'app=rook-ceph-rgw' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-ceph-rgw-my-store ClusterIP 10 .97.98.170 <none> 80 /TCP 95m rook-ceph-rgw-my-store-external NodePort 10 .105.1.131 <none> 80 :32039/TCP 30s Connecting to the bucket with a client As the API is S3 compatible we can connect to the bucket with a variety of tools. In order to to do we need to obtain the HOST , ACCESS_KEY and SECRET_ACCESS_KEY variables. # config-map, secret, OBC will part of default if no specific name space mentioned # NB: You need to use the `metadata: name` for the bucket as defined in the claim export AWS_HOST = $( kubectl -n default get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}' ) export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) The AWS_HOST should also match the details provided abvove as the set-rgw-api-host command for the dashboard. We'll install s3md on the host to check. sudo apt-get update && sudo apt-get install -y s3cmd We'll need to find the node that our service is running on: $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-rgw,rook_object_store=my-store\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 107m 10 .244.0.87 banks.local <none> <none> We'll set the AWS_HOST to banks.local and the NodePort in use (in this case 32039 ). export AWS_HOST = banks.local:32039 Check the buckets our use has access to. $ s3cmd ls --no-ssl --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -11-28 12 :33 s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 Now upload a new file to this bucket and download it again to confirm. # Create object echo \"Hello Rook\" > /tmp/rookObj # Upload s3cmd put /tmp/rookObj \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } \\ s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 # Download s3cmd get s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3/rookObj \\ /tmp/rookObj-download \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } Check the contents $ cat /tmp/rookObj-download Hello Rook $ md5sum /tmp/rookObj* dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj-download Ingress As with the dasboard we'll configure this to have a valid SSL certificate and be accessible under a subdomain (e.g. s3.jamesveitch.dev). # file: ~/rook/storage/object/bucket-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rook-ceph-rgw-my-store-external-ingress namespace : rook-ceph annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - s3.jamesveitch.dev secretName : s3.jamesveitch.dev rules : - host : s3.jamesveitch.dev http : paths : - path : / backend : serviceName : rook-ceph-rgw-my-store servicePort : http --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : s3 namespace : rook-ceph spec : secretName : s3.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : s3.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - s3.jamesveitch.dev issuerRef : name : letsencrypt-staging kind : ClusterIssuer group : cert-manager.io --- apiVersion : v1 kind : Service metadata : name : rook-ceph-rgw-my-store-external-ingress namespace : rook-ceph labels : app : rook-ceph-rgw rook_cluster : rook-ceph rook_object_store : my-store spec : ports : - name : rgw port : 80 protocol : TCP targetPort : 80 selector : app : rook-ceph-rgw rook_cluster : rook-ceph rook_object_store : my-store sessionAffinity : None Apply this manifest with kubectl apply -f ~/rook/storage/object/bucket-ingress-https.yaml and, once the certificate has been provisioned, check that you get the Fake LE certificate before modifying and using the production server. kubectl -n rook-ceph delete certificate s3 ; \\ sed -i.bak 's/letsencrypt-staging/letsencrypt/g' ~/rook/storage/object/bucket-ingress-https.yaml ; \\ kubectl apply -f ~/rook/storage/object/bucket-ingress-https.yaml ; \\ watch kubectl -n rook-ceph get certificates Once the certificate has been recreated you should be able to navigate to the address and see the below. Connecting to the bucket with a client (external) Using s3cmd we can now connect to our bucket over SSL. Install the client (if not already available) sudo apt-get update && sudo apt-get install -y s3cmd Set some environment variables. export AWS_HOST = s3.jamesveitch.dev export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) Now connect and list out all the buckets. $ s3cmd ls --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -12-20 11 :24 s3://ceph-bkt-adc7524d-3dc6-400d-9a33-74171d2a4786 Teardown See Removing buckets in radosgw (and their contents) kubectl delete -f ~/rook/storage/object/object-bucket-claim-delete.yaml ; \\ kubectl delete -f ~/rook/storage/object/storageclass-bucket-delete.yaml And then, within the toolbox radosgw-admin bucket rm --bucket = ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 --purge-objects","title":"Object"},{"location":"01.infrastructure/04.storage/0202.object/#enable-dashboard-for-the-object-gateway","text":"As per the docs we need to specifically enable access to the object gateway for it to be registered in the Ceph dashboard. # Connect to the toolbox first kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash # Create a system user radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system Make note of the keys # radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system { \"user_id\" : \"666\" , \"display_name\" : \"dashboard\" , \"email\" : \"\" , \"suspended\" : 0 , \"max_buckets\" : 1000 , \"subusers\" : [] , \"keys\" : [ { \"user\" : \"666\" , \"access_key\" : \"MUNSZSY7LF2E202MW1H6\" , \"secret_key\" : \"OF1za2LvibBpYjb6mw0umYDePfBkzfWSRNMeIwL0\" } ] , \"swift_keys\" : [] , \"caps\" : [] , \"op_mask\" : \"read, write, delete\" , \"system\" : \"true\" , \"default_placement\" : \"\" , \"default_storage_class\" : \"\" , \"placement_tags\" : [] , \"bucket_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"user_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"temp_url_keys\" : [] , \"type\" : \"rgw\" , \"mfa_ids\" : [] } Get the access_key and secret_access_key radosgw-admin user info --uid = 666 Now apply these credentials to the dashboard ceph dashboard set-rgw-api-access-key <access_key> ceph dashboard set-rgw-api-secret-key <secret_key> Set the host. You can get the service details with kubectl -n rook-ceph describe svc -l \"app=rook-ceph-rgw\" # use the format `service`.`namespace` as per docs # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ ceph dashboard set-rgw-api-host rook-ceph-rgw-my-store.rook-ceph ceph dashboard set-rgw-api-port 80","title":"Enable dashboard for the Object Gateway"},{"location":"01.infrastructure/04.storage/0202.object/#create-a-bucket","text":"With an object store configured we can create a bucket. A bucket is created by defining a storage class and then registering an associated claim. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml kubectl create -f ~/rook/storage/object/storageclass-bucket-delete.yaml ; \\ kubectl create -f ~/rook/storage/object/object-bucket-claim-delete.yaml We should now see something like this when navigating on the dashboard to Object Gateway >> Buckets","title":"Create a bucket"},{"location":"01.infrastructure/04.storage/0202.object/#enable-external-access","text":"Much like the Ceph Dashboard we want to expose the bucket to services that potentially live outside of the cluster. As with the dashboard we can either use a NodePort or Ingress to do this.","title":"Enable external access"},{"location":"01.infrastructure/04.storage/0202.object/#nodeport","text":"We'll create a new service for external access. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/rgw-external.yaml kubectl create -f rgw-external.yaml We should now have a service running listening on a NodePort $ kubectl get svc -n rook-ceph -l 'app=rook-ceph-rgw' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-ceph-rgw-my-store ClusterIP 10 .97.98.170 <none> 80 /TCP 95m rook-ceph-rgw-my-store-external NodePort 10 .105.1.131 <none> 80 :32039/TCP 30s","title":"NodePort"},{"location":"01.infrastructure/04.storage/0202.object/#connecting-to-the-bucket-with-a-client","text":"As the API is S3 compatible we can connect to the bucket with a variety of tools. In order to to do we need to obtain the HOST , ACCESS_KEY and SECRET_ACCESS_KEY variables. # config-map, secret, OBC will part of default if no specific name space mentioned # NB: You need to use the `metadata: name` for the bucket as defined in the claim export AWS_HOST = $( kubectl -n default get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}' ) export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) The AWS_HOST should also match the details provided abvove as the set-rgw-api-host command for the dashboard. We'll install s3md on the host to check. sudo apt-get update && sudo apt-get install -y s3cmd We'll need to find the node that our service is running on: $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-rgw,rook_object_store=my-store\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 107m 10 .244.0.87 banks.local <none> <none> We'll set the AWS_HOST to banks.local and the NodePort in use (in this case 32039 ). export AWS_HOST = banks.local:32039 Check the buckets our use has access to. $ s3cmd ls --no-ssl --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -11-28 12 :33 s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 Now upload a new file to this bucket and download it again to confirm. # Create object echo \"Hello Rook\" > /tmp/rookObj # Upload s3cmd put /tmp/rookObj \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } \\ s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 # Download s3cmd get s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3/rookObj \\ /tmp/rookObj-download \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } Check the contents $ cat /tmp/rookObj-download Hello Rook $ md5sum /tmp/rookObj* dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj-download","title":"Connecting to the bucket with a client"},{"location":"01.infrastructure/04.storage/0202.object/#ingress","text":"As with the dasboard we'll configure this to have a valid SSL certificate and be accessible under a subdomain (e.g. s3.jamesveitch.dev). # file: ~/rook/storage/object/bucket-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rook-ceph-rgw-my-store-external-ingress namespace : rook-ceph annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - s3.jamesveitch.dev secretName : s3.jamesveitch.dev rules : - host : s3.jamesveitch.dev http : paths : - path : / backend : serviceName : rook-ceph-rgw-my-store servicePort : http --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : s3 namespace : rook-ceph spec : secretName : s3.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : s3.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - s3.jamesveitch.dev issuerRef : name : letsencrypt-staging kind : ClusterIssuer group : cert-manager.io --- apiVersion : v1 kind : Service metadata : name : rook-ceph-rgw-my-store-external-ingress namespace : rook-ceph labels : app : rook-ceph-rgw rook_cluster : rook-ceph rook_object_store : my-store spec : ports : - name : rgw port : 80 protocol : TCP targetPort : 80 selector : app : rook-ceph-rgw rook_cluster : rook-ceph rook_object_store : my-store sessionAffinity : None Apply this manifest with kubectl apply -f ~/rook/storage/object/bucket-ingress-https.yaml and, once the certificate has been provisioned, check that you get the Fake LE certificate before modifying and using the production server. kubectl -n rook-ceph delete certificate s3 ; \\ sed -i.bak 's/letsencrypt-staging/letsencrypt/g' ~/rook/storage/object/bucket-ingress-https.yaml ; \\ kubectl apply -f ~/rook/storage/object/bucket-ingress-https.yaml ; \\ watch kubectl -n rook-ceph get certificates Once the certificate has been recreated you should be able to navigate to the address and see the below.","title":"Ingress"},{"location":"01.infrastructure/04.storage/0202.object/#connecting-to-the-bucket-with-a-client-external","text":"Using s3cmd we can now connect to our bucket over SSL. Install the client (if not already available) sudo apt-get update && sudo apt-get install -y s3cmd Set some environment variables. export AWS_HOST = s3.jamesveitch.dev export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) Now connect and list out all the buckets. $ s3cmd ls --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -12-20 11 :24 s3://ceph-bkt-adc7524d-3dc6-400d-9a33-74171d2a4786","title":"Connecting to the bucket with a client (external)"},{"location":"01.infrastructure/04.storage/0202.object/#teardown","text":"See Removing buckets in radosgw (and their contents) kubectl delete -f ~/rook/storage/object/object-bucket-claim-delete.yaml ; \\ kubectl delete -f ~/rook/storage/object/storageclass-bucket-delete.yaml And then, within the toolbox radosgw-admin bucket rm --bucket = ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 --purge-objects","title":"Teardown"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/","text":"As we've got Ceph (via Rook) installed we'll initially setup Prometheus to monitor metrics and then display them in Deploy and configure Prometheus mkdir -p ~/monitoring ; \\ cd ~/monitoring export OPERATOR_VERSION = v0.34.0 wget https://raw.githubusercontent.com/coreos/prometheus-operator/ ${ OPERATOR_VERSION } /bundle.yaml kubectl apply -f ~/monitoring/bundle.yaml Then wait for the prometheus-operator pod to be Running with kubectl get pods -w . Then we need to configure the Ceph specific configuration: monitoring endpoints, alarm levels etc... cd ~/monitoring ; \\ wget https://raw.githubusercontent.com/packet-labs/Rook-on-Bare-Metal-Workshop/master/configs/ceph-monitoring.yml ; \\ kubectl apply -f ~/monitoring/ceph-monitoring.yml At this point we should be able to reach the Prometheus UI at: IP = $( kubectl get nodes -o jsonpath = '{.items[0].status.addresses[].address}' ) PORT = $( kubectl -n rook-ceph get svc rook-prometheus -o jsonpath = '{.spec.ports[].nodePort}' ) echo \"Your Prometheus UI is available at: http:// $IP : $PORT /\" NB: This, by default, sets up a NodePort service. We'll fix this later. If you're node is not immediately acessible via the $IP then you can check the available addresses for it, maybe choosing to access via the Hostname instead. $ kubectl get nodes -o json | jq '.items[0].status.addresses[]' { \"address\" : \"172.17.0.1\" , \"type\" : \"InternalIP\" } { \"address\" : \"banks.local\" , \"type\" : \"Hostname\" } Head over to Status >> Target and make sure that the ceph-mgr target is UP . Then go to Graph and graph following query ceph_cluster_total_used_bytes/(1024^3) to show the total space used in gigabyte over time. Another query of (ceph_cluster_total_used_bytes / ceph_cluster_total_used_raw_bytes) * 100 will show the % of available space used. Install Helm Irrespective of the method for ultimately exposing the service we'll use helm to install grafana from the stable charts repository. # Install helm export HELM_VERSION = v3.0.0 wget https://get.helm.sh/helm- ${ HELM_VERSION } -linux-amd64.tar.gz tar -xvzf helm- ${ HELM_VERSION } -linux-amd64.tar.gz chmod +x linux-amd64/helm sudo mv linux-amd64/helm /usr/local/bin/ rm -rf { helm*,linux-amd64 } # Add repository helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm repo update Install Grafana As with other services, we can choose to use either a NodePort or Ingress (via a ClusterIP ) to expose our services. My preference is the ingress. ClusterIP helm install grafana stable/grafana \\ --set service.type = ClusterIP \\ --set persistence.enabled = true \\ --set persistence.type = pvc \\ --set persistence.size = 10Gi \\ --set persistence.storageClassName = rook-ceph-block NodePort helm install grafana stable/grafana \\ --set service.type = NodePort \\ --set persistence.enabled = true \\ --set persistence.type = pvc \\ --set persistence.size = 10Gi \\ --set persistence.storageClassName = rook-ceph-block As can be seen we're using persistence with a pvc and telling it to use our rook-ceph-block storage. You'll get something that look like the below as an output. Follow the instructions. Info NAME: grafana LAST DEPLOYED: Fri Dec 20 12 :28:11 2019 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1 . Get your 'admin' user password by running: kubectl get secret --namespace default grafana -o jsonpath = \"{.data.admin-password}\" | base64 --decode ; echo 2 . The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.default.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME = $( kubectl get pods --namespace default -l \"app=grafana,release=grafana\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace default port-forward $POD_NAME 3000 3 . Login with the password from step 1 and the username: admin Once logged in you'll see a screen similar to below. Hit the Add data source and select Prometheus . The url needs to be the details we identified above. We'll need the either the Cluster-IP or an internal dns reference (i.e rook-prometheus.rook-ceph ) and Port of the service (choose the container port, not the externally exposed one). $ kubectl -n rook-ceph get svc rook-prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-prometheus NodePort 10 .110.175.22 <none> 9090 :30900/TCP 16h Hit Save & Test and you should hopefully see a Data source is working check appear. Hit Back to go back to the main screen. Back on the main screen click on the + and select Import . Ceph has published some open dashboards with the IDs 2842 , 5336 and 5342 . NB: On two of the dashboards you need to select Prometheus as the datasource. Ingress As the helm chart has already created the Service we need to just create an Ingress and map it to this and then ensure we request a valid certificate from LetsEncrypt. # file: ~/monitoring/grafana-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : grafana-external-ingress namespace : default annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - grafana.jamesveitch.dev secretName : grafana.jamesveitch.dev rules : - host : grafana.jamesveitch.dev http : paths : - path : / backend : serviceName : grafana servicePort : service --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : grafana namespace : default spec : secretName : grafana.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : grafana.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - grafana.jamesveitch.dev issuerRef : name : letsencrypt kind : ClusterIssuer group : cert-manager.io","title":"Prometheus and Grafana"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/#deploy-and-configure-prometheus","text":"mkdir -p ~/monitoring ; \\ cd ~/monitoring export OPERATOR_VERSION = v0.34.0 wget https://raw.githubusercontent.com/coreos/prometheus-operator/ ${ OPERATOR_VERSION } /bundle.yaml kubectl apply -f ~/monitoring/bundle.yaml Then wait for the prometheus-operator pod to be Running with kubectl get pods -w . Then we need to configure the Ceph specific configuration: monitoring endpoints, alarm levels etc... cd ~/monitoring ; \\ wget https://raw.githubusercontent.com/packet-labs/Rook-on-Bare-Metal-Workshop/master/configs/ceph-monitoring.yml ; \\ kubectl apply -f ~/monitoring/ceph-monitoring.yml At this point we should be able to reach the Prometheus UI at: IP = $( kubectl get nodes -o jsonpath = '{.items[0].status.addresses[].address}' ) PORT = $( kubectl -n rook-ceph get svc rook-prometheus -o jsonpath = '{.spec.ports[].nodePort}' ) echo \"Your Prometheus UI is available at: http:// $IP : $PORT /\" NB: This, by default, sets up a NodePort service. We'll fix this later. If you're node is not immediately acessible via the $IP then you can check the available addresses for it, maybe choosing to access via the Hostname instead. $ kubectl get nodes -o json | jq '.items[0].status.addresses[]' { \"address\" : \"172.17.0.1\" , \"type\" : \"InternalIP\" } { \"address\" : \"banks.local\" , \"type\" : \"Hostname\" } Head over to Status >> Target and make sure that the ceph-mgr target is UP . Then go to Graph and graph following query ceph_cluster_total_used_bytes/(1024^3) to show the total space used in gigabyte over time. Another query of (ceph_cluster_total_used_bytes / ceph_cluster_total_used_raw_bytes) * 100 will show the % of available space used.","title":"Deploy and configure Prometheus"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/#install-helm","text":"Irrespective of the method for ultimately exposing the service we'll use helm to install grafana from the stable charts repository. # Install helm export HELM_VERSION = v3.0.0 wget https://get.helm.sh/helm- ${ HELM_VERSION } -linux-amd64.tar.gz tar -xvzf helm- ${ HELM_VERSION } -linux-amd64.tar.gz chmod +x linux-amd64/helm sudo mv linux-amd64/helm /usr/local/bin/ rm -rf { helm*,linux-amd64 } # Add repository helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm repo update","title":"Install Helm"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/#install-grafana","text":"As with other services, we can choose to use either a NodePort or Ingress (via a ClusterIP ) to expose our services. My preference is the ingress. ClusterIP helm install grafana stable/grafana \\ --set service.type = ClusterIP \\ --set persistence.enabled = true \\ --set persistence.type = pvc \\ --set persistence.size = 10Gi \\ --set persistence.storageClassName = rook-ceph-block NodePort helm install grafana stable/grafana \\ --set service.type = NodePort \\ --set persistence.enabled = true \\ --set persistence.type = pvc \\ --set persistence.size = 10Gi \\ --set persistence.storageClassName = rook-ceph-block As can be seen we're using persistence with a pvc and telling it to use our rook-ceph-block storage. You'll get something that look like the below as an output. Follow the instructions. Info NAME: grafana LAST DEPLOYED: Fri Dec 20 12 :28:11 2019 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1 . Get your 'admin' user password by running: kubectl get secret --namespace default grafana -o jsonpath = \"{.data.admin-password}\" | base64 --decode ; echo 2 . The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.default.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME = $( kubectl get pods --namespace default -l \"app=grafana,release=grafana\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace default port-forward $POD_NAME 3000 3 . Login with the password from step 1 and the username: admin Once logged in you'll see a screen similar to below. Hit the Add data source and select Prometheus . The url needs to be the details we identified above. We'll need the either the Cluster-IP or an internal dns reference (i.e rook-prometheus.rook-ceph ) and Port of the service (choose the container port, not the externally exposed one). $ kubectl -n rook-ceph get svc rook-prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-prometheus NodePort 10 .110.175.22 <none> 9090 :30900/TCP 16h Hit Save & Test and you should hopefully see a Data source is working check appear. Hit Back to go back to the main screen. Back on the main screen click on the + and select Import . Ceph has published some open dashboards with the IDs 2842 , 5336 and 5342 . NB: On two of the dashboards you need to select Prometheus as the datasource.","title":"Install Grafana"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/#ingress","text":"As the helm chart has already created the Service we need to just create an Ingress and map it to this and then ensure we request a valid certificate from LetsEncrypt. # file: ~/monitoring/grafana-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : grafana-external-ingress namespace : default annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - grafana.jamesveitch.dev secretName : grafana.jamesveitch.dev rules : - host : grafana.jamesveitch.dev http : paths : - path : / backend : serviceName : grafana servicePort : service --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : grafana namespace : default spec : secretName : grafana.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : grafana.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - grafana.jamesveitch.dev issuerRef : name : letsencrypt kind : ClusterIssuer group : cert-manager.io","title":"Ingress"},{"location":"01.infrastructure/05.monitoring/01.kubernetes.dashboard/","text":"Kubernetes has a Web UI that can be turned on and looks like the below. As it's not deployed by default we'll turn it on. Further docs are available on GitHub . We need to customise the steps though as by default the documentation installs in an insecure fashion (self-signed certs etc.) Notes from repo Custom certificates have to be stored in a secret named kubernetes-dashboard-certs in the same namespace as Kubernetes Dashboard. By default the recommended manifests are the aio ones and are found here . The recommended.yaml is the one the README.md will propose. If we want to deploy with authorisation thoguh the alternative.yaml should be used. We need to review the Access control part of the docs. First we should create a namespace for all of our monitoring solutions and then get a LetsEncrypt certificate for the dashboard to consume (as opposed to using some self-signed ones). kubectl create namespace monitoring ~/monitoring/kubernetes-dashboard-certificate.yaml # file: ~/monitoring/kubernetes-dashboard-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : dashboard namespace : monitoring spec : # Secret names are always required. # We should use the name in the docs # https://github.com/kubernetes/dashboard/blob/master/docs/user/installation.md#recommended-setup secretName : kubernetes-dashboard-certs duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : dashboard.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - dashboard.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io Obtain with a kubectl apply -f ~/monitoring/kubernetes-dashboard-certificate.yaml . Once the certificate has been obtained you should be able to see it as a secret. certificate $ kubectl get certificate -n monitoring NAME READY SECRET AGE dashboard True kubernetes-dashboard-certs 9m18s $ kubectl describe secret kubernetes-dashboard-certs -n monitoring Name: kubernetes-dashboard-certs Namespace: monitoring Labels: <none> Annotations: cert-manager.io/alt-names: dashboard.jamesveitch.dev cert-manager.io/certificate-name: dashboard cert-manager.io/common-name: dashboard.jamesveitch.dev cert-manager.io/ip-sans: cert-manager.io/issuer-kind: ClusterIssuer cert-manager.io/issuer-name: letsencrypt cert-manager.io/uri-sans: Type: kubernetes.io/tls Data ==== ca.crt: 0 bytes tls.crt: 3586 bytes tls.key: 1679 bytes Now grab the manifest and, under the Deployment, section add arguments with the pod definition for the certificates as well as changing the namespace to our monitoring one. export DASHBOARD_VERSION = v2.0.0-beta8 ; \\ cd ~/monitoring ; \\ wget https://raw.githubusercontent.com/kubernetes/dashboard/ ${ DASHBOARD_VERSION } /aio/deploy/recommended.yaml # Replace namespace sed -i.bak 's/namespace: kubernetes-dashboard/namespace: monitoring/g' ~/monitoring/recommended.yaml # Replace deployment options for namespace and insert tls sed -i.bak '/- --auto-generate-certificates/i\\ - --tls-cert-file=/tls.crt\\n - --tls-key-file=/tls.key' ~/monitoring/recommended.yaml sed -i.bak 's/- --namespace=kubernetes-dashboard/- --namespace=monitoring/g' ~/monitoring/recommended.yaml We'll apply this now (ignore any errors around The Secret \"kubernetes-dashboard-certs\" is invalid ). It creates a namespace for kubernetes-dashboard by default so we'll also delete that afterwards as it's not needed. kubectl apply -f ~/monitoring/recommended.yaml ; \\ kubectl delete ns kubernetes-dashboard And now create an ingress and matching certificate # file: ~/monitoring/dashboard-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : dashboard-external-ingress namespace : monitoring annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTPS\" nginx.ingress.kubernetes.io/server-snippet : | proxy_ssl_verify off; spec : tls : - hosts : - dashboard.jamesveitch.dev secretName : kubernetes-dashboard-certs rules : - host : dashboard.jamesveitch.dev http : paths : - path : / backend : serviceName : kubernetes-dashboard servicePort : 443 kubectl apply -f ~/monitoring/dashboard-ingress-https.yaml If all goes well you should now be able to navigate to your dashboard at dashboard.jamesveitch.dev . Get your token for logging in. $ kubectl -n monitoring get secret NAME TYPE DATA AGE dashboard-jamesveitch-dev kubernetes.io/tls 3 40m dashboard.jamesveitch.dev kubernetes.io/tls 3 12m default-token-xwv2j kubernetes.io/service-account-token 3 41m kubernetes-dashboard-certs kubernetes.io/tls 3 6m17s kubernetes-dashboard-csrf Opaque 1 3m kubernetes-dashboard-key-holder Opaque 2 3m kubernetes-dashboard-token-gdf67 kubernetes.io/service-account-token 3 3m $ kubectl -n monitoring describe secret kubernetes-dashboard-token-gdf67 Name: kubernetes-dashboard-token-gdf67 Namespace: monitoring Labels: <none> Annotations: kubernetes.io/service-account.name: kubernetes-dashboard kubernetes.io/service-account.uid: 798c2145-63df-4852-b539-00f897b61e4a Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 10 bytes token: aVeryLongTokenString On logging in there's some good news and bad news... Good: We can login with a token and have a nice padlock for TLS via LetsEncrypt Bad: It tells us there's nothing running...?! Creating an Admin user Following the guidance in the docs we will create a sample admin user. I say sample because we want, wherever possible, our users to be defined in our central Identity Provider (which is going to be setup later on ). # file: ~/monitoring/admin-user.yaml apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : monitoring --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : monitoring kubectl apply -f ~/monitoring/admin-user.yaml We can now find the token directly from the commandline. kubectl -n monitoring describe secret $( kubectl -n monitoring get secret | grep admin-user | awk '{print $1}' ) Logging back in as this user we can now see everything in the cluster. Teardown Delete the sample admin user as we don't want this hanging around. kubectl delete -f ~/monitoring/admin-user.yaml","title":"Kubernetes Dashboard"},{"location":"01.infrastructure/05.monitoring/01.kubernetes.dashboard/#creating-an-admin-user","text":"Following the guidance in the docs we will create a sample admin user. I say sample because we want, wherever possible, our users to be defined in our central Identity Provider (which is going to be setup later on ). # file: ~/monitoring/admin-user.yaml apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : monitoring --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : monitoring kubectl apply -f ~/monitoring/admin-user.yaml We can now find the token directly from the commandline. kubectl -n monitoring describe secret $( kubectl -n monitoring get secret | grep admin-user | awk '{print $1}' ) Logging back in as this user we can now see everything in the cluster.","title":"Creating an Admin user"},{"location":"01.infrastructure/05.monitoring/01.kubernetes.dashboard/#teardown","text":"Delete the sample admin user as we don't want this hanging around. kubectl delete -f ~/monitoring/admin-user.yaml","title":"Teardown"},{"location":"02.idam/00.idam/","text":"Identity and Access Management is an entire topic on it's own so I'm not going to attempt to do it justice. In my mind though I've got the following requirements: Minimise the number of logins and/or passwords that users need to remember (ideally just one). A single source of truth for users identities for the purposes of authentication (i.e. is James who he says he is). This source of truth should support multiple authentication protocols seamlessly (many applications have different standards such as LDAP, Oauth/OIDC, SAML) so there's only a single integration point for applications. Additional security, via multi-factor, should be available for administrators. The solution should be able to integrate with Kubernetes such that users can obtain tokens for the API through their standard login credentials and can access appropraite resources (authorisation) such as the dashboard. We should be able to integrate OS logins (specifically Linux and MacOS) We're going to create a namespace auth to hold our authorisation workloads. kubectl create ns auth Important Whilst the setup might initially seem overly convulted (by using both OpenLDAP and Keycloak) there is a method to the madness... A number of applications, especially those in the homelab/enterprise space will support external authentication providers but may only support LDAP (as it has been around for longer and is more of a de facto standard in the enterprise space). As a result, we'd have to replicate a load of configuration, ACLs etc. from Keycloak into another user management solution. Supporting (and writing back into LDAP) as the lowest common denominator means that things such as Groups are consistent everywhere and only need to be changed/defined in one area.","title":"IDAM Overview"},{"location":"02.idam/01.openldap/","text":"We're going to follow a previous tutorial I wrote for setting up an initial OpenLDAP installation on a VM and then seed an initial admin user. Dockerising this is done now with the osixia/openldap base image which can be found on GitHub . # Create our working folder mkdir -p ~/auth ; \\ cd ~/auth Set Secrets As we're going to have some username/password combinations defined for our authentication services (e.g. database users, root admin users for LDAP and Keycloak etc.) we should store these as secrets and then add them into the environment or configuration of the containers in the manifests as references (as opposed to being hardcoded). There's two options to initially generate and save these secrets. We'll specify to create them only in the auth namespace. Generic CLI $ kubectl -n auth create secret generic ldap-user-creds \\ --from-literal = LDAP_ADMIN_PASSWORD = admin \\ --from-literal = LDAP_CONFIG_PASSWORD = config secret/ldap-user-creds created Using Manifest # We should `base64` encode the secrets first. $ echo -n 'admin' | base64 YWRtaW4 = $ echo -n 'config' | base64 Y29uZmln --- # Now use these base64 secrets in the manifest --- # file: ~/auth/ldap-user-creds-secret.yaml apiVersion: v1 kind: Secret metadata: name: ldap-user-creds namespace: auth type: Opaque data: LDAP_ADMIN_PASSWORD: YWRtaW4 = LDAP_CONFIG_PASSWORD: Y29uZmln For either of the above methods for storing the secret you then access the key,value combinations in the deployment manifest like follows: envFrom envFrom : # Secrets - secretRef : name : ldap-user-creds # Normal plaintext config - configMapRef : name : ldap-config env env : - name : LDAP_ADMIN_PASSWORD valueFrom : secretKeyRef : name : ldap-user-creds key : password - name : LDAP_CONFIG_PASSWORD valueFrom : secretKeyRef : name : ldap-user-creds key : LDAP_CONFIG_PASSWORD posixGroup, groupOfNames or groupOfUniqueNames I went down the rabbithole all the way back to the 2006 mailing list in order to work out what the accepted wisdom is for creating user groups and then ensuring referential integrity with a backref on the user object. As per the official docs it turns out there's a memberOf attribute that can be maintained on an object to show what groups they are members of. Sounds simple? In some scenarios, it may be desirable for a client to be able to determine which groups an entry is a member of, without performing an additional search. Examples of this are applications using the DIT for access control based on group authorization. The memberof overlay updates an attribute (by default memberOf) whenever changes occur to the membership attribute (by default member) of entries of the objectclass (by default groupOfNames) configured to trigger updates. Thus, it provides maintenance of the list of groups an entry is a member of, when usual maintenance of groups is done by modifying the members on the group entry. It's actually, genuinely, rocket science to get anything like this properly working (or even find some docs that explain it). In order to enable this functionality I found you need to modify the OpenLDAP installation to specifically ask for memberships to be maintained. # file: ~/auth/memberOf.ldif dn: olcOverlay=memberof,olcDatabase={1}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcMemberOf olcOverlay: memberof olcMemberOfRefint: TRUE You can apply this manually with ldapadd -Y EXTERNAL -H ldapi:/// -f memberOf.ldif or, as explained in the docker image notes, we can Seed the ldap database with a ldif . We need to add this file into the /container/service/slapd/assets/config/bootstrap/ldif/custom and then add --copy-service to the startup args of the container. In the world of Kubernetes this means mounting via a ConfigMap. apiVersion : v1 kind : ConfigMap metadata : name : memberof-config namespace : auth data : memberOf.ldif : | dn: olcOverlay=memberof,olcDatabase={1}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcMemberOf olcOverlay: memberof olcMemberOfRefint: TRUE Note: I\u2019m using the name of the file as the key. We can then add this into the volumes and volumeMounts of the container. container ... containers : - name : ldap image : osixia/openldap args : [ \"--copy-service\" ] envFrom : - configMapRef : name : ldap-config ports : - containerPort : 389 name : ldap - containerPort : 636 name : ldaps volumeMounts : - name : ldap-data mountPath : /var/lib/ldap - name : ldap-config mountPath : /etc/ldap/slapd.d - name : ldap-certs mountPath : /container/service/slapd/assets/certs - name : memberof-config mountPath : /container/service/slapd/assets/config/bootstrap/ldif/custom/memberOf.ldif subPath : memberOf.ldif - name : container-run mountPath : /container/run volumes : - name : ldap-data persistentVolumeClaim : claimName : ldap-data-pv-claim - name : ldap-config persistentVolumeClaim : claimName : ldap-config-pv-claim - name : ldap-certs persistentVolumeClaim : claimName : ldap-certs-pv-claim - name : memberof-config configMap : name : memberof-config - name : container-run emptyDir : {} ... Note: the volume references the ConfigMap (memberof-config), the volume mount specifies the mountPath as the file you want to replace (/container/service/slapd/assets/config/bootstrap/ldif/custom/memberOf.ldif) and the subPath property is used to reference the file by key from the ConfigMap data (memberOf.ldif). The final manifest we end up with is below. OpenLDAP manifest # file: ~/auth/ldap.yaml apiVersion : v1 kind : ConfigMap metadata : name : memberof-config namespace : auth data : memberOf.ldif : | dn: olcOverlay=memberof,olcDatabase={1}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcMemberOf olcOverlay: memberof olcMemberOfRefint: TRUE --- apiVersion : v1 kind : ConfigMap metadata : name : ldap-config namespace : auth labels : app : ldap tier : backend data : LDAP_ORGANISATION : James Veitch LDAP_DOMAIN : jamesveitch.dev --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-data-pv-claim namespace : auth labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 20Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-config-pv-claim namespace : auth labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-certs-pv-claim namespace : auth labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Service metadata : name : ldap namespace : auth labels : app : ldap tier : backend spec : selector : app : ldap tier : backend ports : - name : ldap protocol : TCP port : 389 targetPort : 389 - name : ldaps-tcp protocol : TCP port : 636 targetPort : 636 - name : ldaps protocol : UDP port : 636 targetPort : 636 --- apiVersion : apps/v1 kind : Deployment metadata : name : ldap-deployment namespace : auth labels : app : ldap tier : backend spec : replicas : 1 selector : matchLabels : app : ldap tier : backend strategy : type : Recreate template : metadata : labels : app : ldap tier : backend spec : containers : - name : ldap image : osixia/openldap args : [ \"--copy-service\" ] env : - name : LDAP_ADMIN_PASSWORD valueFrom : secretKeyRef : name : ldap-user-creds key : password - name : LDAP_CONFIG_PASSWORD valueFrom : secretKeyRef : name : ldap-user-creds key : LDAP_CONFIG_PASSWORD envFrom : - configMapRef : name : ldap-config ports : - containerPort : 389 name : ldap - containerPort : 636 name : ldaps volumeMounts : - name : ldap-data mountPath : /var/lib/ldap - name : ldap-config mountPath : /etc/ldap/slapd.d - name : ldap-certs mountPath : /container/service/slapd/assets/certs - name : memberof-config mountPath : /container/service/slapd/assets/config/bootstrap/ldif/custom/memberOf.ldif subPath : memberOf.ldif - name : container-run mountPath : /container/run volumes : - name : ldap-data persistentVolumeClaim : claimName : ldap-data-pv-claim - name : ldap-config persistentVolumeClaim : claimName : ldap-config-pv-claim - name : ldap-certs persistentVolumeClaim : claimName : ldap-certs-pv-claim - name : memberof-config configMap : name : memberof-config - name : container-run emptyDir : {} Apply this manifest with a kubectl apply -f ~/auth/ldap.yaml and then wait for the services and pods to spin up. $ kubectl apply -f ldap.yaml configmap/memberof-config created configmap/ldap-config created persistentvolumeclaim/ldap-data-pv-claim created persistentvolumeclaim/ldap-config-pv-claim created persistentvolumeclaim/ldap-certs-pv-claim created service/ldap created deployment.apps/ldap-deployment created Admin If you haven't used a pre-canned LDIF file to seed the initial database we can use a friendly admin tool to quickly add some content into the LDAP backend. LDAP Admin manifest # file: ~/auth/ldap-admin.yaml apiVersion : v1 kind : ConfigMap metadata : name : ldapadmin-config namespace : auth labels : app : ldap tier : frontend data : PHPLDAPADMIN_LDAP_HOSTS : \"ldap.auth\" PHPLDAPADMIN_HTTPS : \"false\" --- apiVersion : v1 kind : Service metadata : name : ldapadmin namespace : auth labels : app : ldap tier : frontend spec : type : LoadBalancer selector : app : ldap tier : frontend ports : - name : http protocol : TCP port : 80 targetPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : ldapadmin-deployment namespace : auth labels : app : ldap tier : frontend spec : replicas : 1 selector : matchLabels : app : ldap tier : frontend strategy : type : Recreate template : metadata : labels : app : ldap tier : frontend spec : containers : - name : ldapadmin image : osixia/phpldapadmin envFrom : - configMapRef : name : ldapadmin-config ports : - containerPort : 80 name : http This creates an internal (to the LAN) Service using a LoadBalancer so it appears on the network via MetalLB. To get the External-IP query the services in the namespace. $ kubectl get svc -n auth NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ldap ClusterIP 10 .96.236.102 <none> 389 /TCP,636/TCP,636/UDP 14h ldapadmin LoadBalancer 10 .96.165.63 192 .168.0.201 80 :30158/TCP 14h To login to the admin panel you'll need to use the username of cn=admin,dc=jamesveitch,dc=dev and then the password. One the left you should see something similar to the below. Click to Create new entry here so we can add the necessary Organisational Units we'll require later on. In the screen that appears choose Generic: Organisational Unit and then call it People . Click through the screens and commit to the database. Generic: Organisational Unit If you click on this new ou=People icon and then Add new attribute you'll see down the bottom we can select description from a drop-down box. Type in something descriptive. LDAP tree where users are Add description Additional entries After adding the above, we need to create the following structure, in the following order (LDAP is massively fiddly...). Alternatively you can just import the example data.ldif via the \"Import\" functionality. Organisational Units : Create these in the same way as above ( Generic: Organisational Unit and then add a description attribute). People : LDAP tree where users are Groups : LDAP tree for Groups that users belong to RealmRoles : Keycloak roles that users have KubernetesRoles : Kubernetes roles that users have Users : These sit inside the People OU tree and should be created as a Default \u2192 inetOrgPerson type. See below section on User Attributes before proceeding. Groups : These sit inside the Groups OU tree and should be created as a groupOfNames . These can only be created after you have a user to assign to them. Select cn as the RDN option from the drop-down when prompted. ldap-user : Standard group for all ldap users. Sits inside RealmRoles OU. ldap-admin : Holds general administrators for the domain. Sits inside RealmRoles OU. realm-admin : Admin for Keycloak and LDAP. Sits inside RealmRoles OU. cluster-admin : Admin for the Kubernetes Cluster. Sits inside KubernetesRoles OU. User Attributes You're presented with a bit of an impenetrable screen when you select Default \u2192 inetOrgPerson type. It's representative of the \"standard\" agreed LDAP schema for this type and is split into Required and Optional Attributes. I suggest the following are filled in. Key Value Notes RDN User Name (uid) Name of the attribute used as the \"top\" of a typical user DN (Distinguished Name). cn James First Name sn Veitch Last Name Password supersecurepassword Select the sha512crypt option User Name jamesveitch Actually an alias for uid Export configuration as LDIF The really helpful thing about this admin interface is that it contains an Export functionality we can use to save down a copy of our databse for subsequent seeding / backup. Make sure you set the following: Base DN : use the root dc=jamesveitch,dc=dev or just leave blank Search Scope : Select the entire subtree Attributes : Keep the wildcard * Save as file : Select this Export format : LDIF Line ends : UNIX Export Seed the database with the exported LDIF (or your own) (#TODO: Implement as part of manifest). See seed-ldap-database-with-ldif from the docs. For now you can just \"import\" the LDIF quickly in the admin interface.","title":"OpenLDAP"},{"location":"02.idam/01.openldap/#set-secrets","text":"As we're going to have some username/password combinations defined for our authentication services (e.g. database users, root admin users for LDAP and Keycloak etc.) we should store these as secrets and then add them into the environment or configuration of the containers in the manifests as references (as opposed to being hardcoded). There's two options to initially generate and save these secrets. We'll specify to create them only in the auth namespace. Generic CLI $ kubectl -n auth create secret generic ldap-user-creds \\ --from-literal = LDAP_ADMIN_PASSWORD = admin \\ --from-literal = LDAP_CONFIG_PASSWORD = config secret/ldap-user-creds created Using Manifest # We should `base64` encode the secrets first. $ echo -n 'admin' | base64 YWRtaW4 = $ echo -n 'config' | base64 Y29uZmln --- # Now use these base64 secrets in the manifest --- # file: ~/auth/ldap-user-creds-secret.yaml apiVersion: v1 kind: Secret metadata: name: ldap-user-creds namespace: auth type: Opaque data: LDAP_ADMIN_PASSWORD: YWRtaW4 = LDAP_CONFIG_PASSWORD: Y29uZmln For either of the above methods for storing the secret you then access the key,value combinations in the deployment manifest like follows: envFrom envFrom : # Secrets - secretRef : name : ldap-user-creds # Normal plaintext config - configMapRef : name : ldap-config env env : - name : LDAP_ADMIN_PASSWORD valueFrom : secretKeyRef : name : ldap-user-creds key : password - name : LDAP_CONFIG_PASSWORD valueFrom : secretKeyRef : name : ldap-user-creds key : LDAP_CONFIG_PASSWORD posixGroup, groupOfNames or groupOfUniqueNames I went down the rabbithole all the way back to the 2006 mailing list in order to work out what the accepted wisdom is for creating user groups and then ensuring referential integrity with a backref on the user object. As per the official docs it turns out there's a memberOf attribute that can be maintained on an object to show what groups they are members of. Sounds simple? In some scenarios, it may be desirable for a client to be able to determine which groups an entry is a member of, without performing an additional search. Examples of this are applications using the DIT for access control based on group authorization. The memberof overlay updates an attribute (by default memberOf) whenever changes occur to the membership attribute (by default member) of entries of the objectclass (by default groupOfNames) configured to trigger updates. Thus, it provides maintenance of the list of groups an entry is a member of, when usual maintenance of groups is done by modifying the members on the group entry. It's actually, genuinely, rocket science to get anything like this properly working (or even find some docs that explain it). In order to enable this functionality I found you need to modify the OpenLDAP installation to specifically ask for memberships to be maintained. # file: ~/auth/memberOf.ldif dn: olcOverlay=memberof,olcDatabase={1}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcMemberOf olcOverlay: memberof olcMemberOfRefint: TRUE You can apply this manually with ldapadd -Y EXTERNAL -H ldapi:/// -f memberOf.ldif or, as explained in the docker image notes, we can Seed the ldap database with a ldif . We need to add this file into the /container/service/slapd/assets/config/bootstrap/ldif/custom and then add --copy-service to the startup args of the container. In the world of Kubernetes this means mounting via a ConfigMap. apiVersion : v1 kind : ConfigMap metadata : name : memberof-config namespace : auth data : memberOf.ldif : | dn: olcOverlay=memberof,olcDatabase={1}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcMemberOf olcOverlay: memberof olcMemberOfRefint: TRUE Note: I\u2019m using the name of the file as the key. We can then add this into the volumes and volumeMounts of the container. container ... containers : - name : ldap image : osixia/openldap args : [ \"--copy-service\" ] envFrom : - configMapRef : name : ldap-config ports : - containerPort : 389 name : ldap - containerPort : 636 name : ldaps volumeMounts : - name : ldap-data mountPath : /var/lib/ldap - name : ldap-config mountPath : /etc/ldap/slapd.d - name : ldap-certs mountPath : /container/service/slapd/assets/certs - name : memberof-config mountPath : /container/service/slapd/assets/config/bootstrap/ldif/custom/memberOf.ldif subPath : memberOf.ldif - name : container-run mountPath : /container/run volumes : - name : ldap-data persistentVolumeClaim : claimName : ldap-data-pv-claim - name : ldap-config persistentVolumeClaim : claimName : ldap-config-pv-claim - name : ldap-certs persistentVolumeClaim : claimName : ldap-certs-pv-claim - name : memberof-config configMap : name : memberof-config - name : container-run emptyDir : {} ... Note: the volume references the ConfigMap (memberof-config), the volume mount specifies the mountPath as the file you want to replace (/container/service/slapd/assets/config/bootstrap/ldif/custom/memberOf.ldif) and the subPath property is used to reference the file by key from the ConfigMap data (memberOf.ldif). The final manifest we end up with is below. OpenLDAP manifest # file: ~/auth/ldap.yaml apiVersion : v1 kind : ConfigMap metadata : name : memberof-config namespace : auth data : memberOf.ldif : | dn: olcOverlay=memberof,olcDatabase={1}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcMemberOf olcOverlay: memberof olcMemberOfRefint: TRUE --- apiVersion : v1 kind : ConfigMap metadata : name : ldap-config namespace : auth labels : app : ldap tier : backend data : LDAP_ORGANISATION : James Veitch LDAP_DOMAIN : jamesveitch.dev --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-data-pv-claim namespace : auth labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 20Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-config-pv-claim namespace : auth labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-certs-pv-claim namespace : auth labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Service metadata : name : ldap namespace : auth labels : app : ldap tier : backend spec : selector : app : ldap tier : backend ports : - name : ldap protocol : TCP port : 389 targetPort : 389 - name : ldaps-tcp protocol : TCP port : 636 targetPort : 636 - name : ldaps protocol : UDP port : 636 targetPort : 636 --- apiVersion : apps/v1 kind : Deployment metadata : name : ldap-deployment namespace : auth labels : app : ldap tier : backend spec : replicas : 1 selector : matchLabels : app : ldap tier : backend strategy : type : Recreate template : metadata : labels : app : ldap tier : backend spec : containers : - name : ldap image : osixia/openldap args : [ \"--copy-service\" ] env : - name : LDAP_ADMIN_PASSWORD valueFrom : secretKeyRef : name : ldap-user-creds key : password - name : LDAP_CONFIG_PASSWORD valueFrom : secretKeyRef : name : ldap-user-creds key : LDAP_CONFIG_PASSWORD envFrom : - configMapRef : name : ldap-config ports : - containerPort : 389 name : ldap - containerPort : 636 name : ldaps volumeMounts : - name : ldap-data mountPath : /var/lib/ldap - name : ldap-config mountPath : /etc/ldap/slapd.d - name : ldap-certs mountPath : /container/service/slapd/assets/certs - name : memberof-config mountPath : /container/service/slapd/assets/config/bootstrap/ldif/custom/memberOf.ldif subPath : memberOf.ldif - name : container-run mountPath : /container/run volumes : - name : ldap-data persistentVolumeClaim : claimName : ldap-data-pv-claim - name : ldap-config persistentVolumeClaim : claimName : ldap-config-pv-claim - name : ldap-certs persistentVolumeClaim : claimName : ldap-certs-pv-claim - name : memberof-config configMap : name : memberof-config - name : container-run emptyDir : {} Apply this manifest with a kubectl apply -f ~/auth/ldap.yaml and then wait for the services and pods to spin up. $ kubectl apply -f ldap.yaml configmap/memberof-config created configmap/ldap-config created persistentvolumeclaim/ldap-data-pv-claim created persistentvolumeclaim/ldap-config-pv-claim created persistentvolumeclaim/ldap-certs-pv-claim created service/ldap created deployment.apps/ldap-deployment created","title":"Set Secrets"},{"location":"02.idam/01.openldap/#admin","text":"If you haven't used a pre-canned LDIF file to seed the initial database we can use a friendly admin tool to quickly add some content into the LDAP backend. LDAP Admin manifest # file: ~/auth/ldap-admin.yaml apiVersion : v1 kind : ConfigMap metadata : name : ldapadmin-config namespace : auth labels : app : ldap tier : frontend data : PHPLDAPADMIN_LDAP_HOSTS : \"ldap.auth\" PHPLDAPADMIN_HTTPS : \"false\" --- apiVersion : v1 kind : Service metadata : name : ldapadmin namespace : auth labels : app : ldap tier : frontend spec : type : LoadBalancer selector : app : ldap tier : frontend ports : - name : http protocol : TCP port : 80 targetPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : ldapadmin-deployment namespace : auth labels : app : ldap tier : frontend spec : replicas : 1 selector : matchLabels : app : ldap tier : frontend strategy : type : Recreate template : metadata : labels : app : ldap tier : frontend spec : containers : - name : ldapadmin image : osixia/phpldapadmin envFrom : - configMapRef : name : ldapadmin-config ports : - containerPort : 80 name : http This creates an internal (to the LAN) Service using a LoadBalancer so it appears on the network via MetalLB. To get the External-IP query the services in the namespace. $ kubectl get svc -n auth NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ldap ClusterIP 10 .96.236.102 <none> 389 /TCP,636/TCP,636/UDP 14h ldapadmin LoadBalancer 10 .96.165.63 192 .168.0.201 80 :30158/TCP 14h To login to the admin panel you'll need to use the username of cn=admin,dc=jamesveitch,dc=dev and then the password. One the left you should see something similar to the below. Click to Create new entry here so we can add the necessary Organisational Units we'll require later on. In the screen that appears choose Generic: Organisational Unit and then call it People . Click through the screens and commit to the database. Generic: Organisational Unit If you click on this new ou=People icon and then Add new attribute you'll see down the bottom we can select description from a drop-down box. Type in something descriptive. LDAP tree where users are Add description","title":"Admin"},{"location":"02.idam/01.openldap/#additional-entries","text":"After adding the above, we need to create the following structure, in the following order (LDAP is massively fiddly...). Alternatively you can just import the example data.ldif via the \"Import\" functionality. Organisational Units : Create these in the same way as above ( Generic: Organisational Unit and then add a description attribute). People : LDAP tree where users are Groups : LDAP tree for Groups that users belong to RealmRoles : Keycloak roles that users have KubernetesRoles : Kubernetes roles that users have Users : These sit inside the People OU tree and should be created as a Default \u2192 inetOrgPerson type. See below section on User Attributes before proceeding. Groups : These sit inside the Groups OU tree and should be created as a groupOfNames . These can only be created after you have a user to assign to them. Select cn as the RDN option from the drop-down when prompted. ldap-user : Standard group for all ldap users. Sits inside RealmRoles OU. ldap-admin : Holds general administrators for the domain. Sits inside RealmRoles OU. realm-admin : Admin for Keycloak and LDAP. Sits inside RealmRoles OU. cluster-admin : Admin for the Kubernetes Cluster. Sits inside KubernetesRoles OU. User Attributes You're presented with a bit of an impenetrable screen when you select Default \u2192 inetOrgPerson type. It's representative of the \"standard\" agreed LDAP schema for this type and is split into Required and Optional Attributes. I suggest the following are filled in. Key Value Notes RDN User Name (uid) Name of the attribute used as the \"top\" of a typical user DN (Distinguished Name). cn James First Name sn Veitch Last Name Password supersecurepassword Select the sha512crypt option User Name jamesveitch Actually an alias for uid","title":"Additional entries"},{"location":"02.idam/01.openldap/#export-configuration-as-ldif","text":"The really helpful thing about this admin interface is that it contains an Export functionality we can use to save down a copy of our databse for subsequent seeding / backup. Make sure you set the following: Base DN : use the root dc=jamesveitch,dc=dev or just leave blank Search Scope : Select the entire subtree Attributes : Keep the wildcard * Save as file : Select this Export format : LDIF Line ends : UNIX Export","title":"Export configuration as LDIF"},{"location":"02.idam/01.openldap/#seed-the-database-with-the-exported-ldif-or-your-own","text":"(#TODO: Implement as part of manifest). See seed-ldap-database-with-ldif from the docs. For now you can just \"import\" the LDIF quickly in the admin interface.","title":"Seed the database with the exported LDIF (or your own)"},{"location":"02.idam/02.keycloak/","text":"Keycloak is an application from Red Hat which allows you to provide a variety of authentication options to applications you run from a single source. It also includes User Federation, Identity Brokering and Social Login. Here's a picture of the current capabilities advertised on their website. Whilst we could link Keycloak to a provider like GitHub (which would allow you to login to other applications with your GitHub username/password combination), instead we're going to manage our own users and Groups in OpenLDAP and have Keycloak sat in front of this directory server. This provides us much more scalability, control and flexibility. In order to deploy and run Keycloak we need the following: keycloak postgres Documentation is available on Docker Hub Set Secrets As with OpenLDAP we will store our sensitive credentials into a secret. Feel free to use the manifest approach. Keycloak $ kubectl -n auth create secret generic keycloak-db-creds \\ --from-literal = DB_USER = postgres \\ --from-literal = DB_PASSWORD = admin secret/keycloak-db-creds created $ kubectl -n auth create secret generic keycloak-user-creds \\ --from-literal = KEYCLOAK_USER = admin \\ --from-literal = KEYCLOAK_PASSWORD = password secret/keycloak-user-creds created Database We want to use the same databse credentials but the postgres container uses a different environment variable name. As such we will remap to a different key in the spec for the container. Keycloak Postgres POSTGRES_USER DB_USER POSTGRES_PASSWORD DB_PASSWORD The mapping will look like this. ... spec : containers : - name : keycloak-db image : postgres:alpine env : # Secrets - name : POSTGRES_USER valueFrom : secretKeyRef : name : keycloak-db-creds key : DB_USER - name : POSTGRES_PASSWORD secretKeyRef : name : keycloak-db-creds key : DB_PASSWORD # Regular environment variables from configmap envFrom : - configMapRef : name : keycloak-db-config ports : - containerPort : 5432 ... Deploy Keycloak Keycloak manifest # file: ~/auth/keycloak.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : keycloak namespace : auth spec : secretName : keycloak-certs duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : auth.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - auth.jamesveitch.dev issuerRef : name : letsencrypt kind : ClusterIssuer group : cert-manager.io --- apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : keycloak-external-ingress namespace : auth labels : app : keycloak tier : frontend annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - auth.jamesveitch.dev secretName : keycloak-certs rules : - host : auth.jamesveitch.dev http : paths : - path : / backend : serviceName : keycloak servicePort : 8080 --- apiVersion : v1 kind : Service metadata : name : keycloak namespace : auth labels : app : keycloak tier : frontend spec : selector : app : keycloak tier : frontend ports : - name : http protocol : TCP port : 8080 targetPort : 8080 --- apiVersion : v1 kind : ConfigMap metadata : name : keycloak-config namespace : auth labels : app : keycloak tier : frontend data : DB_ADDR : keycloak-db.auth # This is required to run keycloak behind a service,ingress etc PROXY_ADDRESS_FORWARDING : \"true\" --- apiVersion : apps/v1 kind : Deployment metadata : name : keycloak-deployment namespace : auth labels : app : keycloak tier : frontend spec : replicas : 1 selector : matchLabels : app : keycloak tier : frontend strategy : type : Recreate template : metadata : labels : app : keycloak tier : frontend spec : containers : - name : keycloak image : jboss/keycloak envFrom : - secretRef : name : keycloak-db-creds - secretRef : name : keycloak-user-creds - configMapRef : name : keycloak-config ports : - containerPort : 8080 name : keycloak --- apiVersion : v1 kind : ConfigMap metadata : name : keycloak-db-config namespace : auth labels : app : keycloak tier : postgres data : POSTGRES_DB : keycloak PGDATA : /var/lib/postgresql/data/pgdata --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : keycloak-db-data-pv-claim namespace : auth labels : app : keycloak tier : postgres spec : accessModes : - ReadWriteOnce resources : requests : storage : 20Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : keycloak-db-config-pv-claim namespace : auth labels : app : keycloak tier : postgres spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Service metadata : name : keycloak-db namespace : auth labels : app : keycloak tier : postgres spec : selector : app : keycloak tier : postgres ports : - name : postgres protocol : TCP port : 5432 targetPort : 5432 --- apiVersion : apps/v1 kind : Deployment metadata : name : keycloak-db-deployment namespace : auth labels : app : keycloak tier : postgres spec : replicas : 1 selector : matchLabels : app : keycloak tier : postgres strategy : type : Recreate template : metadata : labels : app : keycloak tier : postgres spec : containers : - name : keycloak-db image : postgres:alpine env : # Secrets - name : POSTGRES_USER valueFrom : secretKeyRef : name : keycloak-db-creds key : DB_USER - name : POSTGRES_PASSWORD valueFrom : secretKeyRef : name : keycloak-db-creds key : DB_PASSWORD envFrom : - configMapRef : name : keycloak-db-config ports : - containerPort : 5432 name : postgres volumeMounts : - name : keycloak-db-data mountPath : /var/lib/postgresql/data - name : keycloak-db-config mountPath : /usr/share/postgresql volumes : - name : keycloak-db-data persistentVolumeClaim : claimName : keycloak-db-data-pv-claim - name : keycloak-db-config persistentVolumeClaim : claimName : keycloak-db-config-pv-claim Apply the manifest with kubectl apply -f ~/auth/keycloak.yaml and then wait for the pods, services, ingress and certificate to be provisioned. The keycloak container can take up to a minute to properly start up so you can monitor this be looking at the logs for the keycloak-deployment* pod. $ kubectl get pod -n auth NAME READY STATUS RESTARTS AGE keycloak-db-deployment-5d8846dbf9-2dfdh 1 /1 Running 0 14h keycloak-deployment-88bc75877-f8cbr 1 /1 Running 0 14h ldap-deployment-7864dd96cf-9jvmz 1 /1 Running 0 15h ldapadmin-deployment-7575c6d9dc-vb47b 1 /1 Running 0 15h $ kubectl -n auth logs keycloak-deployment-88bc75877-f8cbr ... 23 :56:53,676 INFO [ org.jboss.as ] ( Controller Boot Thread ) WFLYSRV0060: Http management interface listening on http://127.0.0.1:9990/management 23 :56:53,676 INFO [ org.jboss.as ] ( Controller Boot Thread ) WFLYSRV0051: Admin console listening on http://127.0.0.1:9990 23 :56:53,676 INFO [ org.jboss.as ] ( Controller Boot Thread ) WFLYSRV0025: Keycloak 8 .0.1 ( WildFly Core 10 .0.3.Final ) started in 29506ms - Started 684 of 989 services ( 701 services are lazy, passive or on-demand ) Setup Realm As per the docs Realm A realm manages a set of users, credentials, roles, and groups. A user belongs to and logs into a realm. Realms are isolated from one another and can only manage and authenticate the users that they control. We'll create a realm specifically for jamesveitch.dev . Navigate to auth.jamesveitch.dev \u2192 Administration Console and login with the KEYCLOAK_USER and KEYCLOAK_PASSWORD set in the deployment. This will get you into the Master realm initially. Hover over the drop down arrow next to Master in the top left and select Add realm . Call it development . Add realm User Federation Identity Provider (LDAP) To configure Keycloak to work with OpenLDAP we need to login and setup our ldap container as a User Federation provider. Settings Key settings you'll need to modify are as follows: Edit Mode : WRITABLE Sync Registrations : On Vendor : Other Connection URL : ldap://ldap.auth Users DN : ou=People,dc=jamesveitch,dc=dev Bind DN : cn=admin,dc=jamesveitch,dc=dev Bind Credential : Search Scope : Subtree (Optionally) Periodic Full Sync : On (and keep defaults for the Period ) Periodic Changed Users Sync : On (and set Period to 5 ) Identity Provider Configuration (Settings) Hit Save \u2192 Synchronize all users and you should see a success flash message appear at the top of the screen. Mappers Mappers help to provide a translation layer between how data is stored in the provider and how we'd like to use it in Keycloak. For example, there are some standard ones created for you automatically based on your selection of the Vendor in the previous Settings tab. Clicking on the username will show you that: Type : It's a user-attribute mapper (used to map a single attribute from a LDAP user to the Keycloak user model) Model Attribute : This is what the attribute will be called in Keycloak LDAP Attribute : What the attribute is called in LDAP Username attribute-ldap-mapper We are going to add some of our own custom mappers so that we can do things like identify what security groups a user is part of and, therefore, what resources they should be able to access inside Kubernetes. Create Groups and Mappers As we created a number of Groups in our previous step (or have seeded with an LDIF) we need to tell Keycloak how to find and interpret them. Go back into our User Federation \u2192 Ldap \u2192 Mappers and create two new ones. Realm Roles This is a special mapping which will allow us to automatically allocate people to the right roles within the Keycloak administration setup. Name : realm roles Type : role-ldap-mapper LDAP Groups DN : ou=RealmRoles,ou=Groups,dc=jamesveitch,dc=dev Use Realm Roles Mapping : ON Click Save \u2192 Sync LDAP Roles To Keycloak Navigating to Roles in the menu should show now our LDAP roles populated as ldap-admin , ldap-user and realm-admin In addition, navigating to Users and then clicking into our user should show, on the Role Mappings tab, that we have these roles assigned to us. We need to give this role some powers. In order to administer the development realm click on the realm-admin role and then turn Composite Roles to ON . From the Client Roles drop-down select realm-management then assign realm-admin . Assign realm-admin powers Navigate to https://auth.jamesveitch.dev/auth/realms/development/account/ and you should be able to login now as that user and manage your account. Navigate to https://auth.jamesveitch.dev/auth/admin/development/console and you should be able to administer the domain. Groups This mapping will allow us to find any groups within the Groups OU and then map them automatically to users. Name : groups Type : group-ldap-mapper LDAP Groups DN : ou=Groups,dc=jamesveitch,dc=dev Mapped Group Attributes : description Click on the Sync LDAP Groups To Keycloak button on each one down the bottom after saving the mapper and ensure you get a success overlay message that it's now imported the Groups we created earlier. Go to the Settings tab and then, down the bottom, click Synchronize all users to get a message about one user being updated (because it's now spotted some groups we're part of). Users Go to Users and then click View all users to force it to perform an initial sync. You should see your user appear with an additional admin user (if using Master realm). If you now select your user you should note on the Groups tab you have a Group Membership identified for all of the groups you're part of. In addition you have admin as an Effective Role in the Role Mappings tab (once you select realm-management from the Client Roles drop down) because it's mapped this through from your membership of the realm-admin group. User: Group Membership User: Role Mapping Enabling MFA Stealing with pride from documentation elsewhere we're going to enable TOTP based MFA for our initial user. In the GUI you would navigate to Authentication \u2192 OTP Policy and then update the following settings as required. The below are those we're using: OTP Type : Time Based OTP Hash Algorith : SHA512 Number of Digits : 8 Look Ahead Window : 3 OTP Token Period : 30 Depending on appetite we can also navigate to the Authentication \u2192 Required Actions tab and tick the Default Action box against Configure OTP if we want to enforce this for everyone by default. Impersonate user Navigate to Users and then select Impersonate next to ours. This should change and give you a different screen where we can now setup MFA for our user. Select the Authenticator option and follow the instructions to get setup. Login Logout using the Sign Out button in the top right of the screen and then attempt to sign back in with your new user. You'll be presented with the below requiring you to input a code from your app. User management User Management Going forwards you can use use Keyckloak as a GUI to manage users and assign them to groups and it will be persisted back into LDAP. It's much more user friendly and removes another requirement so we'll delete the ldap-admin resources. kubectl delete all -l 'app=ldap, tier=frontend' -n auth Bootstrapping We'll now bootstrap this configuration into Keycloak via the Importing a realm option. The easiest way to get your configuration back out of the system is to go the Export setting and then get the JSON output. Select both the options for Export groups and role and Export clients and then hit the Export button to download a JSON payload. If you open it up you'll see a lot of familiar things in there based on the steps above. { \"id\" : \"Development\" , \"realm\" : \"development\" , ... \"roles\" : { \"realm\" : [ ... { \"id\" : \"1e155ef7-2dfc-4c47-99f5-c86a1adec4a3\" , \"name\" : \"realm-admin\" , \"composite\" : true , \"composites\" : { \"client\" : { \"realm-management\" : [ \"realm-admin\" ] ... }, For this to be automatically discovered by Keycloak (and therefore used as seed configuration data) we need to do two things: Mount this JSON file into the container Set an environment variable KEYCLOAK_IMPORT to the path of the mounted file Unfortunately, due to the way in which the image is configured, I found in the past that the given method in the docs doesn't work (as volumes are mounted by root yet the application executes as the jboss user and therefore can't access the files). As a result we'll inherit from and build a custom image with a /realms folder that we can mount the JSON files into. Issue #23 is currently Open for this when I get a chance to implement.","title":"Keycloak"},{"location":"02.idam/02.keycloak/#set-secrets","text":"As with OpenLDAP we will store our sensitive credentials into a secret. Feel free to use the manifest approach. Keycloak $ kubectl -n auth create secret generic keycloak-db-creds \\ --from-literal = DB_USER = postgres \\ --from-literal = DB_PASSWORD = admin secret/keycloak-db-creds created $ kubectl -n auth create secret generic keycloak-user-creds \\ --from-literal = KEYCLOAK_USER = admin \\ --from-literal = KEYCLOAK_PASSWORD = password secret/keycloak-user-creds created Database We want to use the same databse credentials but the postgres container uses a different environment variable name. As such we will remap to a different key in the spec for the container. Keycloak Postgres POSTGRES_USER DB_USER POSTGRES_PASSWORD DB_PASSWORD The mapping will look like this. ... spec : containers : - name : keycloak-db image : postgres:alpine env : # Secrets - name : POSTGRES_USER valueFrom : secretKeyRef : name : keycloak-db-creds key : DB_USER - name : POSTGRES_PASSWORD secretKeyRef : name : keycloak-db-creds key : DB_PASSWORD # Regular environment variables from configmap envFrom : - configMapRef : name : keycloak-db-config ports : - containerPort : 5432 ...","title":"Set Secrets"},{"location":"02.idam/02.keycloak/#deploy-keycloak","text":"Keycloak manifest # file: ~/auth/keycloak.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : keycloak namespace : auth spec : secretName : keycloak-certs duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : auth.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - auth.jamesveitch.dev issuerRef : name : letsencrypt kind : ClusterIssuer group : cert-manager.io --- apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : keycloak-external-ingress namespace : auth labels : app : keycloak tier : frontend annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - auth.jamesveitch.dev secretName : keycloak-certs rules : - host : auth.jamesveitch.dev http : paths : - path : / backend : serviceName : keycloak servicePort : 8080 --- apiVersion : v1 kind : Service metadata : name : keycloak namespace : auth labels : app : keycloak tier : frontend spec : selector : app : keycloak tier : frontend ports : - name : http protocol : TCP port : 8080 targetPort : 8080 --- apiVersion : v1 kind : ConfigMap metadata : name : keycloak-config namespace : auth labels : app : keycloak tier : frontend data : DB_ADDR : keycloak-db.auth # This is required to run keycloak behind a service,ingress etc PROXY_ADDRESS_FORWARDING : \"true\" --- apiVersion : apps/v1 kind : Deployment metadata : name : keycloak-deployment namespace : auth labels : app : keycloak tier : frontend spec : replicas : 1 selector : matchLabels : app : keycloak tier : frontend strategy : type : Recreate template : metadata : labels : app : keycloak tier : frontend spec : containers : - name : keycloak image : jboss/keycloak envFrom : - secretRef : name : keycloak-db-creds - secretRef : name : keycloak-user-creds - configMapRef : name : keycloak-config ports : - containerPort : 8080 name : keycloak --- apiVersion : v1 kind : ConfigMap metadata : name : keycloak-db-config namespace : auth labels : app : keycloak tier : postgres data : POSTGRES_DB : keycloak PGDATA : /var/lib/postgresql/data/pgdata --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : keycloak-db-data-pv-claim namespace : auth labels : app : keycloak tier : postgres spec : accessModes : - ReadWriteOnce resources : requests : storage : 20Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : keycloak-db-config-pv-claim namespace : auth labels : app : keycloak tier : postgres spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Service metadata : name : keycloak-db namespace : auth labels : app : keycloak tier : postgres spec : selector : app : keycloak tier : postgres ports : - name : postgres protocol : TCP port : 5432 targetPort : 5432 --- apiVersion : apps/v1 kind : Deployment metadata : name : keycloak-db-deployment namespace : auth labels : app : keycloak tier : postgres spec : replicas : 1 selector : matchLabels : app : keycloak tier : postgres strategy : type : Recreate template : metadata : labels : app : keycloak tier : postgres spec : containers : - name : keycloak-db image : postgres:alpine env : # Secrets - name : POSTGRES_USER valueFrom : secretKeyRef : name : keycloak-db-creds key : DB_USER - name : POSTGRES_PASSWORD valueFrom : secretKeyRef : name : keycloak-db-creds key : DB_PASSWORD envFrom : - configMapRef : name : keycloak-db-config ports : - containerPort : 5432 name : postgres volumeMounts : - name : keycloak-db-data mountPath : /var/lib/postgresql/data - name : keycloak-db-config mountPath : /usr/share/postgresql volumes : - name : keycloak-db-data persistentVolumeClaim : claimName : keycloak-db-data-pv-claim - name : keycloak-db-config persistentVolumeClaim : claimName : keycloak-db-config-pv-claim Apply the manifest with kubectl apply -f ~/auth/keycloak.yaml and then wait for the pods, services, ingress and certificate to be provisioned. The keycloak container can take up to a minute to properly start up so you can monitor this be looking at the logs for the keycloak-deployment* pod. $ kubectl get pod -n auth NAME READY STATUS RESTARTS AGE keycloak-db-deployment-5d8846dbf9-2dfdh 1 /1 Running 0 14h keycloak-deployment-88bc75877-f8cbr 1 /1 Running 0 14h ldap-deployment-7864dd96cf-9jvmz 1 /1 Running 0 15h ldapadmin-deployment-7575c6d9dc-vb47b 1 /1 Running 0 15h $ kubectl -n auth logs keycloak-deployment-88bc75877-f8cbr ... 23 :56:53,676 INFO [ org.jboss.as ] ( Controller Boot Thread ) WFLYSRV0060: Http management interface listening on http://127.0.0.1:9990/management 23 :56:53,676 INFO [ org.jboss.as ] ( Controller Boot Thread ) WFLYSRV0051: Admin console listening on http://127.0.0.1:9990 23 :56:53,676 INFO [ org.jboss.as ] ( Controller Boot Thread ) WFLYSRV0025: Keycloak 8 .0.1 ( WildFly Core 10 .0.3.Final ) started in 29506ms - Started 684 of 989 services ( 701 services are lazy, passive or on-demand )","title":"Deploy Keycloak"},{"location":"02.idam/02.keycloak/#setup-realm","text":"As per the docs Realm A realm manages a set of users, credentials, roles, and groups. A user belongs to and logs into a realm. Realms are isolated from one another and can only manage and authenticate the users that they control. We'll create a realm specifically for jamesveitch.dev . Navigate to auth.jamesveitch.dev \u2192 Administration Console and login with the KEYCLOAK_USER and KEYCLOAK_PASSWORD set in the deployment. This will get you into the Master realm initially. Hover over the drop down arrow next to Master in the top left and select Add realm . Call it development . Add realm","title":"Setup Realm"},{"location":"02.idam/02.keycloak/#user-federation-identity-provider-ldap","text":"To configure Keycloak to work with OpenLDAP we need to login and setup our ldap container as a User Federation provider.","title":"User Federation Identity Provider (LDAP)"},{"location":"02.idam/02.keycloak/#settings","text":"Key settings you'll need to modify are as follows: Edit Mode : WRITABLE Sync Registrations : On Vendor : Other Connection URL : ldap://ldap.auth Users DN : ou=People,dc=jamesveitch,dc=dev Bind DN : cn=admin,dc=jamesveitch,dc=dev Bind Credential : Search Scope : Subtree (Optionally) Periodic Full Sync : On (and keep defaults for the Period ) Periodic Changed Users Sync : On (and set Period to 5 ) Identity Provider Configuration (Settings) Hit Save \u2192 Synchronize all users and you should see a success flash message appear at the top of the screen.","title":"Settings"},{"location":"02.idam/02.keycloak/#mappers","text":"Mappers help to provide a translation layer between how data is stored in the provider and how we'd like to use it in Keycloak. For example, there are some standard ones created for you automatically based on your selection of the Vendor in the previous Settings tab. Clicking on the username will show you that: Type : It's a user-attribute mapper (used to map a single attribute from a LDAP user to the Keycloak user model) Model Attribute : This is what the attribute will be called in Keycloak LDAP Attribute : What the attribute is called in LDAP Username attribute-ldap-mapper We are going to add some of our own custom mappers so that we can do things like identify what security groups a user is part of and, therefore, what resources they should be able to access inside Kubernetes.","title":"Mappers"},{"location":"02.idam/02.keycloak/#create-groups-and-mappers","text":"As we created a number of Groups in our previous step (or have seeded with an LDIF) we need to tell Keycloak how to find and interpret them. Go back into our User Federation \u2192 Ldap \u2192 Mappers and create two new ones.","title":"Create Groups and Mappers"},{"location":"02.idam/02.keycloak/#realm-roles","text":"This is a special mapping which will allow us to automatically allocate people to the right roles within the Keycloak administration setup. Name : realm roles Type : role-ldap-mapper LDAP Groups DN : ou=RealmRoles,ou=Groups,dc=jamesveitch,dc=dev Use Realm Roles Mapping : ON Click Save \u2192 Sync LDAP Roles To Keycloak Navigating to Roles in the menu should show now our LDAP roles populated as ldap-admin , ldap-user and realm-admin In addition, navigating to Users and then clicking into our user should show, on the Role Mappings tab, that we have these roles assigned to us. We need to give this role some powers. In order to administer the development realm click on the realm-admin role and then turn Composite Roles to ON . From the Client Roles drop-down select realm-management then assign realm-admin . Assign realm-admin powers Navigate to https://auth.jamesveitch.dev/auth/realms/development/account/ and you should be able to login now as that user and manage your account. Navigate to https://auth.jamesveitch.dev/auth/admin/development/console and you should be able to administer the domain.","title":"Realm Roles"},{"location":"02.idam/02.keycloak/#groups","text":"This mapping will allow us to find any groups within the Groups OU and then map them automatically to users. Name : groups Type : group-ldap-mapper LDAP Groups DN : ou=Groups,dc=jamesveitch,dc=dev Mapped Group Attributes : description Click on the Sync LDAP Groups To Keycloak button on each one down the bottom after saving the mapper and ensure you get a success overlay message that it's now imported the Groups we created earlier. Go to the Settings tab and then, down the bottom, click Synchronize all users to get a message about one user being updated (because it's now spotted some groups we're part of).","title":"Groups"},{"location":"02.idam/02.keycloak/#users","text":"Go to Users and then click View all users to force it to perform an initial sync. You should see your user appear with an additional admin user (if using Master realm). If you now select your user you should note on the Groups tab you have a Group Membership identified for all of the groups you're part of. In addition you have admin as an Effective Role in the Role Mappings tab (once you select realm-management from the Client Roles drop down) because it's mapped this through from your membership of the realm-admin group. User: Group Membership User: Role Mapping","title":"Users"},{"location":"02.idam/02.keycloak/#enabling-mfa","text":"Stealing with pride from documentation elsewhere we're going to enable TOTP based MFA for our initial user. In the GUI you would navigate to Authentication \u2192 OTP Policy and then update the following settings as required. The below are those we're using: OTP Type : Time Based OTP Hash Algorith : SHA512 Number of Digits : 8 Look Ahead Window : 3 OTP Token Period : 30 Depending on appetite we can also navigate to the Authentication \u2192 Required Actions tab and tick the Default Action box against Configure OTP if we want to enforce this for everyone by default.","title":"Enabling MFA"},{"location":"02.idam/02.keycloak/#impersonate-user","text":"Navigate to Users and then select Impersonate next to ours. This should change and give you a different screen where we can now setup MFA for our user. Select the Authenticator option and follow the instructions to get setup.","title":"Impersonate user"},{"location":"02.idam/02.keycloak/#login","text":"Logout using the Sign Out button in the top right of the screen and then attempt to sign back in with your new user. You'll be presented with the below requiring you to input a code from your app.","title":"Login"},{"location":"02.idam/02.keycloak/#user-management","text":"User Management Going forwards you can use use Keyckloak as a GUI to manage users and assign them to groups and it will be persisted back into LDAP. It's much more user friendly and removes another requirement so we'll delete the ldap-admin resources. kubectl delete all -l 'app=ldap, tier=frontend' -n auth","title":"User management"},{"location":"02.idam/02.keycloak/#bootstrapping","text":"We'll now bootstrap this configuration into Keycloak via the Importing a realm option. The easiest way to get your configuration back out of the system is to go the Export setting and then get the JSON output. Select both the options for Export groups and role and Export clients and then hit the Export button to download a JSON payload. If you open it up you'll see a lot of familiar things in there based on the steps above. { \"id\" : \"Development\" , \"realm\" : \"development\" , ... \"roles\" : { \"realm\" : [ ... { \"id\" : \"1e155ef7-2dfc-4c47-99f5-c86a1adec4a3\" , \"name\" : \"realm-admin\" , \"composite\" : true , \"composites\" : { \"client\" : { \"realm-management\" : [ \"realm-admin\" ] ... }, For this to be automatically discovered by Keycloak (and therefore used as seed configuration data) we need to do two things: Mount this JSON file into the container Set an environment variable KEYCLOAK_IMPORT to the path of the mounted file Unfortunately, due to the way in which the image is configured, I found in the past that the given method in the docs doesn't work (as volumes are mounted by root yet the application executes as the jboss user and therefore can't access the files). As a result we'll inherit from and build a custom image with a /realms folder that we can mount the JSON files into. Issue #23 is currently Open for this when I get a chance to implement.","title":"Bootstrapping"},{"location":"02.idam/03.integration/00.integration/","text":"The following sections will now detail how to integrate some of our existing services with our new Identity Provider. Kubernetes API Rook+Ceph Grafana","title":"Overview"},{"location":"02.idam/03.integration/01.kubernetes/","text":"There's an excellent article on Medium which partially covers what we're about to do and helped to solidify some of my thoughts. Highly worth a read for some background. In addition there's some official docs on OpenID Connect Tokens . At the bottom of this page I've included links to all of the references I ended up needing to make this work. We're going to now integrate our Kubernetes cluster's Authentication (identify someone) and Authorisation (what they should be able to do) with our Keycloak+OpenLDAP implementation. Initially I'm going to prove that we should be able to login as jamesveitch (my uid ). LDIF Export: James Veitch Remember we exported the LDIF? If you review the file you'll see the following. # Entry 11: cn=James Veitch,ou=People,dc=jamesveitch,dc=dev dn: cn=James Veitch,ou=People,dc=jamesveitch,dc=dev cn: James Veitch gidnumber: 500 givenname: James homedirectory: /home/users/jamesveitch loginshell: /bin/bash objectclass: inetOrgPerson objectclass: posixAccount objectclass: top sn: Veitch uid: jamesveitch uidnumber: 1000 userpassword: {CRYPT}$6$uby4/8dS$23r8h349f$ggnQE7Z7GUIW3IXe.1z4pUZ4HDQlukEwB6N4z6/p swn6r2Pg40wF6w5wopOP1f46f4MOI7BJ0 Configure Keycloak We need to setup a new Client inside Keycloak for Kubernetes to allow the Kubernetes API Server to authenticate users and read attributes (like whether or not they're members of the cluster-admin group). Login to Keycloak and, in the Development realm, create a new client with Clients \u2192 Create . Create Client Client ID : kubernetes Protocol : openid-connect Root URL : (leave this blank) You're now presented with a fairly lengthy configuration screen. Use the following options as overrides to what should be provided as existing defaults. Access Type : confidential Valid Redirect URIs : http://localhost:8000 http://localhost:18000 Once saved we now have a new client. Tip Navigating to the Credentials tab will show the necessary secret that needs to be given to Kubernetes. Telling applications about our users groups/roles There's a few ways to achieve the outcome we're looking for \u2192 obtain a list of Groups for a selected User from OpenLDAP and then inform an application in our token response of these when authenticating this user. The same way we performed some mapping from OpenLDAP into the Keycloak schema previously we'll now need to perform a similar task from Keycloak to our respective applications. I'll cover both methods of passing data back in the token: Realm-wide change to a default Client Scope Individual client Mapper Realm-wide Scope By default Keycloak creates a set of default scopes for all clients which sit in the Client Scopes menu. If you click on the tab with the same name in the kubernetes client we've created you'll see that theres some sub-tabs for Setup and Evaluate . Setup By default our client has access to the roles scope. Evaluate If you select your user and click Evaluate it will show you the token you'll be returning to Kubernetes. We can hook into this mechanic if we want so that, by default, the roles scope returns a list of Groups our user is part of. User Groups You can check in Keycloak against your user - this will show what we've already mapped through from OpenLDAP. First off we can change the roles scope globally to include some additional information about groups . This gets assigned by default to all clients. Navigate to Client Scopes \u2192 roles (edit) \u2192 Mappers (tab) and add a new one. Name: groups Mapper Type: Group Membership Token Claim Name: groups Add Mapper to Default Scope If you evaluate the client for your user you'll now see the following in the token. ... \"groups\" : [ \"/ldap-user\" , \"/realm-admin\" , \"/ldap-admin\" , \"/cluster-admin\" ] , ... Client Mapper You can associate groups for an individual client by adding the following mapper: Name : groups Mapper Type : Group Membership Client ID : kubernetes Token Claim Name : groups Add to ID token : on Configure Kubectl with kubelogin Kubelogin is a kubectl plugin for Kubernetes OpenID Connect authentication designed to allow you to obtain a token from keycloak (or any OIDC provider) and then pass this to kubectl to use to authenticate as your user. Install latest version export KUBELOGIN_VERSION = v1.15.2 curl -LO https://github.com/int128/kubelogin/releases/download/ ${ KUBELOGIN_VERSION } /kubelogin_linux_amd64.zip 7z x kubelogin_linux_amd64.zip chmod +x kubelogin sudo mv kubelogin /usr/local/bin/kubectl-oidc_login rm kubelogin_linux_amd64.zip LICENSE 1. Set up the OIDC provider We should now be able to run an initial setup command, using the credentials we created in Keycloak for the client. Local Port Forwarding If you're working on a remote headless server (like me) then, in order for the below to work, you'll need to forward the port 8000 from your remote host to localhost so that the oidc webpage that grabs your token can load. ssh -L 8000 :localhost:8000 adminlocal@banks.local 1. Set up the OIDC provider kubectl oidc-login setup \\ --oidc-issuer-url = https://auth.jamesveitch.dev/auth/realms/development \\ --oidc-client-id = kubernetes \\ --oidc-client-secret = YOUR_CLIENT_SECRET On successfully validating access via localhost:8000 the following will be displayed. Have a quick look but don't worry, we'll step through each step individually. Command Output 2. Verify authentication ## 2. Verify authentication Open http://localhost:8000 for authentication You got the following claims in the token: acr = 1 nbf = 0 iss = https://auth.jamesveitch.dev/auth/realms/development nonce = grKgiUYt6w5R6shPmv_kIoKa5eDf6n310VDG5aaFg1s session_state = 4de7b14d-df9c-4edd-b165-54731bb74d76 name = James Veitch preferred_username = jamesveitch groups =[ /ldap-user /realm-admin /ldap-admin /cluster-admin ] family_name = Veitch exp = 1577571593 email_verified = false aud = kubernetes sub = f0db1df3-5a2f-4095-b556-72912f8315cf azp = kubernetes given_name = James iat = 1577571293 typ = ID auth_time = 1577571293 jti = 0dfc35b1-99e3-4f4b-8e4d-89424b1b3e51 3. Bind a role ## 3. Bind a role Run the following command: kubectl apply -f - <<-EOF kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: oidc-cluster-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: User name: https://auth.jamesveitch.dev/auth/realms/development# EOF 4. Set up the Kubernetes API server ## 4. Set up the Kubernetes API server Add the following options to the kube-apiserver: --oidc-issuer-url = https://auth.jamesveitch.dev/auth/realms/development --oidc-client-id = kubernetes 5. Set up the kubeconfig ## 5. Set up the kubeconfig Add the following user to the kubeconfig : users : - name : google user : exec : apiVersion : client.authentication.k8s.io/v1beta1 command : kubectl args : - oidc-login - get-token - --oidc-issuer-url=https://auth.jamesveitch.dev/auth/realms/development - --oidc-client-id=kubernetes - --oidc-client-secret=5ffe6efd-bc31-4009-bec4-0ef2f0b15505 2. Verify authentication If we look at the commands output in this section you can see that it shows the details of the token received from Keycloak. A couple of important bits to note. preferred_username : This is what we will be using to log into kubernetes resources (and matches our username for keycloak) groups : An array of groups our user is a member of. Importantly we've got /cluster-admin in here... we'll have to access this with oidc:/cluster-admin in our RBAC manifests later with the prefix we'll set in the kube-apiserver. 3. Bind a role If you were the impatient type and tried to immediately connect with kubectl you would find that, despite finding your user, you still cen't actually fdo anything. This is because, by default, the RBAC in kubernetes is set to deny all. adminlocal@banks:~$ kubectl get pods Open http://localhost:8000 for authentication Error from server ( Forbidden ) : pods is forbidden: User \"https://auth.jamesveitch.dev/auth/realms/development#jamesveitch\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\" We need to setup a mapping now between our /cluster-admin group from OpenLDAP and our default cluster-admin role in Kubernetes. We could also assign this directly to our User but that feels dirty and inflexible. Left here for reference but commented out. # file: ~/auth/cluster-admin.yaml apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : oidc-cluster-admins roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : # - apiGroup: rbac.authorization.k8s.io # kind: User # name: oidc/jamesveitch - apiGroup : rbac.authorization.k8s.io kind : Group name : oidc/cluster-admin Apply with kubectl apply -f ~/auth/cluster-admin.yaml . 4. Set up the Kubernetes API server We'll now configure the Kubernetes API Server to authenticate against an OIDC provider. In our case that's Keycloak. This is running as it's own pod currently and we initialised it originally with our kubeadm command when configuring kubernetes . $ kubectl -n kube-system get pods NAME READY STATUS RESTARTS AGE calico-kube-controllers-74c9747c46-qlcqc 1 /1 Running 2 9d calico-node-f5t9f 1 /1 Running 2 9d coredns-6955765f44-49w5d 1 /1 Running 2 9d coredns-6955765f44-gvqlw 1 /1 Running 2 9d etcd-banks.local 1 /1 Running 2 9d kube-apiserver-banks.local 1 /1 Running 2 9d kube-controller-manager-banks.local 1 /1 Running 2 9d kube-proxy-lxsr6 1 /1 Running 2 9d kube-scheduler-banks.local 1 /1 Running 2 9d If you look inside this pod with describe you'll see some commands have been passed to the container. We essentially need to edit and add to these so that it knows about our OIDC settings. The configuration file is stored by kubeadm in /etc/kubernetes/manifests/kube-apiserver.yaml . # file: /etc/kubernetes/manifests/kube-apiserver.yaml spec : containers : - command : - kube-apiserver - --advertise-address=192.168.0.104 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --enable-admission-plugins=NodeRestriction - --enable-bootstrap-token-auth=true - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key - --etcd-servers=https://127.0.0.1:2379 ... - --oidc-issuer-url=https://auth.jamesveitch.dev/auth/realms/development - --oidc-client-id=kubernetes - --oidc-username-claim=preferred_username - --oidc-username-prefix=oidc/ - --oidc-groups-claim=groups - --oidc-groups-prefix=oidc NB: You'll notice that the oidc/ and oidc are different for the groups and users above... This is deliberate as the groups come through automatically with the / prefixed to them. In addition I was unable to get the apiserver to load with the default : appended - must be an error with unsanitised string inputs to the commandline. Once modified this should actually be detected and automatically reloaded/applied. Debugging the API Server If something goes wrong above your apiserver will simply fail to load and you'll lose access to the cluster. To debug, run the following command and check the logs. sudo docker logs $( sudo docker ps -a | grep k8s_kube-apiserver | awk '{print $1}' ) Or, even better, undo what you've just done... 5. Set up the kubeconfig Whilst the output from the initial setup command was a little cryptic, it's trying to tell us to create a new user associated with our cluster and then specify how credentials need to be obtained. Edit your ~/.kube/config and ensure we do the following: contexts: add a user to our context for the correct cluster (in our case kubernetes ) called keycloak users: create a user called keycloak with the correct oidc settings. ~/.kube/config # file: ~/.kube/config apiVersion : v1 clusters : - cluster : certificate-authority-data : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWU$ server : https://192.168.0.104:6443 name : kubernetes contexts : - context : cluster : kubernetes user : keycloak name : keycloak-oidc current-context : keycloak-oidc kind : Config preferences : {} users : - name : keycloak user : exec : apiVersion : client.authentication.k8s.io/v1beta1 command : kubectl args : - oidc-login - get-token - --oidc-issuer-url=https://auth.jamesveitch.dev/auth/realms/development - --oidc-client-id=kubernetes - --oidc-client-secret=5ffe6efd-bc31-4009-bec4-0ef2f0b15505 External ~/.kube/config At the moment we've been working exclusively within one of the nodes via ssh. Ideally though we can now copy this config somewhere locally to our actual development machine and leave any hardcoded credentials (like the admin keys) on the server instead. Follow the setup guide to install kubelogin on the client machine and then configure your ~/.kube/config as follows, changing out the necessary sections as appropriate (e.g. the server ip address). # file: ~/.kube/config apiVersion : v1 clusters : - cluster : certificate-authority-data : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWU$ server : https://192.168.0.104:6443 name : kubernetes contexts : - context : cluster : kubernetes user : keycloak name : keycloak-oidc current-context : keycloak-oidc kind : Config preferences : {} users : - name : keycloak user : exec : apiVersion : client.authentication.k8s.io/v1beta1 command : kubectl args : - oidc-login - get-token - --oidc-issuer-url=https://auth.jamesveitch.dev/auth/realms/development - --oidc-client-id=kubernetes - --oidc-client-secret=5ffe6efd-bc31-4009-bec4-0ef2f0b15505 This file can now be provided to anyone* in a secure** fashion. * They'll need to be able to route to the cluster's IP address (more on that later); and ** have a valid login in LDAP for it to be of any use though. References As detailed in Issue #19 the following were helpful resources in my travels to get this working. Kubernetes Day 2 Operations: AuthN/AuthZ with OIDC and a Little Help From Keycloak Openldap Keycloak and docker Deep Dive: Kubernetes Single Sign-On (SSO) with OpenID Connection via G Suite Single Sign-On for Kubernetes: Dashboard Experience Configuring Kubernetes login with Keycloak kubectl with OpenID Connect Protect Kubernetes Dashboard with OpenID Connect Kubernetes Identity Management Part II \u2013 RBAC and User Provisioning","title":"Kubernetes (API)"},{"location":"02.idam/03.integration/01.kubernetes/#configure-keycloak","text":"We need to setup a new Client inside Keycloak for Kubernetes to allow the Kubernetes API Server to authenticate users and read attributes (like whether or not they're members of the cluster-admin group). Login to Keycloak and, in the Development realm, create a new client with Clients \u2192 Create . Create Client Client ID : kubernetes Protocol : openid-connect Root URL : (leave this blank) You're now presented with a fairly lengthy configuration screen. Use the following options as overrides to what should be provided as existing defaults. Access Type : confidential Valid Redirect URIs : http://localhost:8000 http://localhost:18000 Once saved we now have a new client. Tip Navigating to the Credentials tab will show the necessary secret that needs to be given to Kubernetes.","title":"Configure Keycloak"},{"location":"02.idam/03.integration/01.kubernetes/#telling-applications-about-our-users-groupsroles","text":"There's a few ways to achieve the outcome we're looking for \u2192 obtain a list of Groups for a selected User from OpenLDAP and then inform an application in our token response of these when authenticating this user. The same way we performed some mapping from OpenLDAP into the Keycloak schema previously we'll now need to perform a similar task from Keycloak to our respective applications. I'll cover both methods of passing data back in the token: Realm-wide change to a default Client Scope Individual client Mapper Realm-wide Scope By default Keycloak creates a set of default scopes for all clients which sit in the Client Scopes menu. If you click on the tab with the same name in the kubernetes client we've created you'll see that theres some sub-tabs for Setup and Evaluate . Setup By default our client has access to the roles scope. Evaluate If you select your user and click Evaluate it will show you the token you'll be returning to Kubernetes. We can hook into this mechanic if we want so that, by default, the roles scope returns a list of Groups our user is part of. User Groups You can check in Keycloak against your user - this will show what we've already mapped through from OpenLDAP. First off we can change the roles scope globally to include some additional information about groups . This gets assigned by default to all clients. Navigate to Client Scopes \u2192 roles (edit) \u2192 Mappers (tab) and add a new one. Name: groups Mapper Type: Group Membership Token Claim Name: groups Add Mapper to Default Scope If you evaluate the client for your user you'll now see the following in the token. ... \"groups\" : [ \"/ldap-user\" , \"/realm-admin\" , \"/ldap-admin\" , \"/cluster-admin\" ] , ... Client Mapper You can associate groups for an individual client by adding the following mapper: Name : groups Mapper Type : Group Membership Client ID : kubernetes Token Claim Name : groups Add to ID token : on","title":"Telling applications about our users groups/roles"},{"location":"02.idam/03.integration/01.kubernetes/#configure-kubectl-with-kubelogin","text":"Kubelogin is a kubectl plugin for Kubernetes OpenID Connect authentication designed to allow you to obtain a token from keycloak (or any OIDC provider) and then pass this to kubectl to use to authenticate as your user.","title":"Configure Kubectl with kubelogin"},{"location":"02.idam/03.integration/01.kubernetes/#install-latest-version","text":"export KUBELOGIN_VERSION = v1.15.2 curl -LO https://github.com/int128/kubelogin/releases/download/ ${ KUBELOGIN_VERSION } /kubelogin_linux_amd64.zip 7z x kubelogin_linux_amd64.zip chmod +x kubelogin sudo mv kubelogin /usr/local/bin/kubectl-oidc_login rm kubelogin_linux_amd64.zip LICENSE","title":"Install latest version"},{"location":"02.idam/03.integration/01.kubernetes/#1-set-up-the-oidc-provider","text":"We should now be able to run an initial setup command, using the credentials we created in Keycloak for the client. Local Port Forwarding If you're working on a remote headless server (like me) then, in order for the below to work, you'll need to forward the port 8000 from your remote host to localhost so that the oidc webpage that grabs your token can load. ssh -L 8000 :localhost:8000 adminlocal@banks.local 1. Set up the OIDC provider kubectl oidc-login setup \\ --oidc-issuer-url = https://auth.jamesveitch.dev/auth/realms/development \\ --oidc-client-id = kubernetes \\ --oidc-client-secret = YOUR_CLIENT_SECRET On successfully validating access via localhost:8000 the following will be displayed. Have a quick look but don't worry, we'll step through each step individually. Command Output 2. Verify authentication ## 2. Verify authentication Open http://localhost:8000 for authentication You got the following claims in the token: acr = 1 nbf = 0 iss = https://auth.jamesveitch.dev/auth/realms/development nonce = grKgiUYt6w5R6shPmv_kIoKa5eDf6n310VDG5aaFg1s session_state = 4de7b14d-df9c-4edd-b165-54731bb74d76 name = James Veitch preferred_username = jamesveitch groups =[ /ldap-user /realm-admin /ldap-admin /cluster-admin ] family_name = Veitch exp = 1577571593 email_verified = false aud = kubernetes sub = f0db1df3-5a2f-4095-b556-72912f8315cf azp = kubernetes given_name = James iat = 1577571293 typ = ID auth_time = 1577571293 jti = 0dfc35b1-99e3-4f4b-8e4d-89424b1b3e51 3. Bind a role ## 3. Bind a role Run the following command: kubectl apply -f - <<-EOF kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: oidc-cluster-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: User name: https://auth.jamesveitch.dev/auth/realms/development# EOF 4. Set up the Kubernetes API server ## 4. Set up the Kubernetes API server Add the following options to the kube-apiserver: --oidc-issuer-url = https://auth.jamesveitch.dev/auth/realms/development --oidc-client-id = kubernetes 5. Set up the kubeconfig ## 5. Set up the kubeconfig Add the following user to the kubeconfig : users : - name : google user : exec : apiVersion : client.authentication.k8s.io/v1beta1 command : kubectl args : - oidc-login - get-token - --oidc-issuer-url=https://auth.jamesveitch.dev/auth/realms/development - --oidc-client-id=kubernetes - --oidc-client-secret=5ffe6efd-bc31-4009-bec4-0ef2f0b15505","title":"1. Set up the OIDC provider"},{"location":"02.idam/03.integration/01.kubernetes/#2-verify-authentication","text":"If we look at the commands output in this section you can see that it shows the details of the token received from Keycloak. A couple of important bits to note. preferred_username : This is what we will be using to log into kubernetes resources (and matches our username for keycloak) groups : An array of groups our user is a member of. Importantly we've got /cluster-admin in here... we'll have to access this with oidc:/cluster-admin in our RBAC manifests later with the prefix we'll set in the kube-apiserver.","title":"2. Verify authentication"},{"location":"02.idam/03.integration/01.kubernetes/#3-bind-a-role","text":"If you were the impatient type and tried to immediately connect with kubectl you would find that, despite finding your user, you still cen't actually fdo anything. This is because, by default, the RBAC in kubernetes is set to deny all. adminlocal@banks:~$ kubectl get pods Open http://localhost:8000 for authentication Error from server ( Forbidden ) : pods is forbidden: User \"https://auth.jamesveitch.dev/auth/realms/development#jamesveitch\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\" We need to setup a mapping now between our /cluster-admin group from OpenLDAP and our default cluster-admin role in Kubernetes. We could also assign this directly to our User but that feels dirty and inflexible. Left here for reference but commented out. # file: ~/auth/cluster-admin.yaml apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : oidc-cluster-admins roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : # - apiGroup: rbac.authorization.k8s.io # kind: User # name: oidc/jamesveitch - apiGroup : rbac.authorization.k8s.io kind : Group name : oidc/cluster-admin Apply with kubectl apply -f ~/auth/cluster-admin.yaml .","title":"3. Bind a role"},{"location":"02.idam/03.integration/01.kubernetes/#4-set-up-the-kubernetes-api-server","text":"We'll now configure the Kubernetes API Server to authenticate against an OIDC provider. In our case that's Keycloak. This is running as it's own pod currently and we initialised it originally with our kubeadm command when configuring kubernetes . $ kubectl -n kube-system get pods NAME READY STATUS RESTARTS AGE calico-kube-controllers-74c9747c46-qlcqc 1 /1 Running 2 9d calico-node-f5t9f 1 /1 Running 2 9d coredns-6955765f44-49w5d 1 /1 Running 2 9d coredns-6955765f44-gvqlw 1 /1 Running 2 9d etcd-banks.local 1 /1 Running 2 9d kube-apiserver-banks.local 1 /1 Running 2 9d kube-controller-manager-banks.local 1 /1 Running 2 9d kube-proxy-lxsr6 1 /1 Running 2 9d kube-scheduler-banks.local 1 /1 Running 2 9d If you look inside this pod with describe you'll see some commands have been passed to the container. We essentially need to edit and add to these so that it knows about our OIDC settings. The configuration file is stored by kubeadm in /etc/kubernetes/manifests/kube-apiserver.yaml . # file: /etc/kubernetes/manifests/kube-apiserver.yaml spec : containers : - command : - kube-apiserver - --advertise-address=192.168.0.104 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --enable-admission-plugins=NodeRestriction - --enable-bootstrap-token-auth=true - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key - --etcd-servers=https://127.0.0.1:2379 ... - --oidc-issuer-url=https://auth.jamesveitch.dev/auth/realms/development - --oidc-client-id=kubernetes - --oidc-username-claim=preferred_username - --oidc-username-prefix=oidc/ - --oidc-groups-claim=groups - --oidc-groups-prefix=oidc NB: You'll notice that the oidc/ and oidc are different for the groups and users above... This is deliberate as the groups come through automatically with the / prefixed to them. In addition I was unable to get the apiserver to load with the default : appended - must be an error with unsanitised string inputs to the commandline. Once modified this should actually be detected and automatically reloaded/applied. Debugging the API Server If something goes wrong above your apiserver will simply fail to load and you'll lose access to the cluster. To debug, run the following command and check the logs. sudo docker logs $( sudo docker ps -a | grep k8s_kube-apiserver | awk '{print $1}' ) Or, even better, undo what you've just done...","title":"4. Set up the Kubernetes API server"},{"location":"02.idam/03.integration/01.kubernetes/#5-set-up-the-kubeconfig","text":"Whilst the output from the initial setup command was a little cryptic, it's trying to tell us to create a new user associated with our cluster and then specify how credentials need to be obtained. Edit your ~/.kube/config and ensure we do the following: contexts: add a user to our context for the correct cluster (in our case kubernetes ) called keycloak users: create a user called keycloak with the correct oidc settings. ~/.kube/config # file: ~/.kube/config apiVersion : v1 clusters : - cluster : certificate-authority-data : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWU$ server : https://192.168.0.104:6443 name : kubernetes contexts : - context : cluster : kubernetes user : keycloak name : keycloak-oidc current-context : keycloak-oidc kind : Config preferences : {} users : - name : keycloak user : exec : apiVersion : client.authentication.k8s.io/v1beta1 command : kubectl args : - oidc-login - get-token - --oidc-issuer-url=https://auth.jamesveitch.dev/auth/realms/development - --oidc-client-id=kubernetes - --oidc-client-secret=5ffe6efd-bc31-4009-bec4-0ef2f0b15505","title":"5. Set up the kubeconfig"},{"location":"02.idam/03.integration/01.kubernetes/#external-kubeconfig","text":"At the moment we've been working exclusively within one of the nodes via ssh. Ideally though we can now copy this config somewhere locally to our actual development machine and leave any hardcoded credentials (like the admin keys) on the server instead. Follow the setup guide to install kubelogin on the client machine and then configure your ~/.kube/config as follows, changing out the necessary sections as appropriate (e.g. the server ip address). # file: ~/.kube/config apiVersion : v1 clusters : - cluster : certificate-authority-data : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWU$ server : https://192.168.0.104:6443 name : kubernetes contexts : - context : cluster : kubernetes user : keycloak name : keycloak-oidc current-context : keycloak-oidc kind : Config preferences : {} users : - name : keycloak user : exec : apiVersion : client.authentication.k8s.io/v1beta1 command : kubectl args : - oidc-login - get-token - --oidc-issuer-url=https://auth.jamesveitch.dev/auth/realms/development - --oidc-client-id=kubernetes - --oidc-client-secret=5ffe6efd-bc31-4009-bec4-0ef2f0b15505 This file can now be provided to anyone* in a secure** fashion. * They'll need to be able to route to the cluster's IP address (more on that later); and ** have a valid login in LDAP for it to be of any use though.","title":"External ~/.kube/config"},{"location":"02.idam/03.integration/01.kubernetes/#references","text":"As detailed in Issue #19 the following were helpful resources in my travels to get this working. Kubernetes Day 2 Operations: AuthN/AuthZ with OIDC and a Little Help From Keycloak Openldap Keycloak and docker Deep Dive: Kubernetes Single Sign-On (SSO) with OpenID Connection via G Suite Single Sign-On for Kubernetes: Dashboard Experience Configuring Kubernetes login with Keycloak kubectl with OpenID Connect Protect Kubernetes Dashboard with OpenID Connect Kubernetes Identity Management Part II \u2013 RBAC and User Provisioning","title":"References"},{"location":"02.idam/03.integration/02.kubernetes.dashboard/","text":"Having integrated the Kubernetes API with Keycloak in our previous exercise we'll now implement some integrated security around the Kubernetes dashboard which we installed and saw earlier on . As we noticed previously we could implement an authentication/authorisation step with the dashboard via a login page which allowed users to upload a kubeconfig file or enter a bearer token. Alternatively, the dashboard supports the use of authorisation headers to supply bearer tokens (Authorization: Bearer ) . If we can intercept the request to the dasboahrd and inject this header based on our OIDC id-token integration we could bypass this login screen and implement some Single Sign On (SSO). Checklist Pre-requisites You'll need to have the following in place before proceeding: Kubernetes cluster with RBAC OpenLDAP+Keycloak installed A valid realm development A user jamesveitch memberOf the cluster-admin group Kubernetes API setup to accept tokens issued by Keycloak A Kubernetes ClusterRoleBinding for the cluster-admin role mapped against the cluster-admin LDAP group We'll configure A Keycloak OIDC Client for the Kubernetes Dashboard Keycloak Gatekeeper (previously called keycloak-proxy ) server to intercept requests Setup OIDC Client Following similar steps to previously used we need to create a new Client . Client ID : kubernetes-dashboard Protocol : openid-connect Root URL : (leave this blank) You're now presented with a fairly lengthy configuration screen. Use the following options as overrides to what should be provided as existing defaults. Access Type : confidential Valid Redirect URIs : https://dashboard.jamesveitch.dev/oauth/callback (this is what we define in the Ingress ) Once saved we now have a new client. Mapping of Groups Depending on how you setup the mapping of our Groups previously you either need to create a specific client mapper again or (if you used the Scope it will just inherit). Setup keycloak-gatekeeper We'll now need to modify our previous dashboard setup process to incorporate the proxy server. Previously we applied the following manifests: name description recommended creates service accounts, configmaps, roles and the dashboard deployment certificate requests a trusted TLS certificate, via cert-manager from LetsEncrypt ingress configured an nginx ingress, using the certificate to direct inbound traffic on dashboard.jamesveitch.dev to the dashboard service configured by recommended above In order to effectively still maintain all of the existing goodness, plus then add in some security with keycloak-gatekeeper , we need to create a new Deployment and associated Service to intercept the requests and point the existing Ingress at this Service instead. Essentially introducing an additional hop. The flow of traffic will now be: Client \u2192 Nginx (Ingress) \u2192 Keycloak Gatekeeper (Service) \u2192 Kubernetes Dashboard (Service) \u2192 Kubernetes Dashboard (Pod) \u2192 Kubernetes Dashboard (Container) Really the client will hit the DNS first and then a LoadBalancer but we'll skip that in the above for the sake of readability. Further Reading There's some detailed documentation for configuring keycloak-gatekeeper in case the below isn't to your liking in terms of settings. Firstly let's create an encryption key for the proxy to use. In order to remain stateless and not have to rely on a central cache to persist the refresh_tokens, the refresh token is encrypted and added as a cookie using crypto/aes. The key must be the same if you are running behind a load balancer. The key length should be either 16 or 32 bytes, depending or whether you want AES-128 or AES-256. # Generate 32bits of randomness, SHA256 hash it and then base64 encode before taking the # first 32 characters dd if = /dev/urandom bs = 1 count = 32 2 >/dev/null | sha256sum | base64 | head -c 32 ; echo Add a mapper for client_id As per 2.4.29. Known Issues (I always love these...) There is a known issue with the Keycloak server 4.7.0.Final in which Gatekeeper is unable to find the client_id in the aud claim. This is due to the fact the client_id is not in the audience anymore. The workaround is to add the \"Audience\" protocol mapper to the client with the audience pointed to the client_id. For more information, see KEYCLOAK-8954 . So we need to add a mapper specifically into our client in Keycloak to ensure the client_id is passed back. Otherwise you'll get a blank screen on redirecting (post-authentication) and your logs will show the below. 1 .5776252005434518e+09 error no session found in request, redirecting for authorization { \"error\" : \"authentication session not found\" } 1 .5776252006586719e+09 error unable to verify the id token { \"error\" : \"oidc: JWT claims invalid: invalid claims, cannot find 'client_id' in 'aud' claim, aud=[realm-management account], client_id=kubernetes-dashboard\" } proxy-buffer-size You need to increase the nginx-ingress proxy buffer size. Note the use of the annotation on the Ingress. nginx.ingress.kubernetes.io/proxy-buffer-size : \"64k\"","title":"Kubernetes (Dashboard)"},{"location":"02.idam/03.integration/02.kubernetes.dashboard/#setup-oidc-client","text":"Following similar steps to previously used we need to create a new Client . Client ID : kubernetes-dashboard Protocol : openid-connect Root URL : (leave this blank) You're now presented with a fairly lengthy configuration screen. Use the following options as overrides to what should be provided as existing defaults. Access Type : confidential Valid Redirect URIs : https://dashboard.jamesveitch.dev/oauth/callback (this is what we define in the Ingress ) Once saved we now have a new client. Mapping of Groups Depending on how you setup the mapping of our Groups previously you either need to create a specific client mapper again or (if you used the Scope it will just inherit).","title":"Setup OIDC Client"},{"location":"02.idam/03.integration/02.kubernetes.dashboard/#setup-keycloak-gatekeeper","text":"We'll now need to modify our previous dashboard setup process to incorporate the proxy server. Previously we applied the following manifests: name description recommended creates service accounts, configmaps, roles and the dashboard deployment certificate requests a trusted TLS certificate, via cert-manager from LetsEncrypt ingress configured an nginx ingress, using the certificate to direct inbound traffic on dashboard.jamesveitch.dev to the dashboard service configured by recommended above In order to effectively still maintain all of the existing goodness, plus then add in some security with keycloak-gatekeeper , we need to create a new Deployment and associated Service to intercept the requests and point the existing Ingress at this Service instead. Essentially introducing an additional hop. The flow of traffic will now be: Client \u2192 Nginx (Ingress) \u2192 Keycloak Gatekeeper (Service) \u2192 Kubernetes Dashboard (Service) \u2192 Kubernetes Dashboard (Pod) \u2192 Kubernetes Dashboard (Container) Really the client will hit the DNS first and then a LoadBalancer but we'll skip that in the above for the sake of readability. Further Reading There's some detailed documentation for configuring keycloak-gatekeeper in case the below isn't to your liking in terms of settings. Firstly let's create an encryption key for the proxy to use. In order to remain stateless and not have to rely on a central cache to persist the refresh_tokens, the refresh token is encrypted and added as a cookie using crypto/aes. The key must be the same if you are running behind a load balancer. The key length should be either 16 or 32 bytes, depending or whether you want AES-128 or AES-256. # Generate 32bits of randomness, SHA256 hash it and then base64 encode before taking the # first 32 characters dd if = /dev/urandom bs = 1 count = 32 2 >/dev/null | sha256sum | base64 | head -c 32 ; echo Add a mapper for client_id As per 2.4.29. Known Issues (I always love these...) There is a known issue with the Keycloak server 4.7.0.Final in which Gatekeeper is unable to find the client_id in the aud claim. This is due to the fact the client_id is not in the audience anymore. The workaround is to add the \"Audience\" protocol mapper to the client with the audience pointed to the client_id. For more information, see KEYCLOAK-8954 . So we need to add a mapper specifically into our client in Keycloak to ensure the client_id is passed back. Otherwise you'll get a blank screen on redirecting (post-authentication) and your logs will show the below. 1 .5776252005434518e+09 error no session found in request, redirecting for authorization { \"error\" : \"authentication session not found\" } 1 .5776252006586719e+09 error unable to verify the id token { \"error\" : \"oidc: JWT claims invalid: invalid claims, cannot find 'client_id' in 'aud' claim, aud=[realm-management account], client_id=kubernetes-dashboard\" } proxy-buffer-size You need to increase the nginx-ingress proxy buffer size. Note the use of the annotation on the Ingress. nginx.ingress.kubernetes.io/proxy-buffer-size : \"64k\"","title":"Setup keycloak-gatekeeper"},{"location":"98.wip/00.external.dns/","text":"We're going to install the ExternalDNS addon for Kubernetes in order to make our services discoverable externally via public DNS. ... it retrieves a list of resources (Services, Ingresses, etc.) from the Kubernetes API to determine a desired list of DNS records. So it uses annotations inside Kubernetes in order to identify the services you;d like to expose. ... allows you to control DNS records dynamically via Kubernetes resources in a DNS provider-agnostic way. And then, using these annotations, will configure an external DNS provider of your choice to populate the necessary records to expose your services. At time of writing the current version is v0.5 . This provides Beta support for Cloudflare . The Setting up ExternalDNS for Services on Cloudflare tutorial will be used as a starting point. Because though we're using an ingress we will need to adapt it slightly to use an ingress as opposed to service . Cloudflare Login to Cloudflare and generate an API Token. As per the docs: When using API Token authentication the token should be granted Zone Read and DNS Edit privileges API Token will be preferred for authentication if CF_API_TOKEN environment variable is set. Otherwise CF_API_KEY and CF_API_EMAIL should be set to run ExternalDNS with Cloudflare. Make note of this token as we'll need it below. In addition there's an open issue #1127 which highlights. for now, the API token must be granted for all zone, cannot only to a specific zone. We're going to use Kubernetes secrets in order to store our api credentials from Cloudflare. kubectl create secret generic cloudflare-api-token --from-literal = CF_API_TOKEN = Aq6g43899Am0567SURF2diL2DSZBGEYIAUHdmluaB This should now be encoded and stored. To check, run a describe. $ kubectl get secret cloudflare-api-token -o yaml apiVersion: v1 data: CF_API_TOKEN: QXE2Z21VRUFtMFN3dkhTVVJGWGRpTDJEU1pCR0VZSUFVSGRtbHVhQg == kind: Secret metadata: creationTimestamp: \"2019-12-02T09:00:15Z\" name: cloudflare-api-token namespace: default resourceVersion: \"2983185\" selfLink: /api/v1/namespaces/default/secrets/cloudflare-api-token uid: 2917b431-0bd9-4fcc-8c2b-a401c8a3a3c3 type: Opaque # file: ~/external-dns/external-dns.yaml apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest args : # - --source=service # ingress is also possible - --source=ingress - --domain-filter=jamesveitch.dev # (optional) limit to only jamesveitch.dev domains; change to match the zone created above. - --provider=cloudflare - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...) # - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --log-level=debug - --interval=5s env : # - name: CF_API_KEY # value: \"YOUR_CLOUDFLARE_API_KEY\" # - name: CF_API_EMAIL # value: \"YOUR_CLOUDFLARE_EMAIL\" - name : CF_API_TOKEN valueFrom : secretKeyRef : name : cloudflare-api-token key : CF_API_TOKEN # now apply the manifest kubectl create -f ~/external-dns/external-dns.yaml As we've set this up to monitor --source=service (as opposed to ingress ) we now need to modify the service manifest for applications we'd like to expose. Let's modify the whoami spec and add the necessary annotations. For those of you who have used Traefik with Docker or DockerCompose in the past this is a very similar concept. apiVersion : v1 kind : Service metadata : name : whoami annotations : external-dns.alpha.kubernetes.io/hostname : whoami.jamesveitch.dev external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 kubectl logs -l 'app=external-dns'","title":"00.external.dns"},{"location":"98.wip/00.external.dns/#cloudflare","text":"Login to Cloudflare and generate an API Token. As per the docs: When using API Token authentication the token should be granted Zone Read and DNS Edit privileges API Token will be preferred for authentication if CF_API_TOKEN environment variable is set. Otherwise CF_API_KEY and CF_API_EMAIL should be set to run ExternalDNS with Cloudflare. Make note of this token as we'll need it below. In addition there's an open issue #1127 which highlights. for now, the API token must be granted for all zone, cannot only to a specific zone. We're going to use Kubernetes secrets in order to store our api credentials from Cloudflare. kubectl create secret generic cloudflare-api-token --from-literal = CF_API_TOKEN = Aq6g43899Am0567SURF2diL2DSZBGEYIAUHdmluaB This should now be encoded and stored. To check, run a describe. $ kubectl get secret cloudflare-api-token -o yaml apiVersion: v1 data: CF_API_TOKEN: QXE2Z21VRUFtMFN3dkhTVVJGWGRpTDJEU1pCR0VZSUFVSGRtbHVhQg == kind: Secret metadata: creationTimestamp: \"2019-12-02T09:00:15Z\" name: cloudflare-api-token namespace: default resourceVersion: \"2983185\" selfLink: /api/v1/namespaces/default/secrets/cloudflare-api-token uid: 2917b431-0bd9-4fcc-8c2b-a401c8a3a3c3 type: Opaque # file: ~/external-dns/external-dns.yaml apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest args : # - --source=service # ingress is also possible - --source=ingress - --domain-filter=jamesveitch.dev # (optional) limit to only jamesveitch.dev domains; change to match the zone created above. - --provider=cloudflare - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...) # - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --log-level=debug - --interval=5s env : # - name: CF_API_KEY # value: \"YOUR_CLOUDFLARE_API_KEY\" # - name: CF_API_EMAIL # value: \"YOUR_CLOUDFLARE_EMAIL\" - name : CF_API_TOKEN valueFrom : secretKeyRef : name : cloudflare-api-token key : CF_API_TOKEN # now apply the manifest kubectl create -f ~/external-dns/external-dns.yaml As we've set this up to monitor --source=service (as opposed to ingress ) we now need to modify the service manifest for applications we'd like to expose. Let's modify the whoami spec and add the necessary annotations. For those of you who have used Traefik with Docker or DockerCompose in the past this is a very similar concept. apiVersion : v1 kind : Service metadata : name : whoami annotations : external-dns.alpha.kubernetes.io/hostname : whoami.jamesveitch.dev external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 kubectl logs -l 'app=external-dns'","title":"Cloudflare"},{"location":"98.wip/03.ingress.and.service.mesh/","text":"As described in the official kubernetes docs an Ingress object manages external access to services in a cluster, typically HTTP. They can provide load balancing, SSL termination and name-based virtual hosting. We're going to combine a couple of sections of the official docs to deploy the following: Istio ingress with LetsEncrypt via cert-manager ( docs ); Mutual TLS between pods ( docs ) Istio's CNI, so that privileged injection for sidecars no longer needed, ( docs ) Install mkdir -p ~/istio ; \\ cd ~/istio curl -L https://git.io/getLatestIstio | sh - export ISTIO_DIR = $( ls -d istio-* ) # enable autocompletion and add to path tee >> ~/.bash_profile <<EOL # add istio to path export PATH=~/istio/$ISTIO_DIR/bin:\\$PATH # add istioctl autocompletion test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash EOL source ~/.bash_profile Now we'll install the new Istio CNI plugin as per the docs with a couple of additional tweaks as outlined for ingress . Istio comes with a number of Installation Configuration Profiles that we want to customise slightly before deploying. First generate a manifest with the settings we want. We're deploying the default profile and adding in the necessary additional components for sds and cni . istioctl manifest generate \\ --set values.gateways.istio-ingressgateway.sds.enabled = true \\ --set values.global.k8sIngress.enabled = true \\ --set values.global.k8sIngress.enableHttps = true \\ --set values.global.k8sIngress.gatewayName = ingressgateway \\ # --set cni.enabled=true \\ > ~/istio/generated-manifest.yaml Now apply istioctl manifest apply -f ~/istio/generated-manifest.yaml the required ClusterRole:istio-cni is not ready Currently using the --set cni-enabled=true flag doesn't seem to work. When you run an istioctl verify-installation -f ~/istio/generated-manifest.yaml it will present you with an error of the required ClusterRole:istio-cni is not ready . To be investigated further... Reviewing the list of services on the cluster we should now see an ingress-gateway created with an EXTERNAL-IP allocated by MetalLB in the istio-system namespace. Running Services $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE default kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 12h istio-system istio-citadel ClusterIP 10 .96.218.252 <none> 8060 /TCP,15014/TCP 82s istio-system istio-galley ClusterIP 10 .96.101.76 <none> 443 /TCP,15014/TCP,9901/TCP,15019/TCP 82s istio-system istio-ingressgateway LoadBalancer 10 .96.113.231 192 .168.0.200 15020 :30088/TCP,80:32168/TCP,443:31899/TCP,.... 80s istio-system istio-pilot ClusterIP 10 .96.232.65 <none> 15010 /TCP,15011/TCP,8080/TCP,15014/TCP 81s istio-system istio-policy ClusterIP 10 .96.42.154 <none> 9091 /TCP,15004/TCP,15014/TCP 80s istio-system istio-sidecar-injector ClusterIP 10 .96.147.243 <none> 443 /TCP 82s istio-system istio-telemetry ClusterIP 10 .96.8.149 <none> 9091 /TCP,15004/TCP,15014/TCP,42422/TCP 80s istio-system prometheus ClusterIP 10 .96.144.9 <none> 9090 /TCP 82s kube-system kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP,9153/TCP 12h We'll now create a deployment and see how many pods are created. # file: ~/istio/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 Apply the manifest with kubectl apply -f ~/istio/whoami.yaml and check the deployment and the number of pods created as a result. $ kubectl get deployment -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR whoami-deployment 1 /1 1 1 13s whoami containous/whoami app = whoami $ kubectl get pod -l app = whoami NAME READY STATUS RESTARTS AGE whoami-deployment-5b4bb9c787-9cwg8 1 /1 Running 0 10s There's 1/1 ready for the pods which means we only have the single container running and sidecar injection is not yet configured. We'll enable it for the default namespace and then delete the pod (forcing it to recreate with a sidecar). $ kubectl label namespace default istio-injection = enabled $ kubectl delete pod -l app = whoami $ kubectl get pod -l app = whoami NAME READY STATUS RESTARTS AGE whoami-deployment-5b4bb9c787-snm9h 2 /2 Running 0 8s The 2/2 shows that we now have an injected sidecar container. We can inspect some details to see more with kubectl describe pod -l app=whoami . You should see the injected istio-proxy container and corresponding volumes. Sidecar Proxy kubectl describe pod -l app = whoami ... Containers : whoami : Container ID : docker://5bbf03517d378e3e615a32396239f9704092f22e81fedcd00907d38891baaf07 Image : containous/whoami Image ID : docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b Port : 80/TCP Host Port : 0/TCP State : Running Started : Fri, 13 Dec 2019 06:47:57 +0000 Ready : True Restart Count : 0 Environment : <none> Mounts : /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro) istio-proxy : Container ID : docker://3e93f0370eecf8a5686e90013e3681c44f3b152891561bde8c56c3defd240d4b Image : docker.io/istio/proxyv2:1.4.2 Image ID : docker-pullable://istio/proxyv2@sha256:c98b4d277b724a2ad0c6bf22008bd02ddeb2525f19e35cdefe8a3181313716e7 Port : 15090/TCP Host Port : 0/TCP ... ... Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 90s default-scheduler Successfully assigned default/whoami-deployment-5b4bb9c787-snm9h to banks.local Normal Pulled 89s kubelet, banks.local Container image \"docker.io/istio/proxyv2:1.4.2\" already present on machine Normal Created 89s kubelet, banks.local Created container istio-init Normal Started 88s kubelet, banks.local Started container istio-init Normal Pulling 87s kubelet, banks.local Pulling image \"containous/whoami\" Normal Pulled 85s kubelet, banks.local Successfully pulled image \"containous/whoami\" Normal Created 85s kubelet, banks.local Created container whoami Normal Started 85s kubelet, banks.local Started container whoami Normal Pulled 85s kubelet, banks.local Container image \"docker.io/istio/proxyv2:1.4.2\" already present on machine Normal Created 84s kubelet, banks.local Created container istio-proxy Normal Started 84s kubelet, banks.local Started container istio-proxy We can now create a Service and Ingress for the deployment so that we can access it externally. # file: ~/istio/whoami-svc.yaml apiVersion : v1 kind : Service metadata : name : whoami labels : app : whoami spec : ports : - protocol : TCP port : 80 name : http selector : app : whoami create it with kubectl apply -f ~/istio/whoami-svc.yaml # file: ~/istio/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : labels : app : whoami annotations : kubernetes.io/ingress.class : istio # cert-manager.io/issuer: \"letsencrypt\" # external-dns.alpha.kubernetes.io/hostname: whoami.jamesveitch.dev name : whoami-ingress spec : rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 TLS via Cert-Manager Setup a secret to hold our cloudflare api key (dns validation) kubectl create secret generic cloudflare-apikey-secret --from-literal = apikey = MYKEYFROMCLOUDA","title":"03.ingress.and.service.mesh"},{"location":"98.wip/03.ingress.and.service.mesh/#install","text":"mkdir -p ~/istio ; \\ cd ~/istio curl -L https://git.io/getLatestIstio | sh - export ISTIO_DIR = $( ls -d istio-* ) # enable autocompletion and add to path tee >> ~/.bash_profile <<EOL # add istio to path export PATH=~/istio/$ISTIO_DIR/bin:\\$PATH # add istioctl autocompletion test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash EOL source ~/.bash_profile Now we'll install the new Istio CNI plugin as per the docs with a couple of additional tweaks as outlined for ingress . Istio comes with a number of Installation Configuration Profiles that we want to customise slightly before deploying. First generate a manifest with the settings we want. We're deploying the default profile and adding in the necessary additional components for sds and cni . istioctl manifest generate \\ --set values.gateways.istio-ingressgateway.sds.enabled = true \\ --set values.global.k8sIngress.enabled = true \\ --set values.global.k8sIngress.enableHttps = true \\ --set values.global.k8sIngress.gatewayName = ingressgateway \\ # --set cni.enabled=true \\ > ~/istio/generated-manifest.yaml Now apply istioctl manifest apply -f ~/istio/generated-manifest.yaml the required ClusterRole:istio-cni is not ready Currently using the --set cni-enabled=true flag doesn't seem to work. When you run an istioctl verify-installation -f ~/istio/generated-manifest.yaml it will present you with an error of the required ClusterRole:istio-cni is not ready . To be investigated further... Reviewing the list of services on the cluster we should now see an ingress-gateway created with an EXTERNAL-IP allocated by MetalLB in the istio-system namespace. Running Services $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE default kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 12h istio-system istio-citadel ClusterIP 10 .96.218.252 <none> 8060 /TCP,15014/TCP 82s istio-system istio-galley ClusterIP 10 .96.101.76 <none> 443 /TCP,15014/TCP,9901/TCP,15019/TCP 82s istio-system istio-ingressgateway LoadBalancer 10 .96.113.231 192 .168.0.200 15020 :30088/TCP,80:32168/TCP,443:31899/TCP,.... 80s istio-system istio-pilot ClusterIP 10 .96.232.65 <none> 15010 /TCP,15011/TCP,8080/TCP,15014/TCP 81s istio-system istio-policy ClusterIP 10 .96.42.154 <none> 9091 /TCP,15004/TCP,15014/TCP 80s istio-system istio-sidecar-injector ClusterIP 10 .96.147.243 <none> 443 /TCP 82s istio-system istio-telemetry ClusterIP 10 .96.8.149 <none> 9091 /TCP,15004/TCP,15014/TCP,42422/TCP 80s istio-system prometheus ClusterIP 10 .96.144.9 <none> 9090 /TCP 82s kube-system kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP,9153/TCP 12h We'll now create a deployment and see how many pods are created. # file: ~/istio/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 Apply the manifest with kubectl apply -f ~/istio/whoami.yaml and check the deployment and the number of pods created as a result. $ kubectl get deployment -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR whoami-deployment 1 /1 1 1 13s whoami containous/whoami app = whoami $ kubectl get pod -l app = whoami NAME READY STATUS RESTARTS AGE whoami-deployment-5b4bb9c787-9cwg8 1 /1 Running 0 10s There's 1/1 ready for the pods which means we only have the single container running and sidecar injection is not yet configured. We'll enable it for the default namespace and then delete the pod (forcing it to recreate with a sidecar). $ kubectl label namespace default istio-injection = enabled $ kubectl delete pod -l app = whoami $ kubectl get pod -l app = whoami NAME READY STATUS RESTARTS AGE whoami-deployment-5b4bb9c787-snm9h 2 /2 Running 0 8s The 2/2 shows that we now have an injected sidecar container. We can inspect some details to see more with kubectl describe pod -l app=whoami . You should see the injected istio-proxy container and corresponding volumes. Sidecar Proxy kubectl describe pod -l app = whoami ... Containers : whoami : Container ID : docker://5bbf03517d378e3e615a32396239f9704092f22e81fedcd00907d38891baaf07 Image : containous/whoami Image ID : docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b Port : 80/TCP Host Port : 0/TCP State : Running Started : Fri, 13 Dec 2019 06:47:57 +0000 Ready : True Restart Count : 0 Environment : <none> Mounts : /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro) istio-proxy : Container ID : docker://3e93f0370eecf8a5686e90013e3681c44f3b152891561bde8c56c3defd240d4b Image : docker.io/istio/proxyv2:1.4.2 Image ID : docker-pullable://istio/proxyv2@sha256:c98b4d277b724a2ad0c6bf22008bd02ddeb2525f19e35cdefe8a3181313716e7 Port : 15090/TCP Host Port : 0/TCP ... ... Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 90s default-scheduler Successfully assigned default/whoami-deployment-5b4bb9c787-snm9h to banks.local Normal Pulled 89s kubelet, banks.local Container image \"docker.io/istio/proxyv2:1.4.2\" already present on machine Normal Created 89s kubelet, banks.local Created container istio-init Normal Started 88s kubelet, banks.local Started container istio-init Normal Pulling 87s kubelet, banks.local Pulling image \"containous/whoami\" Normal Pulled 85s kubelet, banks.local Successfully pulled image \"containous/whoami\" Normal Created 85s kubelet, banks.local Created container whoami Normal Started 85s kubelet, banks.local Started container whoami Normal Pulled 85s kubelet, banks.local Container image \"docker.io/istio/proxyv2:1.4.2\" already present on machine Normal Created 84s kubelet, banks.local Created container istio-proxy Normal Started 84s kubelet, banks.local Started container istio-proxy We can now create a Service and Ingress for the deployment so that we can access it externally. # file: ~/istio/whoami-svc.yaml apiVersion : v1 kind : Service metadata : name : whoami labels : app : whoami spec : ports : - protocol : TCP port : 80 name : http selector : app : whoami create it with kubectl apply -f ~/istio/whoami-svc.yaml # file: ~/istio/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : labels : app : whoami annotations : kubernetes.io/ingress.class : istio # cert-manager.io/issuer: \"letsencrypt\" # external-dns.alpha.kubernetes.io/hostname: whoami.jamesveitch.dev name : whoami-ingress spec : rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80","title":"Install"},{"location":"98.wip/03.ingress.and.service.mesh/#tls-via-cert-manager","text":"Setup a secret to hold our cloudflare api key (dns validation) kubectl create secret generic cloudflare-apikey-secret --from-literal = apikey = MYKEYFROMCLOUDA","title":"TLS via Cert-Manager"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/","text":"Installing Calicoctl See the calico docs for more details. mkdir -p ~/calico ; \\ cd ~/calico # Install as a pod wget https://docs.projectcalico.org/v3.10/manifests/calicoctl.yaml ; \\ kubectl apply -f ~/calico/calicoctl.yaml You can then run commands using kubectl as shown below. alicoctl $ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get profiles -o wide NAME LABELS kns.default kns.kube-node-lease kns.kube-public kns.kube-system ksa.default.default ksa.kube-node-lease.default ksa.kube-public.default ksa.kube-system.attachdetach-controller ksa.kube-system.bootstrap-signer ksa.kube-system.calicoctl ksa.kube-system.canal ksa.kube-system.certificate-controller ksa.kube-system.clusterrole-aggregation-controller ksa.kube-system.coredns ksa.kube-system.cronjob-controller ksa.kube-system.daemon-set-controller ksa.kube-system.default ksa.kube-system.deployment-controller ksa.kube-system.disruption-controller ksa.kube-system.endpoint-controller ksa.kube-system.expand-controller ksa.kube-system.generic-garbage-collector ksa.kube-system.horizontal-pod-autoscaler ksa.kube-system.job-controller ksa.kube-system.kube-proxy ksa.kube-system.namespace-controller ksa.kube-system.node-controller ksa.kube-system.persistent-volume-binder ksa.kube-system.pod-garbage-collector ksa.kube-system.pv-protection-controller ksa.kube-system.pvc-protection-controller ksa.kube-system.replicaset-controller ksa.kube-system.replication-controller ksa.kube-system.resourcequota-controller ksa.kube-system.service-account-controller ksa.kube-system.service-controller ksa.kube-system.statefulset-controller ksa.kube-system.token-cleaner ksa.kube-system.ttl-controller We'll create an alias of the kubectl/calicoctl command for sanity. tee ~/.bash_profile <<EOF # Load .bashrc if it exists test -f ~/.bashrc && source ~/.bashrc # Load .bash_aliases if it exists test -f ~/.bash_aliases && source ~/.bash_aliases EOF tee ~/.bash_aliases <<EOF # Calico alias alias calicoctl=\"kubectl exec -i -n kube-system calicoctl /calicoctl -- \" EOF Note: In order to use manifests with this alias you need to redirect them. E.g. calicoctl create -f - < my_manifest.yaml Enable application layer policy (ALP) Application layer policy for Calico allows you to write policies that enforce against application layer attributes like HTTP methods or paths as well as against cryptographically secure identities. Support for application layer policy is not enabled by default in Calico installs, since it requires extra CPU and memory resources to operate. Issue #2943 As detailed in the calico docs Istio 1.1.7 does not support Kubernetes 1.16+. This issue documents the current workarounds and status. As a result the below is left for record of how to implement ALP when a fix is available. Now enable the application layer policy. cd ~/calico # Run calicoctl and output the existing config calicoctl get felixconfiguration default \\ --export -o yaml | \\ sed -e '/ policySyncPathPrefix:/d' \\ -e '$ a\\ policySyncPathPrefix: /var/run/nodeagent' > felix-config.yaml calicoctl apply -f - < felix-config.yaml Install Istio mkdir ~/istio ; \\ cd ~/istio curl -L https://git.io/getLatestIstio | sh - export ISTIO_DIR = $( ls -d istio-* ) export PATH = ~/istio/ $ISTIO_DIR /bin: $PATH # enable autocompletion and add to path tee >> ~/.bash_profile <<EOL # add istio to path export PATH=~/istio/$ISTIO_DIR/bin:\\$PATH # add istioctl autocompletion test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash EOL source ~/.bash_profile Now we'll install the new Istio CNI plugin as per the docs # file: ~/istio/cni.yaml apiVersion : install.istio.io/v1alpha2 kind : IstioControlPlane spec : cni : enabled : true values : cni : excludeNamespaces : - istio-system - kube-system unvalidatedValues : cni : logLevel : info and apply with istioctl manifest apply -f ~/istio/cni.yaml . Now we should enable sidecar injection on the namespaces. kubectl label namespace default istio-injection = enabled Mutual TLS See docs for further details. We'll now create a mesh-wide authentication policy that enables mutual TLS. #file: ~/istio/map.yaml apiVersion : \"authentication.istio.io/v1alpha1\" kind : \"MeshPolicy\" metadata : name : \"default\" spec : peers : - mtls : {} and apply with kubectl apply -f ~/istio/map.yaml And now create a policy which will destination rules. # file: ~/istio/destinations.yaml apiVersion : \"networking.istio.io/v1alpha3\" kind : \"DestinationRule\" metadata : name : \"default\" namespace : \"istio-system\" spec : host : \"*.local\" trafficPolicy : tls : mode : ISTIO_MUTUAL annd apply with kubectl apply -f ~/istio/destinations.yaml kubectl apply -f ${ISTIO_DIR}/nstall/kubernetes/istio-demo-auth.yaml","title":"Installing Calicoctl"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/#installing-calicoctl","text":"See the calico docs for more details. mkdir -p ~/calico ; \\ cd ~/calico # Install as a pod wget https://docs.projectcalico.org/v3.10/manifests/calicoctl.yaml ; \\ kubectl apply -f ~/calico/calicoctl.yaml You can then run commands using kubectl as shown below. alicoctl $ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get profiles -o wide NAME LABELS kns.default kns.kube-node-lease kns.kube-public kns.kube-system ksa.default.default ksa.kube-node-lease.default ksa.kube-public.default ksa.kube-system.attachdetach-controller ksa.kube-system.bootstrap-signer ksa.kube-system.calicoctl ksa.kube-system.canal ksa.kube-system.certificate-controller ksa.kube-system.clusterrole-aggregation-controller ksa.kube-system.coredns ksa.kube-system.cronjob-controller ksa.kube-system.daemon-set-controller ksa.kube-system.default ksa.kube-system.deployment-controller ksa.kube-system.disruption-controller ksa.kube-system.endpoint-controller ksa.kube-system.expand-controller ksa.kube-system.generic-garbage-collector ksa.kube-system.horizontal-pod-autoscaler ksa.kube-system.job-controller ksa.kube-system.kube-proxy ksa.kube-system.namespace-controller ksa.kube-system.node-controller ksa.kube-system.persistent-volume-binder ksa.kube-system.pod-garbage-collector ksa.kube-system.pv-protection-controller ksa.kube-system.pvc-protection-controller ksa.kube-system.replicaset-controller ksa.kube-system.replication-controller ksa.kube-system.resourcequota-controller ksa.kube-system.service-account-controller ksa.kube-system.service-controller ksa.kube-system.statefulset-controller ksa.kube-system.token-cleaner ksa.kube-system.ttl-controller We'll create an alias of the kubectl/calicoctl command for sanity. tee ~/.bash_profile <<EOF # Load .bashrc if it exists test -f ~/.bashrc && source ~/.bashrc # Load .bash_aliases if it exists test -f ~/.bash_aliases && source ~/.bash_aliases EOF tee ~/.bash_aliases <<EOF # Calico alias alias calicoctl=\"kubectl exec -i -n kube-system calicoctl /calicoctl -- \" EOF Note: In order to use manifests with this alias you need to redirect them. E.g. calicoctl create -f - < my_manifest.yaml","title":"Installing Calicoctl"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/#enable-application-layer-policy-alp","text":"Application layer policy for Calico allows you to write policies that enforce against application layer attributes like HTTP methods or paths as well as against cryptographically secure identities. Support for application layer policy is not enabled by default in Calico installs, since it requires extra CPU and memory resources to operate. Issue #2943 As detailed in the calico docs Istio 1.1.7 does not support Kubernetes 1.16+. This issue documents the current workarounds and status. As a result the below is left for record of how to implement ALP when a fix is available. Now enable the application layer policy. cd ~/calico # Run calicoctl and output the existing config calicoctl get felixconfiguration default \\ --export -o yaml | \\ sed -e '/ policySyncPathPrefix:/d' \\ -e '$ a\\ policySyncPathPrefix: /var/run/nodeagent' > felix-config.yaml calicoctl apply -f - < felix-config.yaml","title":"Enable application layer policy (ALP)"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/#install-istio","text":"mkdir ~/istio ; \\ cd ~/istio curl -L https://git.io/getLatestIstio | sh - export ISTIO_DIR = $( ls -d istio-* ) export PATH = ~/istio/ $ISTIO_DIR /bin: $PATH # enable autocompletion and add to path tee >> ~/.bash_profile <<EOL # add istio to path export PATH=~/istio/$ISTIO_DIR/bin:\\$PATH # add istioctl autocompletion test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash EOL source ~/.bash_profile Now we'll install the new Istio CNI plugin as per the docs # file: ~/istio/cni.yaml apiVersion : install.istio.io/v1alpha2 kind : IstioControlPlane spec : cni : enabled : true values : cni : excludeNamespaces : - istio-system - kube-system unvalidatedValues : cni : logLevel : info and apply with istioctl manifest apply -f ~/istio/cni.yaml . Now we should enable sidecar injection on the namespaces. kubectl label namespace default istio-injection = enabled","title":"Install Istio"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/#mutual-tls","text":"See docs for further details. We'll now create a mesh-wide authentication policy that enables mutual TLS. #file: ~/istio/map.yaml apiVersion : \"authentication.istio.io/v1alpha1\" kind : \"MeshPolicy\" metadata : name : \"default\" spec : peers : - mtls : {} and apply with kubectl apply -f ~/istio/map.yaml And now create a policy which will destination rules. # file: ~/istio/destinations.yaml apiVersion : \"networking.istio.io/v1alpha3\" kind : \"DestinationRule\" metadata : name : \"default\" namespace : \"istio-system\" spec : host : \"*.local\" trafficPolicy : tls : mode : ISTIO_MUTUAL annd apply with kubectl apply -f ~/istio/destinations.yaml kubectl apply -f ${ISTIO_DIR}/nstall/kubernetes/istio-demo-auth.yaml","title":"Mutual TLS"},{"location":"98.wip/coreos/bare_metal/","text":"Installing CoreOS to a Bare Metal host If, like, me you want to run this on-prem then this chapter will quickly run through how to get setup in an environment where you've got a physical host and don't want to rely on a cloud provider. We're going to download an iso, install to disk and then run through some configuration specifics for my environment. Bits might be useful / relevant to you depending on your own setup so YMMV. Obtain the latest image Navigate to the CoreOS, Container Linux website and download the latest iso file. cd ~/Downloads wget https://stable.release.core-os.net/amd64-usr/current/coreos_production_iso_image.iso With the image downloaded now write it to disk with a sudo dd if=~/Downloads/coreos_production_iso_image.iso of=/dev/path/tousb bs=1m same as any other iso. Boot the host using usb Couple of things to note: CoreOS relies on BIOS as opposed to UEFI Make sure you set the USB emulation to Hard Disk if given the option in your BIOS Once booted up you can run a sudo passwd core to set a password on the main user and then head back to a screen somewhere else to continue the setup over ssh. Configuration There's a few examples on the official website but the best documentation currently resides in the GitHub repo . My particular setup at home has the following characteristics: Network bonding Physical rack-mounted hosts each with +4 NICs (Dell R610 and Dell R710) Managed switch with configured LACP for bonded network interfaces As a result I'll also be configuring the CoreOS images to run bonded network 802.3ad NICs to maximise network throughput. Local Filesystem On each of the servers I've got a slightly different storage setup. I'll mount these extra disks (over and above the standard root) into the host under the /data directory. Avahi Network Discovery For the sake of ease I'm going to run avahi to broadcast/multicast my server hostnames etc. on the local network. There's a custom docker image, associated systemd service and a config file being created as a result. Config vs Ignitiion Container Linux has the concept of creating a human-readable Container Linux Config and then compiling this into a condensed (and validated) machine-readable format called Ignition . As a result we'll create a config file and then transpile it into an ignition one (which is actually what gets loaded and read by CoreOS). The tool for performing this can be found on GitHub here . Configuratiuon My configs can be found here so I won't bother replicating wholesale here too. Once you've created your own we're ready to go. Either ssh into the machine and run vi config to copy/paste the above config file in or scp it across and then ssh in. If you haven't installed the ct application locally to transpile the config we need to get it on the remote host. Go to the releases to find the latest one. Transpile the config wget https://github.com/coreos/container-linux-config-transpiler/releases/download/v0.9.0/ct-v0.9.0-x86_64-unknown-linux-gnu && \\ sudo mkdir -p /opt/bin && \\ sudo mv ct* /opt/bin/ct && \\ sudo chmod +x /opt/bin/ct Typing ct --help should now return the following output in the terminal. ct Usage of ct: -files-dir string Directory to read local files from. -help Print help and exit. -in-file string Path to the container linux config. Standard input unless specified otherwise. -out-file string Path to the resulting Ignition config. Standard output unless specified otherwise. -platform string Platform to target. Accepted values: [azure digitalocean ec2 gce packet openstack-metadata vagrant-virtualbox cloudstack-configdrive custom]. -pretty Indent the resulting Ignition config. -strict Fail if any warnings are encountered. -version Print the version and exit. We'll now transpile the config. ct -platform custom -in-file /path/to/config.yml -out-file /path/to/ignition.json If there aren't any errors you can review the minified output file which will be a condensed JSON object with as much whitespace removed as possible. Installation With the ignition file now created it's time to install (change your device as necessary)... sudo coreos-install -d /dev/sda -i ignition.json # Once completed reboot sudo shutdown -r now","title":"Installing CoreOS to a Bare Metal host"},{"location":"98.wip/coreos/bare_metal/#installing-coreos-to-a-bare-metal-host","text":"If, like, me you want to run this on-prem then this chapter will quickly run through how to get setup in an environment where you've got a physical host and don't want to rely on a cloud provider. We're going to download an iso, install to disk and then run through some configuration specifics for my environment. Bits might be useful / relevant to you depending on your own setup so YMMV. Obtain the latest image Navigate to the CoreOS, Container Linux website and download the latest iso file. cd ~/Downloads wget https://stable.release.core-os.net/amd64-usr/current/coreos_production_iso_image.iso With the image downloaded now write it to disk with a sudo dd if=~/Downloads/coreos_production_iso_image.iso of=/dev/path/tousb bs=1m same as any other iso. Boot the host using usb Couple of things to note: CoreOS relies on BIOS as opposed to UEFI Make sure you set the USB emulation to Hard Disk if given the option in your BIOS Once booted up you can run a sudo passwd core to set a password on the main user and then head back to a screen somewhere else to continue the setup over ssh.","title":"Installing CoreOS to a Bare Metal host"},{"location":"98.wip/coreos/bare_metal/#configuration","text":"There's a few examples on the official website but the best documentation currently resides in the GitHub repo . My particular setup at home has the following characteristics: Network bonding Physical rack-mounted hosts each with +4 NICs (Dell R610 and Dell R710) Managed switch with configured LACP for bonded network interfaces As a result I'll also be configuring the CoreOS images to run bonded network 802.3ad NICs to maximise network throughput. Local Filesystem On each of the servers I've got a slightly different storage setup. I'll mount these extra disks (over and above the standard root) into the host under the /data directory. Avahi Network Discovery For the sake of ease I'm going to run avahi to broadcast/multicast my server hostnames etc. on the local network. There's a custom docker image, associated systemd service and a config file being created as a result.","title":"Configuration"},{"location":"98.wip/coreos/bare_metal/#config-vs-ignitiion","text":"Container Linux has the concept of creating a human-readable Container Linux Config and then compiling this into a condensed (and validated) machine-readable format called Ignition . As a result we'll create a config file and then transpile it into an ignition one (which is actually what gets loaded and read by CoreOS). The tool for performing this can be found on GitHub here .","title":"Config vs Ignitiion"},{"location":"98.wip/coreos/bare_metal/#configuratiuon","text":"My configs can be found here so I won't bother replicating wholesale here too. Once you've created your own we're ready to go. Either ssh into the machine and run vi config to copy/paste the above config file in or scp it across and then ssh in. If you haven't installed the ct application locally to transpile the config we need to get it on the remote host. Go to the releases to find the latest one.","title":"Configuratiuon"},{"location":"98.wip/coreos/bare_metal/#transpile-the-config","text":"wget https://github.com/coreos/container-linux-config-transpiler/releases/download/v0.9.0/ct-v0.9.0-x86_64-unknown-linux-gnu && \\ sudo mkdir -p /opt/bin && \\ sudo mv ct* /opt/bin/ct && \\ sudo chmod +x /opt/bin/ct Typing ct --help should now return the following output in the terminal. ct Usage of ct: -files-dir string Directory to read local files from. -help Print help and exit. -in-file string Path to the container linux config. Standard input unless specified otherwise. -out-file string Path to the resulting Ignition config. Standard output unless specified otherwise. -platform string Platform to target. Accepted values: [azure digitalocean ec2 gce packet openstack-metadata vagrant-virtualbox cloudstack-configdrive custom]. -pretty Indent the resulting Ignition config. -strict Fail if any warnings are encountered. -version Print the version and exit. We'll now transpile the config. ct -platform custom -in-file /path/to/config.yml -out-file /path/to/ignition.json If there aren't any errors you can review the minified output file which will be a condensed JSON object with as much whitespace removed as possible.","title":"Transpile the config"},{"location":"98.wip/coreos/bare_metal/#installation","text":"With the ignition file now created it's time to install (change your device as necessary)... sudo coreos-install -d /dev/sda -i ignition.json # Once completed reboot sudo shutdown -r now","title":"Installation"},{"location":"98.wip/rancheros/rancheros/","text":"Configuration for RancherOS is performed by a cloud-init file. The main things we want to do is enable the kernel-headers services (so that we can run wireguard later on) and then install our configuration via a yaml file. sudo ros service enable kernel-headers sudo ros service enable kernel-headers-system-docker This can be added to a cloud-init file. Running sudo ros config export will show the actual settings to add. $ sudo ros config export rancher: environment: EXTRA_CMDLINE: /init services_include: kernel-headers: true kernel-headers-system-docker: true ssh_authorized_keys: [] Whilst each individual node will potentially have some different configurations for the likes of physcial networking we will have a section at the top that simply enables wireguard. rancher : # Load Kernel module for wireguard modules : [ wireguard ] We also want to install a compatible docker version rancher : # https://rancher.com/docs/os/v1.x/en/installation/configuration/switching-docker-versions/ docker : engine : docker-18.09.9","title":"Rancheros"},{"location":"98.wip/storage-server/0203.cache-tier/","text":"We're going to create a couple of storage pools across our Ceph disks in order to showcase how cache-tiering can be applied to speed up performance of read/write operations using SSDs whilst still persisting longer-term storage onto cheaper HDD media. Create Storage Pools and Classes We'll create two pools: hot-storage : running on SSDs with a replica of 1 only ecpool : running on HDDs with erasureCode applied (roughly equivalent to a RAID5 configuration) # file: ~/rook/storage-pools.yaml # Create a pool with only 1 replica for the # SSDs to use. # Use just ssds for this. apiVersion : ceph.rook.io/v1 kind : CephBlockPool metadata : name : hot-storage namespace : rook-ceph spec : failureDomain : osd replicated : size : 1 deviceClass : ssd --- # Create a pool with erasure coding for the # backing tier to use for persistence (media). # Use just hdds for this. apiVersion : ceph.rook.io/v1 kind : CephBlockPool metadata : name : ecpool namespace : rook-ceph spec : failureDomain : osd erasureCoded : dataChunks : 2 codingChunks : 1 deviceClass : hdd Apply with kubectl apply -f ~/rook/storage-pools.yaml Storage Class We now need to create associated StorageClass objects that use these pools. # file: ~/rook/storageClasses.yaml apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block-ecpool provisioner : rook-ceph.rbd.csi.ceph.com parameters : clusterID : rook-ceph pool : ecpool imageFormat : \"2\" imageFeatures : layering # The secrets contain Ceph admin credentials. csi.storage.k8s.io/provisioner-secret-name : rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace : rook-ceph csi.storage.k8s.io/node-stage-secret-name : rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace : rook-ceph # Specify the filesystem type of the volume. If not specified, csi-provisioner # will set default as `ext4`. csi.storage.k8s.io/fstype : xfs # Delete the rbd volume when a PVC is deleted reclaimPolicy : Delete --- apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block-hot-storage provisioner : rook-ceph.rbd.csi.ceph.com parameters : clusterID : rook-ceph pool : host-storage imageFormat : \"2\" imageFeatures : layering # The secrets contain Ceph admin credentials. csi.storage.k8s.io/provisioner-secret-name : rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace : rook-ceph csi.storage.k8s.io/node-stage-secret-name : rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace : rook-ceph # Specify the filesystem type of the volume. If not specified, csi-provisioner # will set default as `ext4`. csi.storage.k8s.io/fstype : xfs # Delete the rbd volume when a PVC is deleted reclaimPolicy : Delete Apply with kubectl apply -f ~/rook/storageClasses.yaml Testing Performance (without cache) Before we apply the tiering we should generate a baseline for the disk performance. We'll create a simple deployment that consumes a persistent volume. Apply cacheing Testing Performance (with cache) Go into ceph-tools and find the available pools $ ceph osd pool ls cachepool ecpool Now place the cachepool as the tier of ecpool in writeback mode so that every write and read to the ecpool are actually using the cachepool and benefit from its flexibility and speed. $ ceph osd tier add ecpool cachepool; \\ ceph osd tier cache-mode cachepool writeback; \\ ceph osd tier set-overlay ecpool cachepool pool 'cachepool' is now (or already was) a tier of 'ecpool' set cache-mode for pool 'cachepool' to writeback overlay for 'ecpool' is now (or already was) 'cachepool' Now we can create some workloads that consume this and check. Using an existing rclone.conf file we will create a task which copies from a configured remote to our new storage.","title":"0203.cache tier"},{"location":"98.wip/storage-server/0203.cache-tier/#create-storage-pools-and-classes","text":"We'll create two pools: hot-storage : running on SSDs with a replica of 1 only ecpool : running on HDDs with erasureCode applied (roughly equivalent to a RAID5 configuration) # file: ~/rook/storage-pools.yaml # Create a pool with only 1 replica for the # SSDs to use. # Use just ssds for this. apiVersion : ceph.rook.io/v1 kind : CephBlockPool metadata : name : hot-storage namespace : rook-ceph spec : failureDomain : osd replicated : size : 1 deviceClass : ssd --- # Create a pool with erasure coding for the # backing tier to use for persistence (media). # Use just hdds for this. apiVersion : ceph.rook.io/v1 kind : CephBlockPool metadata : name : ecpool namespace : rook-ceph spec : failureDomain : osd erasureCoded : dataChunks : 2 codingChunks : 1 deviceClass : hdd Apply with kubectl apply -f ~/rook/storage-pools.yaml","title":"Create Storage Pools and Classes"},{"location":"98.wip/storage-server/0203.cache-tier/#storage-class","text":"We now need to create associated StorageClass objects that use these pools. # file: ~/rook/storageClasses.yaml apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block-ecpool provisioner : rook-ceph.rbd.csi.ceph.com parameters : clusterID : rook-ceph pool : ecpool imageFormat : \"2\" imageFeatures : layering # The secrets contain Ceph admin credentials. csi.storage.k8s.io/provisioner-secret-name : rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace : rook-ceph csi.storage.k8s.io/node-stage-secret-name : rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace : rook-ceph # Specify the filesystem type of the volume. If not specified, csi-provisioner # will set default as `ext4`. csi.storage.k8s.io/fstype : xfs # Delete the rbd volume when a PVC is deleted reclaimPolicy : Delete --- apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block-hot-storage provisioner : rook-ceph.rbd.csi.ceph.com parameters : clusterID : rook-ceph pool : host-storage imageFormat : \"2\" imageFeatures : layering # The secrets contain Ceph admin credentials. csi.storage.k8s.io/provisioner-secret-name : rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace : rook-ceph csi.storage.k8s.io/node-stage-secret-name : rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace : rook-ceph # Specify the filesystem type of the volume. If not specified, csi-provisioner # will set default as `ext4`. csi.storage.k8s.io/fstype : xfs # Delete the rbd volume when a PVC is deleted reclaimPolicy : Delete Apply with kubectl apply -f ~/rook/storageClasses.yaml","title":"Storage Class"},{"location":"98.wip/storage-server/0203.cache-tier/#testing-performance-without-cache","text":"Before we apply the tiering we should generate a baseline for the disk performance. We'll create a simple deployment that consumes a persistent volume.","title":"Testing Performance (without cache)"},{"location":"98.wip/storage-server/0203.cache-tier/#apply-cacheing","text":"","title":"Apply cacheing"},{"location":"98.wip/storage-server/0203.cache-tier/#testing-performance-with-cache","text":"Go into ceph-tools and find the available pools $ ceph osd pool ls cachepool ecpool Now place the cachepool as the tier of ecpool in writeback mode so that every write and read to the ecpool are actually using the cachepool and benefit from its flexibility and speed. $ ceph osd tier add ecpool cachepool; \\ ceph osd tier cache-mode cachepool writeback; \\ ceph osd tier set-overlay ecpool cachepool pool 'cachepool' is now (or already was) a tier of 'ecpool' set cache-mode for pool 'cachepool' to writeback overlay for 'ecpool' is now (or already was) 'cachepool' Now we can create some workloads that consume this and check. Using an existing rclone.conf file we will create a task which copies from a configured remote to our new storage.","title":"Testing Performance (with cache)"},{"location":"99.snippets/ceph/","text":"Useful snippets for interacting with Ceph. Toolbox Connect to the toolbox kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash You can then run commands such as ceph device ls Normal If you don't have any devices or OSDs created check what's happening during startup. See docs # get the prepare pods in the cluster $ kubectl -n rook-ceph get pod -l app = rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-node1-fvmrp 0 /1 Completed 0 18m rook-ceph-osd-prepare-node2-w9xv9 0 /1 Completed 0 22m rook-ceph-osd-prepare-node3-7rgnv 0 /1 Completed 0 22m # view the logs for the node of interest in the \"provision\" container $ kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision Remove and wipe kubectl delete -f ~/rook/toolbox.yaml ; \\ kubectl delete -f ~/rook/cluster.yaml ; \\ kubectl delete -f ~/rook/operator.yaml ; \\ kubectl delete -f ~/rook/common.yaml ; \\ kubectl delete -f ~/rook/ ; \\ kubectl delete namespace rook-ceph ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Ceph"},{"location":"99.snippets/ceph/#toolbox","text":"Connect to the toolbox kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash You can then run commands such as ceph device ls","title":"Toolbox"},{"location":"99.snippets/ceph/#normal","text":"If you don't have any devices or OSDs created check what's happening during startup. See docs # get the prepare pods in the cluster $ kubectl -n rook-ceph get pod -l app = rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-node1-fvmrp 0 /1 Completed 0 18m rook-ceph-osd-prepare-node2-w9xv9 0 /1 Completed 0 22m rook-ceph-osd-prepare-node3-7rgnv 0 /1 Completed 0 22m # view the logs for the node of interest in the \"provision\" container $ kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision","title":"Normal"},{"location":"99.snippets/ceph/#remove-and-wipe","text":"kubectl delete -f ~/rook/toolbox.yaml ; \\ kubectl delete -f ~/rook/cluster.yaml ; \\ kubectl delete -f ~/rook/operator.yaml ; \\ kubectl delete -f ~/rook/common.yaml ; \\ kubectl delete -f ~/rook/ ; \\ kubectl delete namespace rook-ceph ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Remove and wipe"},{"location":"99.snippets/kubernetes/","text":"Useful snippets of code for interfacing with Kubernetes Wiping a kubernetes installation and starting again Because sometimes stuff goes wrong... kubeadm reset ; \\ sudo apt-get -y purge kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get -y autoremove ; \\ sudo rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd sudo shutdown -r now sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl sudo kubeadm init \\ --pod-network-cidr = 10 .244.0.0/16 \\ --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) To also ensure that rook configuration has been removed sudo rm -rf /var/lib/rook/* Getting the IP address of a node export NODE_NAME = banks kubectl get node ${ NODE_NAME } -o jsonpath = '{.status.addresses[0].address}' Kill a namespace stuck as \"Terminating\" See: https://stackoverflow.com/a/53661717/322358 export NAMESPACE = your-rogue-namespace kubectl proxy & kubectl get namespace $NAMESPACE -o json | jq '.spec = {\"finalizers\":[]}' >temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127 .0.0.1:8001/api/v1/namespaces/ $NAMESPACE /finalize","title":"Kubernetes"},{"location":"99.snippets/kubernetes/#wiping-a-kubernetes-installation-and-starting-again","text":"Because sometimes stuff goes wrong... kubeadm reset ; \\ sudo apt-get -y purge kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get -y autoremove ; \\ sudo rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd sudo shutdown -r now sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl sudo kubeadm init \\ --pod-network-cidr = 10 .244.0.0/16 \\ --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) To also ensure that rook configuration has been removed sudo rm -rf /var/lib/rook/*","title":"Wiping a kubernetes installation and starting again"},{"location":"99.snippets/kubernetes/#getting-the-ip-address-of-a-node","text":"export NODE_NAME = banks kubectl get node ${ NODE_NAME } -o jsonpath = '{.status.addresses[0].address}'","title":"Getting the IP address of a node"},{"location":"99.snippets/kubernetes/#kill-a-namespace-stuck-as-terminating","text":"See: https://stackoverflow.com/a/53661717/322358 export NAMESPACE = your-rogue-namespace kubectl proxy & kubectl get namespace $NAMESPACE -o json | jq '.spec = {\"finalizers\":[]}' >temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127 .0.0.1:8001/api/v1/namespaces/ $NAMESPACE /finalize","title":"Kill a namespace stuck as \"Terminating\""},{"location":"99.snippets/storage.disks/","text":"How to setup encrypted disks for usage by Ceph If Rook determines that a device is not available (has existing partitions or a formatted file system ) then it will skip consuming the devices. As a result we need to encrypt the disk and leave it without partitions or a filesystem for it to be read correctly. Finding the relevant disks is done with lsblk . example lsblk $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID } Auto-mount encrypted devices at boot We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Storage"},{"location":"99.snippets/storage.disks/#how-to-setup-encrypted-disks-for-usage-by-ceph","text":"If Rook determines that a device is not available (has existing partitions or a formatted file system ) then it will skip consuming the devices. As a result we need to encrypt the disk and leave it without partitions or a filesystem for it to be read correctly. Finding the relevant disks is done with lsblk . example lsblk $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID }","title":"How to setup encrypted disks for usage by Ceph"},{"location":"99.snippets/storage.disks/#auto-mount-encrypted-devices-at-boot","text":"We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Auto-mount encrypted devices at boot"}]}