{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Darth-Veitcher's Homelab This repository contains docker details, notes and general setup musings around configuring my (overkill) homelab end-to-end. This has been deliberately done the hard way (i.e. handcrafted from scratch) wherever possible so that I understand what's going on under the hood. I'm not a big fan of black boxes. High level requirements, roadmap and table of contents Reference Architecture (appendix) Configuring physical nodes VPN between nodes Basic hardening Configuring kubernetes Networking and Network Policy via Canal Service Discovery via CoreDNS Ceph storage via Rook Block storage for pods Shared fielsystem Object storage with LDAP for authentication ; and Integrated with Vault for secure key management With dashboard Enabled object gateway management via SSO Monitoring with Prometheus and Grafana Cloudflare integration with external-dns TLS certificates with Cert-Manager Self-signed Root CA for internal services Let's Encrypt for external services Identity and Access Management with OIDC OpenLDAP as directory service KeyCloak/Dex as identity provider Multi-factor auth for key admin users OIDC Forward Auth for additional fine grained RBAC Secure VPN access for users Integrated with PKI Integrated with IDAM, Keycloak+OpenLDAP Services Stack: Developer Docker registry Gitea","title":"Introduction"},{"location":"#darth-veitchers-homelab","text":"This repository contains docker details, notes and general setup musings around configuring my (overkill) homelab end-to-end. This has been deliberately done the hard way (i.e. handcrafted from scratch) wherever possible so that I understand what's going on under the hood. I'm not a big fan of black boxes.","title":"Darth-Veitcher's Homelab"},{"location":"#high-level-requirements-roadmap-and-table-of-contents","text":"Reference Architecture (appendix) Configuring physical nodes VPN between nodes Basic hardening Configuring kubernetes Networking and Network Policy via Canal Service Discovery via CoreDNS Ceph storage via Rook Block storage for pods Shared fielsystem Object storage with LDAP for authentication ; and Integrated with Vault for secure key management With dashboard Enabled object gateway management via SSO Monitoring with Prometheus and Grafana Cloudflare integration with external-dns TLS certificates with Cert-Manager Self-signed Root CA for internal services Let's Encrypt for external services Identity and Access Management with OIDC OpenLDAP as directory service KeyCloak/Dex as identity provider Multi-factor auth for key admin users OIDC Forward Auth for additional fine grained RBAC Secure VPN access for users Integrated with PKI Integrated with IDAM, Keycloak+OpenLDAP Services Stack: Developer Docker registry Gitea","title":"High level requirements, roadmap and table of contents"},{"location":"01 ldap/","text":"OpenLDAP | Directory Information We're going to follow a previous tutorial I wrote for setting up an initial OpenLDAP installation on a VM and then seed an initial admin user. Dockerising this is done now with the osixia/openldap base image which can be found on GitHub . We'll follow the Seed ldap database with ldif method to modify and seed our initial datasets which also supports the following substitutions. {{ LDAP_BASE_DN }} {{ LDAP_BACKEND }} {{ LDAP_DOMAIN }} {{ LDAP_READONLY_USER_USERNAME }} {{ LDAP_READONLY_USER_PASSWORD_ENCRYPTED }} The ldif files can then be loaded into the /container/service/slapd/assets/config/bootstrap/ldif/ folder as a volume mount. Once running we can execute the below in order to query and check success of the seed data: # replace `supermansucks` with whatever you define as the admin password # in the LDAP_ADMIN_PASSWORD environment variables docker exec ldap ldapsearch -x -H ldap://localhost -b dc=xmansion,dc=local -D \"cn=admin,dc=xmansion,dc=local\" -w supermansucks The output should look like this. # extended LDIF # # LDAPv3 # base <dc=xmansion,dc=local> with scope subtree # filter: (objectclass=*) # requesting: ALL # # xmansion.local dn: dc=xmansion,dc=local objectClass: top objectClass: dcObject objectClass: organization o: Mancave Inc. dc: xmansion # admin, xmansion.local dn: cn=admin,dc=xmansion,dc=local objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9N1UyOFVFbEY5WWlZS2hIOVBwdjNoNXJsZ1ZHQ0RPUnQ= # People, xmansion.local dn: ou=People,dc=xmansion,dc=local objectClass: organizationalUnit ou: People # Groups, xmansion.local dn: ou=Groups,dc=xmansion,dc=local objectClass: organizationalUnit ou: Groups # admins, Groups, xmansion.local dn: cn=admins,ou=Groups,dc=xmansion,dc=local objectClass: posixGroup cn: admins gidNumber: 5000 # james, People, xmansion.local dn: uid=james,ou=People,dc=xmansion,dc=local objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount uid: james sn: veitch givenName: James cn: James Veitch displayName: James Veitch uidNumber: 10000 gidNumber: 5000 userPassword:: cGFzc3dvcmQ= gecos: James Veitch loginShell: /bin/bash homeDirectory: /home/james # search result search: 2 result: 0 Success # numResponses: 7 # numEntries: 6","title":"OpenLDAP | <small>Directory Information</small>"},{"location":"01 ldap/#openldap-directory-information","text":"We're going to follow a previous tutorial I wrote for setting up an initial OpenLDAP installation on a VM and then seed an initial admin user. Dockerising this is done now with the osixia/openldap base image which can be found on GitHub . We'll follow the Seed ldap database with ldif method to modify and seed our initial datasets which also supports the following substitutions. {{ LDAP_BASE_DN }} {{ LDAP_BACKEND }} {{ LDAP_DOMAIN }} {{ LDAP_READONLY_USER_USERNAME }} {{ LDAP_READONLY_USER_PASSWORD_ENCRYPTED }} The ldif files can then be loaded into the /container/service/slapd/assets/config/bootstrap/ldif/ folder as a volume mount. Once running we can execute the below in order to query and check success of the seed data: # replace `supermansucks` with whatever you define as the admin password # in the LDAP_ADMIN_PASSWORD environment variables docker exec ldap ldapsearch -x -H ldap://localhost -b dc=xmansion,dc=local -D \"cn=admin,dc=xmansion,dc=local\" -w supermansucks The output should look like this. # extended LDIF # # LDAPv3 # base <dc=xmansion,dc=local> with scope subtree # filter: (objectclass=*) # requesting: ALL # # xmansion.local dn: dc=xmansion,dc=local objectClass: top objectClass: dcObject objectClass: organization o: Mancave Inc. dc: xmansion # admin, xmansion.local dn: cn=admin,dc=xmansion,dc=local objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9N1UyOFVFbEY5WWlZS2hIOVBwdjNoNXJsZ1ZHQ0RPUnQ= # People, xmansion.local dn: ou=People,dc=xmansion,dc=local objectClass: organizationalUnit ou: People # Groups, xmansion.local dn: ou=Groups,dc=xmansion,dc=local objectClass: organizationalUnit ou: Groups # admins, Groups, xmansion.local dn: cn=admins,ou=Groups,dc=xmansion,dc=local objectClass: posixGroup cn: admins gidNumber: 5000 # james, People, xmansion.local dn: uid=james,ou=People,dc=xmansion,dc=local objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount uid: james sn: veitch givenName: James cn: James Veitch displayName: James Veitch uidNumber: 10000 gidNumber: 5000 userPassword:: cGFzc3dvcmQ= gecos: James Veitch loginShell: /bin/bash homeDirectory: /home/james # search result search: 2 result: 0 Success # numResponses: 7 # numEntries: 6","title":"OpenLDAP | Directory Information"},{"location":"02 keycloak/","text":"Keycloak | Authentication and Authorisation for services To configure Keycloak to work with OpenLDAP we need to login and setup our ldap container as a User Federation provider. Key setting as follows: Vendor : Other Edit Mode : WRITABLE Connection URL : ldap://ldap Users DN : ou=People,dc=xmansion,dc=local Bind DN : cn=admin,dc=xmansion,dc=local Bind Credential : We bootstrap this into Keycloak via the Importing a realm option. The easiest way to get this back out of the system is to go the Export setting and then get the JSON output. Unfortunately, due to the way in which the image is configured, the given method in the docs doesn't work (as volumes are mounted by root yet the application executes as the jboss user and therefore can't access the files). As a result we inherit from and build a custom image with a /realms folder that we can mount the JSON files into. Enabling MFA Stealing with pride from documentation elsewhere we're going to enable TOTP based MFA for our initial user. In the GUI you would navigate to Authentication \u2192 OTP Policy and then update the following settings as required. The below are those we're using: OTP Type : Time Based OTP Hash Algorith : SHA256 Number of Digits : 8 Look Ahead Window : 3 OTP Token Period : 30 Depending on appetite we can also navigate to the Authentication \u2192 Required Actions tab and tick the Default Action box against Configure OTP if we want to enforce this for everyone by default. Accessing account and admin console The admin console and account details can be accessed from the following urls: Security Admin Console Account","title":"Keycloak | <small>Authentication and Authorisation for services</small>"},{"location":"02 keycloak/#keycloak-authentication-and-authorisation-for-services","text":"To configure Keycloak to work with OpenLDAP we need to login and setup our ldap container as a User Federation provider. Key setting as follows: Vendor : Other Edit Mode : WRITABLE Connection URL : ldap://ldap Users DN : ou=People,dc=xmansion,dc=local Bind DN : cn=admin,dc=xmansion,dc=local Bind Credential : We bootstrap this into Keycloak via the Importing a realm option. The easiest way to get this back out of the system is to go the Export setting and then get the JSON output. Unfortunately, due to the way in which the image is configured, the given method in the docs doesn't work (as volumes are mounted by root yet the application executes as the jboss user and therefore can't access the files). As a result we inherit from and build a custom image with a /realms folder that we can mount the JSON files into.","title":"Keycloak | Authentication and Authorisation for services"},{"location":"02 keycloak/#enabling-mfa","text":"Stealing with pride from documentation elsewhere we're going to enable TOTP based MFA for our initial user. In the GUI you would navigate to Authentication \u2192 OTP Policy and then update the following settings as required. The below are those we're using: OTP Type : Time Based OTP Hash Algorith : SHA256 Number of Digits : 8 Look Ahead Window : 3 OTP Token Period : 30 Depending on appetite we can also navigate to the Authentication \u2192 Required Actions tab and tick the Default Action box against Configure OTP if we want to enforce this for everyone by default.","title":"Enabling MFA"},{"location":"02 keycloak/#accessing-account-and-admin-console","text":"The admin console and account details can be accessed from the following urls: Security Admin Console Account","title":"Accessing account and admin console"},{"location":"01.infrastructure/01.hosts/00.configuring.physical.nodes/","text":"Physical Nodes I'm going to start with a single node install on a server at home and then, slowly, add additional nodes to create a cluster. I'll taint the first node though so that it can run standalone without needing further nodes to operate. My final configuration will look roughly like this: Homelab Server 1: Dell R610, Primary Compute, asimov Server 2: Dell R710, Secondary Compute, banks Server 3: Custom Build, Primary LAN Storage, clarke Cloud Dedicated Server 1: Kimsufi, Ingress Node, donaldson Dedicated Server 2: Kimsufi, Compute Node, eesmith Between the Homelab and Cloud I'll run a VPN such that traffic is encrypted and we can bypass the public internet and any associated NAT / routing issues by having the nodes discover each other using VPN subnet addresses. We'll start with a single node on-prem and then expand and add to this.","title":"Host Configuration"},{"location":"01.infrastructure/01.hosts/00.configuring.physical.nodes/#physical-nodes","text":"I'm going to start with a single node install on a server at home and then, slowly, add additional nodes to create a cluster. I'll taint the first node though so that it can run standalone without needing further nodes to operate. My final configuration will look roughly like this: Homelab Server 1: Dell R610, Primary Compute, asimov Server 2: Dell R710, Secondary Compute, banks Server 3: Custom Build, Primary LAN Storage, clarke Cloud Dedicated Server 1: Kimsufi, Ingress Node, donaldson Dedicated Server 2: Kimsufi, Compute Node, eesmith Between the Homelab and Cloud I'll run a VPN such that traffic is encrypted and we can bypass the public internet and any associated NAT / routing issues by having the nodes discover each other using VPN subnet addresses. We'll start with a single node on-prem and then expand and add to this.","title":"Physical Nodes"},{"location":"01.infrastructure/01.hosts/01.banks/","text":"Banks (compute node) Basic housekeeping First of all install the latest Ubuntu 18.04 LTS release and copy across your ssh key and then follow some simple hardening steps #TODO: link to ansible . We'll configure our hostname too in order to reflect whatever the fqdn of our server is going to be. In my case this will be banks.local as it will just be an internal address. This is used by kubernetes later on. sudo hostnamectl set-hostname banks.local Setup Wireguard (VPN) See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. I've gone with Wireguard over OpenVPN based on it being incorporated into the Linux Kernel and increased performance versus OpenVPN. Install Wireguard sudo add-apt-repository -y ppa:wireguard/wireguard sudo apt-get install -y wireguard sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. root@banks:~# lsmod | grep wireguard wireguard 212992 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Keys You will need to generate a key-pair for every peer (device) that is connected, including things like mobile phones etc. The iOS WireGuard client allow you to generate the keys on the device itself (if you want). # Generate public/private keypair cd /etc/wireguard umask 077 wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey Configure We need to create a network interface now for the wireguard VPN. Common convention is to use wg0 as a name for this. In addition we also need to choose a subnet for the VPN addresses. As I've got a 192.168.0.1/24 configuration at home I'll use 10.10.0.1/24 for the VPN. Note the highlighted IP address we assign to each node here. It will need to be incremented for each to provide a unique address. # file: /etc/wireguard/wg0.conf [ Interface ] PrivateKey = {{ PRIVATE_KEY }} Address = 10 .10.0.1/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT ; iptables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -A FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT ; iptables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -D FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ListenPort = 51820 Now to keepo things DRY we'll run the following to replace the placeholder text with the actual contents of our server's private key we generated earlier. sudo sed -i.bak 's/{{PRIVATE_KEY}}/' $( sudo cat /etc/wireguard/privatekey ) '/' /etc/wireguard/wg0.conf We also need to replace the {{ETH0}} placeholder with the name of our existing primary network interface. A quick one-liner for this is ip -4 route | grep default | awk '{print $5}' which, on my server, gives bond0 as the answer (as I'm running LACP across multiple bonded physical interfaces). sudo sed -i.bak 's/{{ETH0}}/' $( ip -4 route | grep default | awk '{print $5}' ) '/g' /etc/wireguard/wg0.conf Enable forwarding of packets in the host kernel. sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p Finally we can start the wg0 interface. wg-quick up wg0 Hopefully you'll see something like the below output. root@banks:/etc/wireguard# wg-quick up wg0 [ #] ip link add wg0 type wireguard [ #] wg setconf wg0 /dev/fd/63 [ #] ip -4 address add 10.10.0.1/24 dev wg0 [ #] ip -6 address add fd86:ea04:1111::1/64 dev wg0 [ #] ip link set mtu 1420 up dev wg0 [ #] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o bond0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o bond0 -j MASQUERADE Checking status The check the status of wireguard run the wg command. root@banks:/etc/wireguard# wg interface: wg0 public key: I6ZHsLe44SHNH44xE86AI0VEnm8CfzrQUrxSCJVjAEw = private key: ( hidden ) listening port: 51820 In addition, we should now have an additional route appear for our VPN subnet. root@banks:/etc/wireguard# ip route default via 192 .168.0.1 dev bond0 proto dhcp src 192 .168.0.94 metric 100 10 .10.0.0/24 dev wg0 proto kernel scope link src 10 .10.0.1 172 .17.0.0/16 dev docker0 proto kernel scope link src 172 .17.0.1 linkdown ... Systemd service Assuming the above works we can now enable the wg0 interface on boot. sudo systemctl enable wg-quick@wg0.service sudo systemctl daemon-reload You can then manually start sudo service wg-quick@wg0 start and check status service wg-quick@wg0 status of the service. Enable Avahi (Discovery) on VPN and LAN As per Wikipedia . Avahi is a free zero-configuration networking (zeroconf) implementation, including a system for multicast DNS/DNS-SD service discovery. It is licensed under the GNU Lesser General Public License (LGPL). Avahi is a system which enables programs to publish and discover services and hosts running on a local network. For example, a user can plug a computer into a network and have Avahi automatically advertise the network services running on its machine, facilitating user access to those services. We will setup our nodes to publish themselves on both the LAN (so can be accessed via their hostnames) and VPN. export DEBIAN_FRONTEND = noninteractive # from docker sudo apt-get update -y sudo apt-get -qq install -y avahi-daemon avahi-utils The main configuration is held in /etc/avahi/avahi-daemon.conf . We want to modify the following line (to limit which interfaces we advertise on). This will be useful for when we want to only enable it on internal interfaces (e.g. our Cloud node shouldn't try and broadcast across the in) #file: /etc/avahi/avahi-daemon.conf allow-interfaces = bond0, wg0, docker0 # If we're using an internal domain then leave as `local` below, else change to tld domain-name = local Reload / restart the daemon with sudo systemctl daemon-reload && sudo systemctl restart avahi-daemon.service","title":"Compute (Banks)"},{"location":"01.infrastructure/01.hosts/01.banks/#banks-compute-node","text":"","title":"Banks (compute node)"},{"location":"01.infrastructure/01.hosts/01.banks/#basic-housekeeping","text":"First of all install the latest Ubuntu 18.04 LTS release and copy across your ssh key and then follow some simple hardening steps #TODO: link to ansible . We'll configure our hostname too in order to reflect whatever the fqdn of our server is going to be. In my case this will be banks.local as it will just be an internal address. This is used by kubernetes later on. sudo hostnamectl set-hostname banks.local","title":"Basic housekeeping"},{"location":"01.infrastructure/01.hosts/01.banks/#setup-wireguard-vpn","text":"See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. I've gone with Wireguard over OpenVPN based on it being incorporated into the Linux Kernel and increased performance versus OpenVPN.","title":"Setup Wireguard (VPN)"},{"location":"01.infrastructure/01.hosts/01.banks/#install-wireguard","text":"sudo add-apt-repository -y ppa:wireguard/wireguard sudo apt-get install -y wireguard sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. root@banks:~# lsmod | grep wireguard wireguard 212992 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard","title":"Install Wireguard"},{"location":"01.infrastructure/01.hosts/01.banks/#keys","text":"You will need to generate a key-pair for every peer (device) that is connected, including things like mobile phones etc. The iOS WireGuard client allow you to generate the keys on the device itself (if you want). # Generate public/private keypair cd /etc/wireguard umask 077 wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey","title":"Keys"},{"location":"01.infrastructure/01.hosts/01.banks/#configure","text":"We need to create a network interface now for the wireguard VPN. Common convention is to use wg0 as a name for this. In addition we also need to choose a subnet for the VPN addresses. As I've got a 192.168.0.1/24 configuration at home I'll use 10.10.0.1/24 for the VPN. Note the highlighted IP address we assign to each node here. It will need to be incremented for each to provide a unique address. # file: /etc/wireguard/wg0.conf [ Interface ] PrivateKey = {{ PRIVATE_KEY }} Address = 10 .10.0.1/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT ; iptables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -A FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT ; iptables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -D FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ListenPort = 51820 Now to keepo things DRY we'll run the following to replace the placeholder text with the actual contents of our server's private key we generated earlier. sudo sed -i.bak 's/{{PRIVATE_KEY}}/' $( sudo cat /etc/wireguard/privatekey ) '/' /etc/wireguard/wg0.conf We also need to replace the {{ETH0}} placeholder with the name of our existing primary network interface. A quick one-liner for this is ip -4 route | grep default | awk '{print $5}' which, on my server, gives bond0 as the answer (as I'm running LACP across multiple bonded physical interfaces). sudo sed -i.bak 's/{{ETH0}}/' $( ip -4 route | grep default | awk '{print $5}' ) '/g' /etc/wireguard/wg0.conf Enable forwarding of packets in the host kernel. sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p Finally we can start the wg0 interface. wg-quick up wg0 Hopefully you'll see something like the below output. root@banks:/etc/wireguard# wg-quick up wg0 [ #] ip link add wg0 type wireguard [ #] wg setconf wg0 /dev/fd/63 [ #] ip -4 address add 10.10.0.1/24 dev wg0 [ #] ip -6 address add fd86:ea04:1111::1/64 dev wg0 [ #] ip link set mtu 1420 up dev wg0 [ #] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o bond0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o bond0 -j MASQUERADE Checking status The check the status of wireguard run the wg command. root@banks:/etc/wireguard# wg interface: wg0 public key: I6ZHsLe44SHNH44xE86AI0VEnm8CfzrQUrxSCJVjAEw = private key: ( hidden ) listening port: 51820 In addition, we should now have an additional route appear for our VPN subnet. root@banks:/etc/wireguard# ip route default via 192 .168.0.1 dev bond0 proto dhcp src 192 .168.0.94 metric 100 10 .10.0.0/24 dev wg0 proto kernel scope link src 10 .10.0.1 172 .17.0.0/16 dev docker0 proto kernel scope link src 172 .17.0.1 linkdown ...","title":"Configure"},{"location":"01.infrastructure/01.hosts/01.banks/#systemd-service","text":"Assuming the above works we can now enable the wg0 interface on boot. sudo systemctl enable wg-quick@wg0.service sudo systemctl daemon-reload You can then manually start sudo service wg-quick@wg0 start and check status service wg-quick@wg0 status of the service.","title":"Systemd service"},{"location":"01.infrastructure/01.hosts/01.banks/#enable-avahi-discovery-on-vpn-and-lan","text":"As per Wikipedia . Avahi is a free zero-configuration networking (zeroconf) implementation, including a system for multicast DNS/DNS-SD service discovery. It is licensed under the GNU Lesser General Public License (LGPL). Avahi is a system which enables programs to publish and discover services and hosts running on a local network. For example, a user can plug a computer into a network and have Avahi automatically advertise the network services running on its machine, facilitating user access to those services. We will setup our nodes to publish themselves on both the LAN (so can be accessed via their hostnames) and VPN. export DEBIAN_FRONTEND = noninteractive # from docker sudo apt-get update -y sudo apt-get -qq install -y avahi-daemon avahi-utils The main configuration is held in /etc/avahi/avahi-daemon.conf . We want to modify the following line (to limit which interfaces we advertise on). This will be useful for when we want to only enable it on internal interfaces (e.g. our Cloud node shouldn't try and broadcast across the in) #file: /etc/avahi/avahi-daemon.conf allow-interfaces = bond0, wg0, docker0 # If we're using an internal domain then leave as `local` below, else change to tld domain-name = local Reload / restart the daemon with sudo systemctl daemon-reload && sudo systemctl restart avahi-daemon.service","title":"Enable Avahi (Discovery) on VPN and LAN"},{"location":"01.infrastructure/01.hosts/02.clarke/","text":"Clarke (storage node) I've got an old self-built frankenstein storage server that runs a lot of disks in it. Again, I've installed Ubuntu 18.04 LTS on it and will add as a node to the cluster to surface up file/block/object storage to workloads. Setup storage As I'll be using Ceph I'll leave all the disks as standalone (no RAID) and perform fulldisk encryption such that they have to be unlocked if removed from the server. Finding the relevant disks is done with lsblk . example lsblk adminlocal@banks:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID } Auto-mount encrypted devices at boot We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Storage (Clarke)"},{"location":"01.infrastructure/01.hosts/02.clarke/#clarke-storage-node","text":"I've got an old self-built frankenstein storage server that runs a lot of disks in it. Again, I've installed Ubuntu 18.04 LTS on it and will add as a node to the cluster to surface up file/block/object storage to workloads.","title":"Clarke (storage node)"},{"location":"01.infrastructure/01.hosts/02.clarke/#setup-storage","text":"As I'll be using Ceph I'll leave all the disks as standalone (no RAID) and perform fulldisk encryption such that they have to be unlocked if removed from the server. Finding the relevant disks is done with lsblk . example lsblk adminlocal@banks:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID }","title":"Setup storage"},{"location":"01.infrastructure/01.hosts/02.clarke/#auto-mount-encrypted-devices-at-boot","text":"We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Auto-mount encrypted devices at boot"},{"location":"01.infrastructure/01.hosts/03.donaldson/","text":"","title":"Compute (Donaldson)"},{"location":"01.infrastructure/01.hosts/04.donaldson/","text":"For the Kubernetes ingress node I'm going to use a cheap (<$5 per month) dedicated server from Kimsufi which has unmetered traffic and 100mb bandwidth. This will allow me to have a static IP4 address to point my DNS records at whikst then using the internal wireguard VPN to route traffic from here back home / elsewhere.","title":"04.donaldson"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/","text":"Configuring Kubernetes In part 1 we setup and configured the physical nodes we are going to use for our kubernetes cluster, ensuring they could communicate between each other using a secure VPN. We'll now: install kubernetes implement a CNI (network solution for pod communication) add a user and give them the ability to run commands on the cluster Install Kubernetes Docker As Kubernetes is an orchestration layer we will need to install a container runtime eninge. The below commands will install a compatible version of Docker. Kubernetes and Docker versions It's worth being aware that Kubernetes only supports specific versions of docker and, as a result, you should check for compatability in their changelog. At time of writing the latest stable version of Kubernetes was 1.16. The CHANGELOG-1.16 shows that they have validated the following docker versions: The list of validated docker versions remains unchanged. The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09 export K8S_DOCKER_VERSION = 18 .09 sudo apt-get update sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" sudo apt-get update && sudo apt-get install -y docker-ce = $( apt-cache madison docker-ce | grep ${ K8S_DOCKER_VERSION } | head -1 | awk '{print $3}' ) Kubernetes With docker now setup as a runtime we'll install Kubernetes. sudo apt-get update && sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo tee /etc/apt/sources.list.d/kubernetes.list <<EOF deb http://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl Pre-flight fixes Before intialising the cluster we need to change the following: modify the cgroup driver from cgroupfs to systemd disable swap # Setup daemon. sudo tee /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo mkdir -p /etc/systemd/system/docker.service.d # Restart docker. sudo systemctl daemon-reload sudo systemctl restart docker The swap should be disabled both for the current session with sudo swapoff -a and then in /etc/fstab (just add a comment # at the start of the line) so that this persists across reboots. Initialise Initialise the node. We'll use our VPN address here as the address to listen on. # Bind to VPN address sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) You'll see a load of scrolling log text followed by the following indicating success and giving some next step instructions. Success Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10 .10.0.1:6443 --token 64we2d.5jzsjzpa0ysqzagl \\ --discovery-token-ca-cert-hash sha256:9d1dda6163e0e539588e0209f06b37d209d230a373b0167ea5f881cd60537178 Give your user the ability to run both kubectl and sudo. mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config echo \" $USER ALL=(ALL:ALL) NOPASSWD:ALL\" | sudo tee -a /etc/sudoers > /dev/null To test this works we can run a couple of kubernetes commands. $ kubectl get nodes NAME STATUS ROLES AGE VERSION banks.local NotReady master 5m29s v1.16.3 The node will show as NotReady until we complete the following below. Navigating to the addons link provided above will show that we've got some options available. First we will focus on the Networking and Network Policy where we will use Canal as a CNI (networking solution between pods). Canal unites Flannel and Calico, providing networking and network policy. # Role-based access control (RBAC) # Kubernetes API datastore with flannel networking: # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/integration#role-based-access-control-rbac kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-flannel.yaml # Installing Calico for policy and flannel for networking # Installing with the Kubernetes API datastore (recommended) # We can install directly as we're using the pod CIDR 10.244.0.0/16 # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/canal.yaml Remove the Control plane node isolation taint The official docs highlight: By default, your cluster will not schedule pods on the control-plane node for security reasons. This will be a problem if we only have one node running (e.g. homelab development) so we will remove the taint that prevents this. kubectl taint nodes --all node-role.kubernetes.io/master- This will remove the node-role.kubernetes.io/master taint from any nodes that have it, including the control-plane node, meaning that the scheduler will then be able to schedule pods everywhere. $ kubectl get nodes NAME STATUS ROLES AGE VERSION banks.local Ready master 5m31s v1.16.3 Wiping your Kubernetes installation On the off-chance that you ever want to completely uninstall kubernetes and associated resources you can run the commands below to purge from the system. kubeadm reset ; \\ sudo apt-get purge kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get autoremove ; \\ rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd * off-chance being the polite way of saying \"this happens quite a lot when learning\"... Testing a Kubernetes application deployment Whilst the below is optional it's highly recommended to quickly test that our initial Kubernetes installation has been successful. Get a list of available nodes adminlocal@banks:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION banks Ready master 43m v1.16.3 Run a single pod deployment and expose it kubectl run helloworld --image = containous/whoami --port = 80 kubectl expose deployment helloworld --type = NodePort We'll get an output now which shows the port mappings adminlocal@banks:~$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld NodePort 10 .98.3.69 <none> 8080 :31491/TCP 14s kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 58m And quickly check the status of the pod adminlocal@banks:~$ kubectl get pods NAME READY STATUS RESTARTS AGE helloworld-68f956cfd8-dxdfb 1 /1 Running 0 23m Pod stuck on 'Pending' If your pod, per the above, is stuck on pending for a long period of time check whether you've removed the the control plane node taint . Without this, the scheduler will look for somewhere to run the pod but will fail if you've only got one physical node. adminlocal@banks:~$ kubectl get pods NAME READY STATUS RESTARTS AGE helloworld-68f956cfd8-dxdfb 0 /1 Pending 0 15m As we've used NodePort above our internal port of 8080 for the application has been mapped to port 31491 on the node automatically by the scheduler. A description of the service yields soime useful bits of information. adminlocal@banks:~$ kubectl describe services helloworld Name: helloworld Namespace: default Labels: run = helloworld Annotations: <none> Selector: run = helloworld Type: NodePort IP: 10 .98.3.69 Port: <unset> 8080 /TCP TargetPort: 8080 /TCP NodePort: <unset> 31491 /TCP Endpoints: 10 .244.0.4:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> As detailed in Use a Service to Access an Application in a Cluster we can list the pods that are running this application by using the selector highlighted above. adminlocal@banks:~$ kubectl get pods --selector = \"run=helloworld\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES helloworld-68f956cfd8-dxdfb 1 /1 Running 0 60m 10 .244.0.4 banks <none> <none> Now to find the (available only on our internal network for the moment) IP we need to run kubectl describe node banks . This returns a huge amount of information but the most important part being InternalIP . ... Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Mon, 18 Nov 2019 22 :56:21 +0000 Mon, 18 Nov 2019 20 :55:12 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Mon, 18 Nov 2019 22 :56:21 +0000 Mon, 18 Nov 2019 20 :55:12 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Mon, 18 Nov 2019 22 :56:21 +0000 Mon, 18 Nov 2019 20 :55:12 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Mon, 18 Nov 2019 22 :56:21 +0000 Mon, 18 Nov 2019 20 :56:17 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192 .168.0.95 Hostname: banks Capacity: cpu: 16 ephemeral-storage: 114295980Ki hugepages-2Mi: 0 ... Finally we can navigate to http://192.168.0.95:31491 and see a response: adminlocal@banks:~$ curl http://192.168.0.95:31491 CLIENT VALUES: client_address = 192 .168.0.95 command = GET real path = / query = nil request_version = 1 .1 request_uri = http://192.168.0.95:8080/ SERVER VALUES: server_version = nginx: 1 .10.0 - lua: 10001 HEADERS RECEIVED: accept = */* host = 192 .168.0.95:31491 user-agent = curl/7.58.0 BODY: -no body in request- And let's teardown the application deployment and service. kubectl delete services helloworld kubectl delete deployment helloworld","title":"Kubernetes Configuration"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#configuring-kubernetes","text":"In part 1 we setup and configured the physical nodes we are going to use for our kubernetes cluster, ensuring they could communicate between each other using a secure VPN. We'll now: install kubernetes implement a CNI (network solution for pod communication) add a user and give them the ability to run commands on the cluster","title":"Configuring Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#install-kubernetes","text":"","title":"Install Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#docker","text":"As Kubernetes is an orchestration layer we will need to install a container runtime eninge. The below commands will install a compatible version of Docker. Kubernetes and Docker versions It's worth being aware that Kubernetes only supports specific versions of docker and, as a result, you should check for compatability in their changelog. At time of writing the latest stable version of Kubernetes was 1.16. The CHANGELOG-1.16 shows that they have validated the following docker versions: The list of validated docker versions remains unchanged. The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09 export K8S_DOCKER_VERSION = 18 .09 sudo apt-get update sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" sudo apt-get update && sudo apt-get install -y docker-ce = $( apt-cache madison docker-ce | grep ${ K8S_DOCKER_VERSION } | head -1 | awk '{print $3}' )","title":"Docker"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#kubernetes","text":"With docker now setup as a runtime we'll install Kubernetes. sudo apt-get update && sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo tee /etc/apt/sources.list.d/kubernetes.list <<EOF deb http://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl","title":"Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#pre-flight-fixes","text":"Before intialising the cluster we need to change the following: modify the cgroup driver from cgroupfs to systemd disable swap # Setup daemon. sudo tee /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo mkdir -p /etc/systemd/system/docker.service.d # Restart docker. sudo systemctl daemon-reload sudo systemctl restart docker The swap should be disabled both for the current session with sudo swapoff -a and then in /etc/fstab (just add a comment # at the start of the line) so that this persists across reboots.","title":"Pre-flight fixes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#initialise","text":"Initialise the node. We'll use our VPN address here as the address to listen on. # Bind to VPN address sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) You'll see a load of scrolling log text followed by the following indicating success and giving some next step instructions. Success Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10 .10.0.1:6443 --token 64we2d.5jzsjzpa0ysqzagl \\ --discovery-token-ca-cert-hash sha256:9d1dda6163e0e539588e0209f06b37d209d230a373b0167ea5f881cd60537178 Give your user the ability to run both kubectl and sudo. mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config echo \" $USER ALL=(ALL:ALL) NOPASSWD:ALL\" | sudo tee -a /etc/sudoers > /dev/null To test this works we can run a couple of kubernetes commands. $ kubectl get nodes NAME STATUS ROLES AGE VERSION banks.local NotReady master 5m29s v1.16.3 The node will show as NotReady until we complete the following below. Navigating to the addons link provided above will show that we've got some options available. First we will focus on the Networking and Network Policy where we will use Canal as a CNI (networking solution between pods). Canal unites Flannel and Calico, providing networking and network policy. # Role-based access control (RBAC) # Kubernetes API datastore with flannel networking: # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/integration#role-based-access-control-rbac kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-flannel.yaml # Installing Calico for policy and flannel for networking # Installing with the Kubernetes API datastore (recommended) # We can install directly as we're using the pod CIDR 10.244.0.0/16 # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/canal.yaml","title":"Initialise"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#remove-the-control-plane-node-isolation-taint","text":"The official docs highlight: By default, your cluster will not schedule pods on the control-plane node for security reasons. This will be a problem if we only have one node running (e.g. homelab development) so we will remove the taint that prevents this. kubectl taint nodes --all node-role.kubernetes.io/master- This will remove the node-role.kubernetes.io/master taint from any nodes that have it, including the control-plane node, meaning that the scheduler will then be able to schedule pods everywhere. $ kubectl get nodes NAME STATUS ROLES AGE VERSION banks.local Ready master 5m31s v1.16.3 Wiping your Kubernetes installation On the off-chance that you ever want to completely uninstall kubernetes and associated resources you can run the commands below to purge from the system. kubeadm reset ; \\ sudo apt-get purge kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get autoremove ; \\ rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd * off-chance being the polite way of saying \"this happens quite a lot when learning\"... Testing a Kubernetes application deployment Whilst the below is optional it's highly recommended to quickly test that our initial Kubernetes installation has been successful. Get a list of available nodes adminlocal@banks:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION banks Ready master 43m v1.16.3 Run a single pod deployment and expose it kubectl run helloworld --image = containous/whoami --port = 80 kubectl expose deployment helloworld --type = NodePort We'll get an output now which shows the port mappings adminlocal@banks:~$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld NodePort 10 .98.3.69 <none> 8080 :31491/TCP 14s kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 58m And quickly check the status of the pod adminlocal@banks:~$ kubectl get pods NAME READY STATUS RESTARTS AGE helloworld-68f956cfd8-dxdfb 1 /1 Running 0 23m Pod stuck on 'Pending' If your pod, per the above, is stuck on pending for a long period of time check whether you've removed the the control plane node taint . Without this, the scheduler will look for somewhere to run the pod but will fail if you've only got one physical node. adminlocal@banks:~$ kubectl get pods NAME READY STATUS RESTARTS AGE helloworld-68f956cfd8-dxdfb 0 /1 Pending 0 15m As we've used NodePort above our internal port of 8080 for the application has been mapped to port 31491 on the node automatically by the scheduler. A description of the service yields soime useful bits of information. adminlocal@banks:~$ kubectl describe services helloworld Name: helloworld Namespace: default Labels: run = helloworld Annotations: <none> Selector: run = helloworld Type: NodePort IP: 10 .98.3.69 Port: <unset> 8080 /TCP TargetPort: 8080 /TCP NodePort: <unset> 31491 /TCP Endpoints: 10 .244.0.4:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> As detailed in Use a Service to Access an Application in a Cluster we can list the pods that are running this application by using the selector highlighted above. adminlocal@banks:~$ kubectl get pods --selector = \"run=helloworld\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES helloworld-68f956cfd8-dxdfb 1 /1 Running 0 60m 10 .244.0.4 banks <none> <none> Now to find the (available only on our internal network for the moment) IP we need to run kubectl describe node banks . This returns a huge amount of information but the most important part being InternalIP . ... Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Mon, 18 Nov 2019 22 :56:21 +0000 Mon, 18 Nov 2019 20 :55:12 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Mon, 18 Nov 2019 22 :56:21 +0000 Mon, 18 Nov 2019 20 :55:12 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Mon, 18 Nov 2019 22 :56:21 +0000 Mon, 18 Nov 2019 20 :55:12 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Mon, 18 Nov 2019 22 :56:21 +0000 Mon, 18 Nov 2019 20 :56:17 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192 .168.0.95 Hostname: banks Capacity: cpu: 16 ephemeral-storage: 114295980Ki hugepages-2Mi: 0 ... Finally we can navigate to http://192.168.0.95:31491 and see a response: adminlocal@banks:~$ curl http://192.168.0.95:31491 CLIENT VALUES: client_address = 192 .168.0.95 command = GET real path = / query = nil request_version = 1 .1 request_uri = http://192.168.0.95:8080/ SERVER VALUES: server_version = nginx: 1 .10.0 - lua: 10001 HEADERS RECEIVED: accept = */* host = 192 .168.0.95:31491 user-agent = curl/7.58.0 BODY: -no body in request- And let's teardown the application deployment and service. kubectl delete services helloworld kubectl delete deployment helloworld","title":"Remove the Control plane node isolation taint"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/","text":"In part 2 we installed Kubernetes and setup a user (in my case adminlocal ) on our cluster with the ability to run administrative kubernetes commands. etcd (key/value store), Rook, Promethues and Vault are all examples of technologies we will be using in our cluster and are deployed using Kubernetes Operators . In this section we'll be deploying the Rook storage orchestrator with Ceph as a storage provider. Components Rook Rook allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider. More information on the architecure can be found in the docs Ceph Ceph provides three types of storage: object : compatible with S3 API file : files and directories (incl. NFS); and block : replicate a hard drive There a 4 key components of the architecture to be aware of (shown above in the Rook diagram): Monitor (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication Manager daemon: keeps track of state in cluster and metrics OSDs (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage. MSDs (Metatdata Server): concerned with filesystem storage type only Under the hood everything is stored as an object in logical storage pools. Installation and Setup Deployment of the Rook operator See the Rook quickstart We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple mon components onto a single node. Similar to when we removed the the control plane node taint Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes). # create a working directory mkdir -p ~/rook && \\ cd ~/rook # download the sample files # all of these can be found here: https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/common.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/operator.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/cluster.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/toolbox.yaml # modify the cluster spec # - allow multiple mons per node sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml In addition, because we've setup our encrypted data storage to be mounted at /data/${BLKID} we will edit the default storage options to remove the useAllDevices selection and, instead, specify the directories. See the docs for more details. # set default storage location and remove default `useAllDevices: true` sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml # any devices starting with 'sd' (but not sda as that's our root filesystem) sed -i.bak 's/deviceFilter:/deviceFilter: ^sd[^a]/' cluster.yaml # encrypt them with LUKS # see conversation https://github.com/rook/rook/issues/923#issuecomment-557651052 sed -i.bak 's/# encryptedDevice: \"true\"/encryptedDevice: \"true\"/' cluster.yaml With these configurations now downloaded we'll apply them in the following order. kubectl create -f ~/rook/common.yaml ; \\ kubectl create -f ~/rook/operator.yaml Verify the rook-ceph-operator is in the Running state Use kubectl -n rook-ceph get pod to check we have a running state. root@banks:~# kubectl -n rook-ceph get pod NAME READY STATUS RESTARTS AGE ... rook-ceph-operator-c8ff6447d-tbh5c 1 /1 Running 0 6m18s Create the Rook cluster Assuming the operator looks ok we can now create the cluster kubectl create -f ~/rook/cluster.yaml To verify the state of the cluster we will connect to the Rook Toolbox kubectl create -f ~/rook/toolbox.yaml Wait for the toolbox pod to enter a running state: kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" Once the rook-ceph-tools pod is running, you can connect to it with: kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash When inside the toolbox run ceph status . ceph status [ root@banks / ] # ceph status cluster: id: 06da5ebc-d2f3-4366-a51c-db759d8bc664 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c ( age 2m ) mgr: a ( active, since 102s ) osd: 2 osds: 2 up ( since 33s ) , 2 in ( since 33s ) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2 .0 GiB used, 3 .6 TiB / 3 .6 TiB avail pgs: All mons should be in quorum A mgr should be active At least one OSD should be active If the health is not HEALTH_OK, the warnings or errors should be investigated Troubleshooting: [errno 2] NB: You might get an error unable to get monitor info from DNS SRV with service name: ceph-mon or [errno 2] error connecting to the cluster when running ceph status in the toolbox if you've typed all of the above commands very quickly. This is usually because the cluster is still starting and waiting for all the monitors to come up and establish connections. Go get a cup of tea / wait a couple of minutes and try again. In the cluster.yaml spec the default number of mon instances is 3 . As a result if you don't have three of these pods running then your cluster is still initialising. You can run kubectl -n rook-ceph logs -l \"app=rook-ceph-operator\" to see an output of the logs from the operator and search for mons running . As you can see below it took mine around a minute to initialise all 3. To see what monitors you have run kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" . $ kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" NAME READY STATUS RESTARTS AGE rook-ceph-mon-a-5d677b5849-t4xct 1 /1 Running 0 82s rook-ceph-mon-b-6cfbcf8db4-7cwxp 1 /1 Running 0 66s rook-ceph-mon-c-8f858c585-c9z5b 1 /1 Running 0 50s When you are done with the toolbox, you can remove the deployment: kubectl -n rook-ceph delete deployment rook-ceph-tools If you want to delete the cluster and start again... Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse. There are some additional pointers here in the docs. kubectl delete -f toolbox.yaml ; \\ kubectl delete -f cluster.yaml ; \\ kubectl delete -f operator.yaml ; \\ kubectl delete -f common.yaml ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/* Dashboard and Storage We now have a cluster running but no configured storage or an ability to review status (other than logging into the toolbox).","title":"Install the Cluster"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/#components","text":"","title":"Components"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/#rook","text":"Rook allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider. More information on the architecure can be found in the docs","title":"Rook"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/#ceph","text":"Ceph provides three types of storage: object : compatible with S3 API file : files and directories (incl. NFS); and block : replicate a hard drive There a 4 key components of the architecture to be aware of (shown above in the Rook diagram): Monitor (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication Manager daemon: keeps track of state in cluster and metrics OSDs (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage. MSDs (Metatdata Server): concerned with filesystem storage type only Under the hood everything is stored as an object in logical storage pools.","title":"Ceph"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/#deployment-of-the-rook-operator","text":"See the Rook quickstart We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple mon components onto a single node. Similar to when we removed the the control plane node taint Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes). # create a working directory mkdir -p ~/rook && \\ cd ~/rook # download the sample files # all of these can be found here: https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/common.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/operator.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/cluster.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/toolbox.yaml # modify the cluster spec # - allow multiple mons per node sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml In addition, because we've setup our encrypted data storage to be mounted at /data/${BLKID} we will edit the default storage options to remove the useAllDevices selection and, instead, specify the directories. See the docs for more details. # set default storage location and remove default `useAllDevices: true` sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml # any devices starting with 'sd' (but not sda as that's our root filesystem) sed -i.bak 's/deviceFilter:/deviceFilter: ^sd[^a]/' cluster.yaml # encrypt them with LUKS # see conversation https://github.com/rook/rook/issues/923#issuecomment-557651052 sed -i.bak 's/# encryptedDevice: \"true\"/encryptedDevice: \"true\"/' cluster.yaml With these configurations now downloaded we'll apply them in the following order. kubectl create -f ~/rook/common.yaml ; \\ kubectl create -f ~/rook/operator.yaml Verify the rook-ceph-operator is in the Running state Use kubectl -n rook-ceph get pod to check we have a running state. root@banks:~# kubectl -n rook-ceph get pod NAME READY STATUS RESTARTS AGE ... rook-ceph-operator-c8ff6447d-tbh5c 1 /1 Running 0 6m18s","title":"Deployment of the Rook operator"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/#create-the-rook-cluster","text":"Assuming the operator looks ok we can now create the cluster kubectl create -f ~/rook/cluster.yaml To verify the state of the cluster we will connect to the Rook Toolbox kubectl create -f ~/rook/toolbox.yaml Wait for the toolbox pod to enter a running state: kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" Once the rook-ceph-tools pod is running, you can connect to it with: kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash When inside the toolbox run ceph status . ceph status [ root@banks / ] # ceph status cluster: id: 06da5ebc-d2f3-4366-a51c-db759d8bc664 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c ( age 2m ) mgr: a ( active, since 102s ) osd: 2 osds: 2 up ( since 33s ) , 2 in ( since 33s ) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2 .0 GiB used, 3 .6 TiB / 3 .6 TiB avail pgs: All mons should be in quorum A mgr should be active At least one OSD should be active If the health is not HEALTH_OK, the warnings or errors should be investigated","title":"Create the Rook cluster"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/#troubleshooting-errno-2","text":"NB: You might get an error unable to get monitor info from DNS SRV with service name: ceph-mon or [errno 2] error connecting to the cluster when running ceph status in the toolbox if you've typed all of the above commands very quickly. This is usually because the cluster is still starting and waiting for all the monitors to come up and establish connections. Go get a cup of tea / wait a couple of minutes and try again. In the cluster.yaml spec the default number of mon instances is 3 . As a result if you don't have three of these pods running then your cluster is still initialising. You can run kubectl -n rook-ceph logs -l \"app=rook-ceph-operator\" to see an output of the logs from the operator and search for mons running . As you can see below it took mine around a minute to initialise all 3. To see what monitors you have run kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" . $ kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" NAME READY STATUS RESTARTS AGE rook-ceph-mon-a-5d677b5849-t4xct 1 /1 Running 0 82s rook-ceph-mon-b-6cfbcf8db4-7cwxp 1 /1 Running 0 66s rook-ceph-mon-c-8f858c585-c9z5b 1 /1 Running 0 50s When you are done with the toolbox, you can remove the deployment: kubectl -n rook-ceph delete deployment rook-ceph-tools If you want to delete the cluster and start again... Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse. There are some additional pointers here in the docs. kubectl delete -f toolbox.yaml ; \\ kubectl delete -f cluster.yaml ; \\ kubectl delete -f operator.yaml ; \\ kubectl delete -f common.yaml ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Troubleshooting: [errno 2]"},{"location":"01.infrastructure/03.storage/00.setup.ceph.storage.with.rook/#dashboard-and-storage","text":"We now have a cluster running but no configured storage or an ability to review status (other than logging into the toolbox).","title":"Dashboard and Storage"},{"location":"01.infrastructure/03.storage/01.dashboard/","text":"Ceph comes with it's own dashboard for management purposes. Let's check the dashboard status. For full details see the docs root@banks:~/rook# kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-cephfsplugin-metrics ClusterIP 10 .107.29.251 <none> 8080 /TCP,8081/TCP 14m csi-rbdplugin-metrics ClusterIP 10 .102.151.196 <none> 8080 /TCP,8081/TCP 14m rook-ceph-mgr ClusterIP 10 .110.58.77 <none> 9283 /TCP 12m rook-ceph-mgr-dashboard ClusterIP 10 .104.71.31 <none> 8443 /TCP 12m rook-ceph-mon-a ClusterIP 10 .101.139.217 <none> 6789 /TCP,3300/TCP 13m rook-ceph-mon-b ClusterIP 10 .109.239.104 <none> 6789 /TCP,3300/TCP 13m rook-ceph-mon-c ClusterIP 10 .99.14.79 <none> 6789 /TCP,3300/TCP 13m So it looks like our rook-ceph-mgr-dashboard is running on port 8843 . Describing the service indicates it's not yet exposed via a NodePort etc. so we can't access it external to the cluster. root@banks:~/rook# kubectl -n rook-ceph describe services rook-ceph-mgr-dashboard Name: rook-ceph-mgr-dashboard Namespace: rook-ceph Labels: app = rook-ceph-mgr rook_cluster = rook-ceph Annotations: <none> Selector: app = rook-ceph-mgr,rook_cluster = rook-ceph Type: ClusterIP IP: 10 .104.71.31 Port: https-dashboard 8443 /TCP TargetPort: 8443 /TCP Endpoints: 10 .244.0.80:8443 Session Affinity: None Events: <none> We'll enable this via a NodePort now. cd ~/rook wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml kubectl create -f ~/rook/dashboard-external-https.yaml You will see the new service rook-ceph-mgr-dashboard-external-https created: $ kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-cephfsplugin-metrics ClusterIP 10 .102.185.98 <none> 8080 /TCP,8081/TCP 18h csi-rbdplugin-metrics ClusterIP 10 .108.114.119 <none> 8080 /TCP,8081/TCP 18h rook-ceph-mgr ClusterIP 10 .107.152.33 <none> 9283 /TCP 18h rook-ceph-mgr-dashboard ClusterIP 10 .105.220.219 <none> 8443 /TCP 18h rook-ceph-mgr-dashboard-external-https NodePort 10 .98.207.115 <none> 8443 :30995/TCP 9s rook-ceph-mon-a ClusterIP 10 .101.90.100 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-b ClusterIP 10 .107.14.178 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-c ClusterIP 10 .101.91.134 <none> 6789 /TCP,3300/TCP 18h In this example, port 30995 will be opened to expose port 8443 from the ceph-mgr pod . Find the ip address of the node or, if you've used Avahi as per the physcial node installation you could probably access the node directly using the hostname (Kubernetes will listen on that fqdn). Finding the IP address So this isn't that obvious... We've exposed our service via a NodePort but now need to find the IP address of the actual node in order to hit the required port to see the service... $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-mgr\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-mgr-a-59cc7fb98-7wfxf 1 /1 Running 0 19h 10 .244.0.112 banks <none> <none> Our service is running on the banks node. Let's find the IP of that then. $ kubectl describe node banks | grep InternalIP InternalIP: 192 .168.0.95 Or you could find the IP address with: kubectl get node banks -o jsonpath = '{.status.addresses[0].address}' Now you can enter the URL in your browser as https://192.168.0.95:30995 or https://banks.local:30995 and the dashboard will appear. Credentials As described in the docs Rook creates a default user named admin and generates a secret called rook-ceph-dashboard-admin-password in the namespace where rook is running. To retrieve the generated password, you can run the following: kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo This is a good thing to see...","title":"Setup Dashboard"},{"location":"01.infrastructure/03.storage/01.dashboard/#credentials","text":"As described in the docs Rook creates a default user named admin and generates a secret called rook-ceph-dashboard-admin-password in the namespace where rook is running. To retrieve the generated password, you can run the following: kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo This is a good thing to see...","title":"Credentials"},{"location":"01.infrastructure/03.storage/02.storage/","text":"We now have Ceph running and managed via Rook on our Kubernetes cluster (with a nice graphical dashboard) but don't yet have any storage configured. There are some good documented examples we can walk through to get started.","title":"Storage"},{"location":"01.infrastructure/03.storage/0200.block/","text":"As per the docs : Block storage allows a single pod to mount storage We need to create both a StorageClass and a CephBlockPool in order to use black storage on our cluster. mkdir -p ~/rook/storage cd ~/rook/storage wget -O \"storageclass-rbd.yaml\" https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' storageclass-rbd.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' storageclass-rbd.yaml ; \\ kubectl create -f ~/rook/storage/storageclass-rbd.yaml Test this new storage out with a Wordpress installation (including MySQL) which requires the usage of Volume and PersistentVolumeClaim . mkdir -p ~/rook/examples cd ~/rook/examples wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/wordpress.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/mysql.yaml kubectl create -f ~/rook/examples/mysql.yaml ; \\ kubectl create -f ~/rook/examples/wordpress.yaml To review the volumes that have been created run kubectl get pvc $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-29aa4aba-4029-487d-8e7e-b2eb08400382 20Gi RWO rook-ceph-block 74s wp-pv-claim Bound pvc-7251e045-a174-49e1-9393-fbd8dbcfeaa6 20Gi RWO rook-ceph-block 73s Those PVCs can also be seen in the ceph dashboard under Block >> Images Once the pods for Wordpress and MySQL are running get the cluster IP for the wordpress app and navigate to it. You should be presented with the installation wizard. We'll tear this down now before proceeding (but leave the storage class for later usage). $ kubectl delete -f wordpress.yaml service \"wordpress\" deleted persistentvolumeclaim \"wp-pv-claim\" deleted deployment.apps \"wordpress\" deleted $ kubectl delete -f mysql.yaml service \"wordpress-mysql\" deleted persistentvolumeclaim \"mysql-pv-claim\" deleted deployment.apps \"wordpress-mysql\" deleted # kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool # kubectl delete storageclass rook-ceph-block","title":"Block"},{"location":"01.infrastructure/03.storage/0201.filesystem/","text":"As per the docs A shared file system can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared filesystem. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/filesystem.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' filesystem.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' filesystem.yaml ; \\ kubectl create -f ~/rook/storage/filesystem.yaml Wait for the mds pods to start adminlocal@banks:~/rook/storage$ kubectl -n rook-ceph get pod -l app = rook-ceph-mds NAME READY STATUS RESTARTS AGE rook-ceph-mds-myfs-a-84ccb448b5-9jbds 1 /1 Running 0 21s rook-ceph-mds-myfs-b-7d85c48b4c-72q9d 0 /1 Pending 0 20s NB: Because there is a podAntiAffinity spec in the filesystem.yaml placement section you may only see one pod running if we have a single node cluster. This is fine. Running a ceph status via the toolbox will reinforce this. # ceph status ... services: mon: 3 daemons, quorum a,b,c ( age 3d ) mgr: a ( active, since 3d ) mds: myfs:1 { 0 = myfs-a = up:active } osd: 2 osds: 2 up ( since 3d ) , 2 in ( since 3d ) rgw: 1 daemon active ( my.store.a ) ... As with other storage options, we need to create a StorageClass for a Filesystem. This will use the CSI Driver (which is the preferred driver going forward for K8s 1.13 and newer). cd ~/rook/storage wget -O \"storageclass-cephfs.yaml\" https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml kubectl create -f ~/rook/storage/storageclass-cephfs.yaml Consume the Shared File System: K8s Registry Sample We'll deploy a private docker registry that uses this shared filesystem via a PersistentVolumeClaim . cd ~/rook/ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml kubectl create -f ~/rook/kube-registry.yaml Configure registry See Github docs for further details. mkdir -p ~/registry cd ~/registry Now create a service.yaml file. # file: ~/registry/service.yaml apiVersion: v1 kind: Service metadata: name: kube-registry namespace: kube-system labels: k8s-app: kube-registry-upstream kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"KubeRegistry\" spec: selector: k8s-app: kube-registry ports: - name: registry port: 5000 protocol: TCP Apply this with kubectl create -f service.yaml . With the service created we'll use a DaemonSet to deploy a pod onto every node in the cluster (so that Dokcer sees it as localhost ). # file: ~/registry/daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-registry-proxy namespace: kube-system labels: k8s-app: kube-registry-proxy kubernetes.io/cluster-service: \"true\" version: v0.4 spec: selector: matchLabels: name: kube-registry template: metadata: labels: k8s-app: kube-registry-proxy kubernetes.io/name: \"kube-registry-proxy\" kubernetes.io/cluster-service: \"true\" version: v0.4 name: kube-registry spec: containers: - name: kube-registry-proxy image: gcr.io/google_containers/kube-registry-proxy:0.4 resources: limits: cpu: 100m memory: 50Mi env: - name: REGISTRY_HOST value: kube-registry.kube-system.svc.cluster.local - name: REGISTRY_PORT value: \"5000\" ports: - name: registry containerPort: 80 hostPort: 5000 Apply with a kubectl create -f ~/registry/daemonset.yaml and then check for completion of pods. $ kubectl -n kube-system get pod -l 'name=kube-registry' NAME READY STATUS RESTARTS AGE kube-registry-proxy-vtd56 1 /1 Running 0 5m34s We can check the registry has been deployed by running curl localhost:5000/image and expecting a 404 response. $ curl localhost:5000/image 404 page not found Push an image to the registry As per Docker docs we will push a small docker alpine image to our new local private repository. sudo docker pull alpine sudo docker images | grep alpine | grep latest sudo docker tag 965ea09ff2eb 127 .0.0.1:5000/alpine sudo docker push 127 .0.0.1:5000/alpine Mount the filesystem in toolbox to confirm # Create the directory mkdir /tmp/registry # Detect the mon endpoints and the user secret for the connection mon_endpoints = $( grep mon_host /etc/ceph/ceph.conf | awk '{print $3}' ) my_secret = $( grep key /etc/ceph/keyring | awk '{print $3}' ) # Mount the file system mount -t ceph -o mds_namespace = myfs,name = admin,secret = $my_secret $mon_endpoints :/ /tmp/registry # See your mounted file system df -h With the filesystem mounted we'll confirm there's an alpine repository now after our push above. # find /tmp/registry -name \"alpine\" /tmp/registry/volumes/csi/csi-vol-77b79f13-11ee-11ea-9848-7a3d11f24466/docker/registry/v2/repositories/alpine Teardown kubectl delete -f ~/registry/daemonset.yaml ; \\ kubectl delete -f ~/registry/service.yaml ; \\ kubectl delete -f ~/rook/kube-registry.yaml To delete the filesystem components and backing data, delete the Filesystem CRD. Warning: Data will be deleted kubectl -n rook-ceph delete cephfilesystem myfs","title":"Filesystem"},{"location":"01.infrastructure/03.storage/0201.filesystem/#consume-the-shared-file-system-k8s-registry-sample","text":"We'll deploy a private docker registry that uses this shared filesystem via a PersistentVolumeClaim . cd ~/rook/ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml kubectl create -f ~/rook/kube-registry.yaml","title":"Consume the Shared File System: K8s Registry Sample"},{"location":"01.infrastructure/03.storage/0201.filesystem/#configure-registry","text":"See Github docs for further details. mkdir -p ~/registry cd ~/registry Now create a service.yaml file. # file: ~/registry/service.yaml apiVersion: v1 kind: Service metadata: name: kube-registry namespace: kube-system labels: k8s-app: kube-registry-upstream kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"KubeRegistry\" spec: selector: k8s-app: kube-registry ports: - name: registry port: 5000 protocol: TCP Apply this with kubectl create -f service.yaml . With the service created we'll use a DaemonSet to deploy a pod onto every node in the cluster (so that Dokcer sees it as localhost ). # file: ~/registry/daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-registry-proxy namespace: kube-system labels: k8s-app: kube-registry-proxy kubernetes.io/cluster-service: \"true\" version: v0.4 spec: selector: matchLabels: name: kube-registry template: metadata: labels: k8s-app: kube-registry-proxy kubernetes.io/name: \"kube-registry-proxy\" kubernetes.io/cluster-service: \"true\" version: v0.4 name: kube-registry spec: containers: - name: kube-registry-proxy image: gcr.io/google_containers/kube-registry-proxy:0.4 resources: limits: cpu: 100m memory: 50Mi env: - name: REGISTRY_HOST value: kube-registry.kube-system.svc.cluster.local - name: REGISTRY_PORT value: \"5000\" ports: - name: registry containerPort: 80 hostPort: 5000 Apply with a kubectl create -f ~/registry/daemonset.yaml and then check for completion of pods. $ kubectl -n kube-system get pod -l 'name=kube-registry' NAME READY STATUS RESTARTS AGE kube-registry-proxy-vtd56 1 /1 Running 0 5m34s We can check the registry has been deployed by running curl localhost:5000/image and expecting a 404 response. $ curl localhost:5000/image 404 page not found","title":"Configure registry"},{"location":"01.infrastructure/03.storage/0201.filesystem/#push-an-image-to-the-registry","text":"As per Docker docs we will push a small docker alpine image to our new local private repository. sudo docker pull alpine sudo docker images | grep alpine | grep latest sudo docker tag 965ea09ff2eb 127 .0.0.1:5000/alpine sudo docker push 127 .0.0.1:5000/alpine","title":"Push an image to the registry"},{"location":"01.infrastructure/03.storage/0201.filesystem/#mount-the-filesystem-in-toolbox-to-confirm","text":"# Create the directory mkdir /tmp/registry # Detect the mon endpoints and the user secret for the connection mon_endpoints = $( grep mon_host /etc/ceph/ceph.conf | awk '{print $3}' ) my_secret = $( grep key /etc/ceph/keyring | awk '{print $3}' ) # Mount the file system mount -t ceph -o mds_namespace = myfs,name = admin,secret = $my_secret $mon_endpoints :/ /tmp/registry # See your mounted file system df -h With the filesystem mounted we'll confirm there's an alpine repository now after our push above. # find /tmp/registry -name \"alpine\" /tmp/registry/volumes/csi/csi-vol-77b79f13-11ee-11ea-9848-7a3d11f24466/docker/registry/v2/repositories/alpine","title":"Mount the filesystem in toolbox to confirm"},{"location":"01.infrastructure/03.storage/0201.filesystem/#teardown","text":"kubectl delete -f ~/registry/daemonset.yaml ; \\ kubectl delete -f ~/registry/service.yaml ; \\ kubectl delete -f ~/rook/kube-registry.yaml To delete the filesystem components and backing data, delete the Filesystem CRD. Warning: Data will be deleted kubectl -n rook-ceph delete cephfilesystem myfs","title":"Teardown"},{"location":"01.infrastructure/03.storage/0202.object/","text":"As per the docs Object storage exposes an S3 API to the storage cluster for applications to put and get data. We'll first create a CephObjectStore followed by a StorageClass for the bucket. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-ec.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' object-ec.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' object-ec.yaml ; \\ kubectl create -f ~/rook/storage/object-ec.yaml Check that the object store is configured and a rgw pod has started. $ kubectl -n rook-ceph get pod -l app = rook-ceph-rgw NAME READY STATUS RESTARTS AGE rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 27s Enable dashboard for the Object Gateway As per the docs we need to specifically enable access to the object gateway for it to be registered in the Ceph dashboard. # Connect to the toolbox first kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash # Create a system user radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system Make note of the keys # radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system { \"user_id\" : \"666\" , \"display_name\" : \"dashboard\" , \"email\" : \"\" , \"suspended\" : 0 , \"max_buckets\" : 1000 , \"subusers\" : [] , \"keys\" : [ { \"user\" : \"666\" , \"access_key\" : \"MUNSZSY7LF2E202MW1H6\" , \"secret_key\" : \"OF1za2LvibBpYjb6mw0umYDePfBkzfWSRNMeIwL0\" } ] , \"swift_keys\" : [] , \"caps\" : [] , \"op_mask\" : \"read, write, delete\" , \"system\" : \"true\" , \"default_placement\" : \"\" , \"default_storage_class\" : \"\" , \"placement_tags\" : [] , \"bucket_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"user_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"temp_url_keys\" : [] , \"type\" : \"rgw\" , \"mfa_ids\" : [] } Get the access_key and secret_access_key radosgw-admin user info --uid = 666 Now apply these credentials to the dashboard ceph dashboard set-rgw-api-access-key <access_key> ceph dashboard set-rgw-api-secret-key <secret_key> Set the host. You can get the service details with kubectl -n rook-ceph describe svc -l \"app=rook-ceph-rgw\" # use the format `service`.`namespace` as per docs # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ ceph dashboard set-rgw-api-host rook-ceph-rgw-my-store.rook-ceph ceph dashboard set-rgw-api-port 80 Create a bucket With an object store configured we can create a bucket. A bucket is created by defining a storage class and then registering an associated claim. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml kubectl create -f ~/rook/storage/storageclass-bucket-delete.yaml ; \\ kubectl create -f ~/rook/storage/object-bucket-claim-delete.yaml We should now see something like this when navigating on the dashboard to Object Gateway >> Buckets Enable external access Much like the Ceph Dashboard we want to expose the bucket to services that potentially live outside of the cluster. We'll create a new service for external access. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/rgw-external.yaml kubectl create -f rgw-external.yaml We should now have a service running listening on a NodePort $ kubectl get svc -n rook-ceph -l 'app=rook-ceph-rgw' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-ceph-rgw-my-store ClusterIP 10 .97.98.170 <none> 80 /TCP 95m rook-ceph-rgw-my-store-external NodePort 10 .105.1.131 <none> 80 :32039/TCP 30s Connecting to the bucket with a client As the API is S3 compatible we can connect to the bucket with a variety of tools. In order to to do we need to obtain the HOST , ACCESS_KEY and SECRET_ACCESS_KEY variables. # config-map, secret, OBC will part of default if no specific name space mentioned # NB: You need to use the `metadata: name` for the bucket as defined in the claim export AWS_HOST = $( kubectl -n default get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}' ) export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) The AWS_HOST should also match the details provided abvove as the set-rgw-api-host command for the dashboard. We'll install s3md on the host to check. sudo apt-get update && sudo apt-get install -y s3cmd We'll need to find the node that our service is running on: $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-rgw,rook_object_store=my-store\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 107m 10 .244.0.87 banks.local <none> <none> We'll set the AWS_HOST to banks.local and the NodePort in use (in this case 32039 ). export AWS_HOST = banks.local:32039 Check the buckets our use has access to. $ s3cmd ls --no-ssl --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -11-28 12 :33 s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 Now upload a new file to this bucket and download it again to confirm. # Create object echo \"Hello Rook\" > /tmp/rookObj # Upload s3cmd put /tmp/rookObj \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } \\ s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 # Download s3cmd get s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3/rookObj \\ /tmp/rookObj-download \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } Check the contents $ cat /tmp/rookObj-download Hello Rook $ md5sum /tmp/rookObj* dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj-download Teardown See Removing buckets in radosgw (and their contents) kubectl delete -f ~/rook/storage/object-bucket-claim-delete.yaml ; \\ kubectl delete -f ~/rook/storage/storageclass-bucket-delete.yaml And then, within the toolbox radosgw-admin bucket rm --bucket = ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 --purge-objects","title":"Object"},{"location":"01.infrastructure/03.storage/0202.object/#enable-dashboard-for-the-object-gateway","text":"As per the docs we need to specifically enable access to the object gateway for it to be registered in the Ceph dashboard. # Connect to the toolbox first kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash # Create a system user radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system Make note of the keys # radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system { \"user_id\" : \"666\" , \"display_name\" : \"dashboard\" , \"email\" : \"\" , \"suspended\" : 0 , \"max_buckets\" : 1000 , \"subusers\" : [] , \"keys\" : [ { \"user\" : \"666\" , \"access_key\" : \"MUNSZSY7LF2E202MW1H6\" , \"secret_key\" : \"OF1za2LvibBpYjb6mw0umYDePfBkzfWSRNMeIwL0\" } ] , \"swift_keys\" : [] , \"caps\" : [] , \"op_mask\" : \"read, write, delete\" , \"system\" : \"true\" , \"default_placement\" : \"\" , \"default_storage_class\" : \"\" , \"placement_tags\" : [] , \"bucket_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"user_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"temp_url_keys\" : [] , \"type\" : \"rgw\" , \"mfa_ids\" : [] } Get the access_key and secret_access_key radosgw-admin user info --uid = 666 Now apply these credentials to the dashboard ceph dashboard set-rgw-api-access-key <access_key> ceph dashboard set-rgw-api-secret-key <secret_key> Set the host. You can get the service details with kubectl -n rook-ceph describe svc -l \"app=rook-ceph-rgw\" # use the format `service`.`namespace` as per docs # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ ceph dashboard set-rgw-api-host rook-ceph-rgw-my-store.rook-ceph ceph dashboard set-rgw-api-port 80","title":"Enable dashboard for the Object Gateway"},{"location":"01.infrastructure/03.storage/0202.object/#create-a-bucket","text":"With an object store configured we can create a bucket. A bucket is created by defining a storage class and then registering an associated claim. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml kubectl create -f ~/rook/storage/storageclass-bucket-delete.yaml ; \\ kubectl create -f ~/rook/storage/object-bucket-claim-delete.yaml We should now see something like this when navigating on the dashboard to Object Gateway >> Buckets","title":"Create a bucket"},{"location":"01.infrastructure/03.storage/0202.object/#enable-external-access","text":"Much like the Ceph Dashboard we want to expose the bucket to services that potentially live outside of the cluster. We'll create a new service for external access. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/rgw-external.yaml kubectl create -f rgw-external.yaml We should now have a service running listening on a NodePort $ kubectl get svc -n rook-ceph -l 'app=rook-ceph-rgw' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-ceph-rgw-my-store ClusterIP 10 .97.98.170 <none> 80 /TCP 95m rook-ceph-rgw-my-store-external NodePort 10 .105.1.131 <none> 80 :32039/TCP 30s","title":"Enable external access"},{"location":"01.infrastructure/03.storage/0202.object/#connecting-to-the-bucket-with-a-client","text":"As the API is S3 compatible we can connect to the bucket with a variety of tools. In order to to do we need to obtain the HOST , ACCESS_KEY and SECRET_ACCESS_KEY variables. # config-map, secret, OBC will part of default if no specific name space mentioned # NB: You need to use the `metadata: name` for the bucket as defined in the claim export AWS_HOST = $( kubectl -n default get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}' ) export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) The AWS_HOST should also match the details provided abvove as the set-rgw-api-host command for the dashboard. We'll install s3md on the host to check. sudo apt-get update && sudo apt-get install -y s3cmd We'll need to find the node that our service is running on: $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-rgw,rook_object_store=my-store\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 107m 10 .244.0.87 banks.local <none> <none> We'll set the AWS_HOST to banks.local and the NodePort in use (in this case 32039 ). export AWS_HOST = banks.local:32039 Check the buckets our use has access to. $ s3cmd ls --no-ssl --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -11-28 12 :33 s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 Now upload a new file to this bucket and download it again to confirm. # Create object echo \"Hello Rook\" > /tmp/rookObj # Upload s3cmd put /tmp/rookObj \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } \\ s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 # Download s3cmd get s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3/rookObj \\ /tmp/rookObj-download \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } Check the contents $ cat /tmp/rookObj-download Hello Rook $ md5sum /tmp/rookObj* dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj-download","title":"Connecting to the bucket with a client"},{"location":"01.infrastructure/03.storage/0202.object/#teardown","text":"See Removing buckets in radosgw (and their contents) kubectl delete -f ~/rook/storage/object-bucket-claim-delete.yaml ; \\ kubectl delete -f ~/rook/storage/storageclass-bucket-delete.yaml And then, within the toolbox radosgw-admin bucket rm --bucket = ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 --purge-objects","title":"Teardown"},{"location":"01.infrastructure/04.monitoring/00.monitoring.with.prometheus.and.grafana/","text":"As we've got Ceph (via Rook) installed we'll initially setup Prometheus to monitor metrics and then display them in Deploy and configure Prometheus mkdir -p ~/monitoring cd monitoring/ export OPERATOR_VERSION = v0.34.0 wget https://raw.githubusercontent.com/coreos/prometheus-operator/ ${ OPERATOR_VERSION } /bundle.yaml kubectl apply -f ~/monitoring/bundle.yaml Then wait for the prometheus-operator pod to be Running with kubectl get pods -w . Then we need to configure the Ceph specific configuration: monitoring endpoints, alarm levels etc... kubectl apply -f https://raw.githubusercontent.com/packet-labs/Rook-on-Bare-Metal-Workshop/master/configs/ceph-monitoring.yml At this point we should be able to reach the Prometheus UI at: IP = $( kubectl get nodes -o jsonpath = '{.items[0].status.addresses[].address}' ) PORT = $( kubectl -n rook-ceph get svc rook-prometheus -o jsonpath = '{.spec.ports[].nodePort}' ) echo \"Your Prometheus UI is available at: http:// $IP : $PORT /\" Head over to Status >> Target and make sure that the ceph-mgr target is UP . Then go to Graph and graph following query ceph_cluster_total_used_bytes/(1024^3) to show the total space used in gigabyte over time. Another query of (ceph_cluster_total_used_bytes / ceph_cluster_total_used_raw_bytes) * 100 will show the % of available space used. Deploy and configure Grafana # Install helm export HELM_VERSION = v3.0.0 wget https://get.helm.sh/helm- ${ HELM_VERSION } -linux-amd64.tar.gz tar -xvzf helm- ${ HELM_VERSION } -linux-amd64.tar.gz chmod +x linux-amd64/helm sudo mv linux-amd64/helm /usr/local/bin/ rm -rf { helm*,linux-amd64 } # Add repository helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm repo update # Install grafana helm install grafana stable/grafana --set service.type = NodePort --set persistence.enabled = true --set persistence.type = pvc --set persistence.size = 10Gi --set persistence.storageClassName = rook-ceph-block As can be seen we're using persistence with a pvc and telling it to use our rook-ceph-block storage. You'll get a lot of data coming out but, ideally, something that look like the below. export NODE_PORT = $( kubectl get --namespace default -o jsonpath = \"{.spec.ports[0].nodePort}\" services grafana ) export NODE_IP = $( kubectl get nodes --namespace default -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) echo http:// $NODE_IP : $NODE_PORT Once logged in you'll see a screen similar to below. Hit the Add data source and select Prometheus . The url needs to be the details we identified above. We'll need the Cluster-IP and Port of the service (choose the container port, not the externally exposed one). $ kubectl -n rook-ceph get svc rook-prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-prometheus NodePort 10 .110.175.22 <none> 9090 :30900/TCP 16h Hit Save & Test and you should hopefully see a Data source is working check appear. Hit Back to go back to the main screen. Back on the main screen click on the + and select Import . Ceph has published some open dashboards with the IDs 2842 , 5336 and 5342 . NB: On two of the dashboards you need to select Prometheus as the datasource.","title":"Monitoring with Prometheus and Grafana"},{"location":"01.infrastructure/04.monitoring/00.monitoring.with.prometheus.and.grafana/#deploy-and-configure-prometheus","text":"mkdir -p ~/monitoring cd monitoring/ export OPERATOR_VERSION = v0.34.0 wget https://raw.githubusercontent.com/coreos/prometheus-operator/ ${ OPERATOR_VERSION } /bundle.yaml kubectl apply -f ~/monitoring/bundle.yaml Then wait for the prometheus-operator pod to be Running with kubectl get pods -w . Then we need to configure the Ceph specific configuration: monitoring endpoints, alarm levels etc... kubectl apply -f https://raw.githubusercontent.com/packet-labs/Rook-on-Bare-Metal-Workshop/master/configs/ceph-monitoring.yml At this point we should be able to reach the Prometheus UI at: IP = $( kubectl get nodes -o jsonpath = '{.items[0].status.addresses[].address}' ) PORT = $( kubectl -n rook-ceph get svc rook-prometheus -o jsonpath = '{.spec.ports[].nodePort}' ) echo \"Your Prometheus UI is available at: http:// $IP : $PORT /\" Head over to Status >> Target and make sure that the ceph-mgr target is UP . Then go to Graph and graph following query ceph_cluster_total_used_bytes/(1024^3) to show the total space used in gigabyte over time. Another query of (ceph_cluster_total_used_bytes / ceph_cluster_total_used_raw_bytes) * 100 will show the % of available space used.","title":"Deploy and configure Prometheus"},{"location":"01.infrastructure/04.monitoring/00.monitoring.with.prometheus.and.grafana/#deploy-and-configure-grafana","text":"# Install helm export HELM_VERSION = v3.0.0 wget https://get.helm.sh/helm- ${ HELM_VERSION } -linux-amd64.tar.gz tar -xvzf helm- ${ HELM_VERSION } -linux-amd64.tar.gz chmod +x linux-amd64/helm sudo mv linux-amd64/helm /usr/local/bin/ rm -rf { helm*,linux-amd64 } # Add repository helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm repo update # Install grafana helm install grafana stable/grafana --set service.type = NodePort --set persistence.enabled = true --set persistence.type = pvc --set persistence.size = 10Gi --set persistence.storageClassName = rook-ceph-block As can be seen we're using persistence with a pvc and telling it to use our rook-ceph-block storage. You'll get a lot of data coming out but, ideally, something that look like the below. export NODE_PORT = $( kubectl get --namespace default -o jsonpath = \"{.spec.ports[0].nodePort}\" services grafana ) export NODE_IP = $( kubectl get nodes --namespace default -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) echo http:// $NODE_IP : $NODE_PORT Once logged in you'll see a screen similar to below. Hit the Add data source and select Prometheus . The url needs to be the details we identified above. We'll need the Cluster-IP and Port of the service (choose the container port, not the externally exposed one). $ kubectl -n rook-ceph get svc rook-prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-prometheus NodePort 10 .110.175.22 <none> 9090 :30900/TCP 16h Hit Save & Test and you should hopefully see a Data source is working check appear. Hit Back to go back to the main screen. Back on the main screen click on the + and select Import . Ceph has published some open dashboards with the IDs 2842 , 5336 and 5342 . NB: On two of the dashboards you need to select Prometheus as the datasource.","title":"Deploy and configure Grafana"},{"location":"99.snippets/ceph/","text":"Useful snippets for interacting with Ceph. Toolbox Connect to the toolbox kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash You can then run commands such as ceph device ls Normal If you don't have any devices or OSDs created check what's happening during startup. See docs # get the prepare pods in the cluster $ kubectl -n rook-ceph get pod -l app = rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-node1-fvmrp 0 /1 Completed 0 18m rook-ceph-osd-prepare-node2-w9xv9 0 /1 Completed 0 22m rook-ceph-osd-prepare-node3-7rgnv 0 /1 Completed 0 22m # view the logs for the node of interest in the \"provision\" container $ kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision Remove and wipe kubectl delete -f ~/rook/toolbox.yaml ; \\ kubectl delete -f ~/rook/cluster.yaml ; \\ kubectl delete -f ~/rook/operator.yaml ; \\ kubectl delete -f ~/rook/common.yaml ; \\ kubectl delete namespace rook-ceph ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Ceph"},{"location":"99.snippets/ceph/#toolbox","text":"Connect to the toolbox kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash You can then run commands such as ceph device ls","title":"Toolbox"},{"location":"99.snippets/ceph/#normal","text":"If you don't have any devices or OSDs created check what's happening during startup. See docs # get the prepare pods in the cluster $ kubectl -n rook-ceph get pod -l app = rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-node1-fvmrp 0 /1 Completed 0 18m rook-ceph-osd-prepare-node2-w9xv9 0 /1 Completed 0 22m rook-ceph-osd-prepare-node3-7rgnv 0 /1 Completed 0 22m # view the logs for the node of interest in the \"provision\" container $ kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision","title":"Normal"},{"location":"99.snippets/ceph/#remove-and-wipe","text":"kubectl delete -f ~/rook/toolbox.yaml ; \\ kubectl delete -f ~/rook/cluster.yaml ; \\ kubectl delete -f ~/rook/operator.yaml ; \\ kubectl delete -f ~/rook/common.yaml ; \\ kubectl delete namespace rook-ceph ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Remove and wipe"},{"location":"99.snippets/kubernetes/","text":"Useful snippets of code for interfacing with Kubernetes Wiping a kubernetes installation and starting again Because sometimes stuff goes wrong... kubeadm reset ; \\ sudo apt-get -y purge kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get -y autoremove ; \\ sudo rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd sudo shutdown -r now sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl sudo kubeadm init \\ --pod-network-cidr = 10 .244.0.0/16 \\ --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) To also ensure that rook configuration has been removed sudo rm -rf /var/lib/rook/* Getting the IP address of a node export NODE_NAME = banks kubectl get node ${ NODE_NAME } -o jsonpath = '{.status.addresses[0].address}' Kill a namespace stuck as \"Terminating\" See: https://stackoverflow.com/a/53661717/322358 export NAMESPACE = your-rogue-namespace kubectl proxy & kubectl get namespace $NAMESPACE -o json | jq '.spec = {\"finalizers\":[]}' >temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127 .0.0.1:8001/api/v1/namespaces/ $NAMESPACE /finalize","title":"Kubernetes"},{"location":"99.snippets/kubernetes/#wiping-a-kubernetes-installation-and-starting-again","text":"Because sometimes stuff goes wrong... kubeadm reset ; \\ sudo apt-get -y purge kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get -y autoremove ; \\ sudo rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd sudo shutdown -r now sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl sudo kubeadm init \\ --pod-network-cidr = 10 .244.0.0/16 \\ --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) To also ensure that rook configuration has been removed sudo rm -rf /var/lib/rook/*","title":"Wiping a kubernetes installation and starting again"},{"location":"99.snippets/kubernetes/#getting-the-ip-address-of-a-node","text":"export NODE_NAME = banks kubectl get node ${ NODE_NAME } -o jsonpath = '{.status.addresses[0].address}'","title":"Getting the IP address of a node"},{"location":"99.snippets/kubernetes/#kill-a-namespace-stuck-as-terminating","text":"See: https://stackoverflow.com/a/53661717/322358 export NAMESPACE = your-rogue-namespace kubectl proxy & kubectl get namespace $NAMESPACE -o json | jq '.spec = {\"finalizers\":[]}' >temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127 .0.0.1:8001/api/v1/namespaces/ $NAMESPACE /finalize","title":"Kill a namespace stuck as \"Terminating\""},{"location":"99.snippets/storage.disks/","text":"How to setup encrypted disks for usage by Ceph If Rook determines that a device is not available (has existing partitions or a formatted file system ) then it will skip consuming the devices. As a result we need to encrypt the disk and leave it without partitions or a filesystem for it to be read correctly. Finding the relevant disks is done with lsblk . example lsblk $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID } Auto-mount encrypted devices at boot We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Storage"},{"location":"99.snippets/storage.disks/#how-to-setup-encrypted-disks-for-usage-by-ceph","text":"If Rook determines that a device is not available (has existing partitions or a formatted file system ) then it will skip consuming the devices. As a result we need to encrypt the disk and leave it without partitions or a filesystem for it to be read correctly. Finding the relevant disks is done with lsblk . example lsblk $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID }","title":"How to setup encrypted disks for usage by Ceph"},{"location":"99.snippets/storage.disks/#auto-mount-encrypted-devices-at-boot","text":"We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Auto-mount encrypted devices at boot"}]}