{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Darth-Veitcher's Homelab This repository contains docker details, notes and general setup musings around configuring my (overkill) homelab end-to-end. This has been deliberately done the hard way (i.e. handcrafted from scratch) wherever possible so that I understand what's going on under the hood. I'm not a big fan of black boxes. You'll undoubtedly find quicker and easier getting started with Kubernetes guides elsewhere online (personally I'd recommend looking at some of the excellent posts by Alex Ellis ) but I wanted something fully featured. This setup is opinionated and features the following: Hybrid cloud setup with nodes both on-premise (bare metal) and cloud (dedicated servers) VPN links for secure access between nodes Redundant Ceph storage Monitoring via Prometheus, visualisation with Grafana TLS certificates via LetsEncrypt Integrated Identity and Access Management (IDAM), supporting multiple protocols (incl. Oauth, OIDC and LDAP) via Keycloak High level requirements, roadmap and table of contents Reference Architecture (appendix) Configuring physical nodes VPN between nodes Basic hardening Configuring kubernetes Networking and Network Policy via Canal Service Discovery via CoreDNS Ceph storage via Rook Block storage for pods Shared filesystem Object storage with LDAP for authentication ; and with Vault for secure key management With dashboard Enabled object gateway management via SSO Monitoring with Prometheus and Grafana Cloudflare integration with external-dns TLS certificates with Cert-Manager Self-signed Root CA for internal services Let's Encrypt for external services Identity and Access Management with OIDC OpenLDAP as directory service KeyCloak/Dex as identity provider Multi-factor auth for key admin users OIDC Forward Auth for additional fine grained RBAC Secure VPN access for users Integrated with PKI Integrated with IDAM, Keycloak+OpenLDAP Services Stack: Developer Docker registry Gitea","title":"Introduction"},{"location":"#darth-veitchers-homelab","text":"This repository contains docker details, notes and general setup musings around configuring my (overkill) homelab end-to-end. This has been deliberately done the hard way (i.e. handcrafted from scratch) wherever possible so that I understand what's going on under the hood. I'm not a big fan of black boxes. You'll undoubtedly find quicker and easier getting started with Kubernetes guides elsewhere online (personally I'd recommend looking at some of the excellent posts by Alex Ellis ) but I wanted something fully featured. This setup is opinionated and features the following: Hybrid cloud setup with nodes both on-premise (bare metal) and cloud (dedicated servers) VPN links for secure access between nodes Redundant Ceph storage Monitoring via Prometheus, visualisation with Grafana TLS certificates via LetsEncrypt Integrated Identity and Access Management (IDAM), supporting multiple protocols (incl. Oauth, OIDC and LDAP) via Keycloak","title":"Darth-Veitcher's Homelab"},{"location":"#high-level-requirements-roadmap-and-table-of-contents","text":"Reference Architecture (appendix) Configuring physical nodes VPN between nodes Basic hardening Configuring kubernetes Networking and Network Policy via Canal Service Discovery via CoreDNS Ceph storage via Rook Block storage for pods Shared filesystem Object storage with LDAP for authentication ; and with Vault for secure key management With dashboard Enabled object gateway management via SSO Monitoring with Prometheus and Grafana Cloudflare integration with external-dns TLS certificates with Cert-Manager Self-signed Root CA for internal services Let's Encrypt for external services Identity and Access Management with OIDC OpenLDAP as directory service KeyCloak/Dex as identity provider Multi-factor auth for key admin users OIDC Forward Auth for additional fine grained RBAC Secure VPN access for users Integrated with PKI Integrated with IDAM, Keycloak+OpenLDAP Services Stack: Developer Docker registry Gitea","title":"High level requirements, roadmap and table of contents"},{"location":"01.infrastructure/01.hosts/00.configuring.physical.nodes/","text":"Physical Nodes I'm going to start with a single node install on a server at home and then, slowly, add additional nodes to create a cluster. I'll taint the first node though so that it can run standalone without needing further nodes to operate. My final configuration will look roughly like this: Homelab Server 1: Dell R610, Primary Compute, asimov Server 2: Dell R710, Secondary Compute, banks Server 3: Custom Build, Primary LAN Storage, clarke Mini PC: Custom Build x86, LoadBalancer between master nodes for HA (later step), hamilton Intel Celeron J1900 quad core, 8GB RAM, 32GB SSD, x4 LAN Cloud Dedicated Server 1: Kimsufi, Ingress Node, donaldson Dedicated Server 2: Kimsufi, Compute Node, eesmith Between the Homelab and Cloud I'll run a VPN such that traffic is encrypted and we can bypass the public internet and any associated NAT / routing issues by having the nodes discover each other using VPN subnet addresses. We'll start with a single node on-prem and then expand and add to this.","title":"Host Configuration"},{"location":"01.infrastructure/01.hosts/00.configuring.physical.nodes/#physical-nodes","text":"I'm going to start with a single node install on a server at home and then, slowly, add additional nodes to create a cluster. I'll taint the first node though so that it can run standalone without needing further nodes to operate. My final configuration will look roughly like this: Homelab Server 1: Dell R610, Primary Compute, asimov Server 2: Dell R710, Secondary Compute, banks Server 3: Custom Build, Primary LAN Storage, clarke Mini PC: Custom Build x86, LoadBalancer between master nodes for HA (later step), hamilton Intel Celeron J1900 quad core, 8GB RAM, 32GB SSD, x4 LAN Cloud Dedicated Server 1: Kimsufi, Ingress Node, donaldson Dedicated Server 2: Kimsufi, Compute Node, eesmith Between the Homelab and Cloud I'll run a VPN such that traffic is encrypted and we can bypass the public internet and any associated NAT / routing issues by having the nodes discover each other using VPN subnet addresses. We'll start with a single node on-prem and then expand and add to this.","title":"Physical Nodes"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/","text":"This is an optional setup if you want to configure a direct wireguard VPN link between nodes instead of using a CNI plugin. Setup Wireguard (VPN) See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. I've gone with Wireguard over OpenVPN based on it being incorporated into the Linux Kernel and increased performance versus OpenVPN. Install Wireguard sudo add-apt-repository -y ppa:wireguard/wireguard ; \\ sudo apt-get install -y wireguard ; \\ sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. root@banks:~# lsmod | grep wireguard wireguard 212992 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Keys You will need to generate a key-pair for every peer (device) that is connected, including things like mobile phones etc. The iOS WireGuard client allow you to generate the keys on the device itself (if you want). # Generate public/private keypair cd /etc/wireguard umask 077 wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey Configure We need to create a network interface now for the wireguard VPN. Common convention is to use wg0 as a name for this. In addition we also need to choose a subnet for the VPN addresses. As I've got a 192.168.0.1/24 configuration at home I'll use 10.10.0.1/24 for the VPN. Note the highlighted IP address we assign to each node here. It will need to be incremented for each to provide a unique address. # file: /etc/wireguard/wg0.conf [ Interface ] PrivateKey = {{ PRIVATE_KEY }} Address = 10 .10.0.1/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT ; iptables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -A FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT ; iptables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -D FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ListenPort = 51820 Now to keepo things DRY we'll run the following to replace the placeholder text with the actual contents of our server's private key we generated earlier. sudo sed -i.bak 's/{{PRIVATE_KEY}}/' $( sudo cat /etc/wireguard/privatekey ) '/' /etc/wireguard/wg0.conf We also need to replace the {{ETH0}} placeholder with the name of our existing primary network interface. A quick one-liner for this is ip -4 route | grep default | awk '{print $5}' which, on my server, gives bond0 as the answer (as I'm running LACP across multiple bonded physical interfaces). sudo sed -i.bak 's/{{ETH0}}/' $( ip -4 route | grep default | awk '{print $5}' ) '/g' /etc/wireguard/wg0.conf Enable forwarding of packets in the host kernel. sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p Finally we can start the wg0 interface. wg-quick up wg0 Hopefully you'll see something like the below output. root@banks:/etc/wireguard# wg-quick up wg0 [ #] ip link add wg0 type wireguard [ #] wg setconf wg0 /dev/fd/63 [ #] ip -4 address add 10.10.0.1/24 dev wg0 [ #] ip -6 address add fd86:ea04:1111::1/64 dev wg0 [ #] ip link set mtu 1420 up dev wg0 [ #] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o bond0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o bond0 -j MASQUERADE Checking status The check the status of wireguard run the wg command. root@banks:/etc/wireguard# wg interface: wg0 public key: I6ZHsLe44SHNH44xE86AI0VEnm8CfzrQUrxSCJVjAEw = private key: ( hidden ) listening port: 51820 In addition, we should now have an additional route appear for our VPN subnet. root@banks:/etc/wireguard# ip route default via 192 .168.0.1 dev bond0 proto dhcp src 192 .168.0.94 metric 100 10 .10.0.0/24 dev wg0 proto kernel scope link src 10 .10.0.1 172 .17.0.0/16 dev docker0 proto kernel scope link src 172 .17.0.1 linkdown ... Systemd service Assuming the above works we can now enable the wg0 interface on boot. sudo systemctl enable wg-quick@wg0.service sudo systemctl daemon-reload You can then manually start sudo service wg-quick@wg0 start and check status service wg-quick@wg0 status of the service.","title":"00.wireguard.vpn"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#setup-wireguard-vpn","text":"See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. I've gone with Wireguard over OpenVPN based on it being incorporated into the Linux Kernel and increased performance versus OpenVPN.","title":"Setup Wireguard (VPN)"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#install-wireguard","text":"sudo add-apt-repository -y ppa:wireguard/wireguard ; \\ sudo apt-get install -y wireguard ; \\ sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. root@banks:~# lsmod | grep wireguard wireguard 212992 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard","title":"Install Wireguard"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#keys","text":"You will need to generate a key-pair for every peer (device) that is connected, including things like mobile phones etc. The iOS WireGuard client allow you to generate the keys on the device itself (if you want). # Generate public/private keypair cd /etc/wireguard umask 077 wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey","title":"Keys"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#configure","text":"We need to create a network interface now for the wireguard VPN. Common convention is to use wg0 as a name for this. In addition we also need to choose a subnet for the VPN addresses. As I've got a 192.168.0.1/24 configuration at home I'll use 10.10.0.1/24 for the VPN. Note the highlighted IP address we assign to each node here. It will need to be incremented for each to provide a unique address. # file: /etc/wireguard/wg0.conf [ Interface ] PrivateKey = {{ PRIVATE_KEY }} Address = 10 .10.0.1/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT ; iptables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -A FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -A POSTROUTING -o {{ ETH0 }} -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT ; iptables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ; ip6tables -D FORWARD -i wg0 -j ACCEPT ; ip6tables -t nat -D POSTROUTING -o {{ ETH0 }} -j MASQUERADE ListenPort = 51820 Now to keepo things DRY we'll run the following to replace the placeholder text with the actual contents of our server's private key we generated earlier. sudo sed -i.bak 's/{{PRIVATE_KEY}}/' $( sudo cat /etc/wireguard/privatekey ) '/' /etc/wireguard/wg0.conf We also need to replace the {{ETH0}} placeholder with the name of our existing primary network interface. A quick one-liner for this is ip -4 route | grep default | awk '{print $5}' which, on my server, gives bond0 as the answer (as I'm running LACP across multiple bonded physical interfaces). sudo sed -i.bak 's/{{ETH0}}/' $( ip -4 route | grep default | awk '{print $5}' ) '/g' /etc/wireguard/wg0.conf Enable forwarding of packets in the host kernel. sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p Finally we can start the wg0 interface. wg-quick up wg0 Hopefully you'll see something like the below output. root@banks:/etc/wireguard# wg-quick up wg0 [ #] ip link add wg0 type wireguard [ #] wg setconf wg0 /dev/fd/63 [ #] ip -4 address add 10.10.0.1/24 dev wg0 [ #] ip -6 address add fd86:ea04:1111::1/64 dev wg0 [ #] ip link set mtu 1420 up dev wg0 [ #] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o bond0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o bond0 -j MASQUERADE Checking status The check the status of wireguard run the wg command. root@banks:/etc/wireguard# wg interface: wg0 public key: I6ZHsLe44SHNH44xE86AI0VEnm8CfzrQUrxSCJVjAEw = private key: ( hidden ) listening port: 51820 In addition, we should now have an additional route appear for our VPN subnet. root@banks:/etc/wireguard# ip route default via 192 .168.0.1 dev bond0 proto dhcp src 192 .168.0.94 metric 100 10 .10.0.0/24 dev wg0 proto kernel scope link src 10 .10.0.1 172 .17.0.0/16 dev docker0 proto kernel scope link src 172 .17.0.1 linkdown ...","title":"Configure"},{"location":"01.infrastructure/01.hosts/00.wireguard.vpn/#systemd-service","text":"Assuming the above works we can now enable the wg0 interface on boot. sudo systemctl enable wg-quick@wg0.service sudo systemctl daemon-reload You can then manually start sudo service wg-quick@wg0 start and check status service wg-quick@wg0 status of the service.","title":"Systemd service"},{"location":"01.infrastructure/01.hosts/01.banks/","text":"Basic housekeeping First of all install the latest Ubuntu 18.04 LTS release and copy across your ssh key and then follow some simple hardening steps #TODO: link to ansible . Allow sudo without password (optional) You can allow your user to execute sudo commands without needing a password prompt for ease. echo \" $USER ALL=(ALL:ALL) NOPASSWD:ALL\" | sudo tee -a /etc/sudoers > /dev/null We'll configure our hostname too in order to reflect whatever the fqdn of our server is going to be. In my case this will be banks.local as it will just be an internal address. This is used by kubernetes later on. sudo hostnamectl set-hostname banks.local Install wireguard See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. As wireguard is now incorporated into the linux kernel we can install it on the host nodes and then kubernetes network plugins such as wormhole or kilo can then use the kernel module to configure a dynamic VPN mesh between nodes. sudo add-apt-repository -y ppa:wireguard/wireguard ; \\ sudo apt-get install -y wireguard ; \\ sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. $ lsmod | grep wireguard wireguard 208896 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Enable forwarding of packets in the host kernel (there might be a slight delay in reconnecting if doing this over ssh). sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p Enable Avahi (Discovery) on VPN and LAN As per Wikipedia . Avahi is a free zero-configuration networking (zeroconf) implementation, including a system for multicast DNS/DNS-SD service discovery. It is licensed under the GNU Lesser General Public License (LGPL). Avahi is a system which enables programs to publish and discover services and hosts running on a local network. For example, a user can plug a computer into a network and have Avahi automatically advertise the network services running on its machine, facilitating user access to those services. We will setup our nodes to publish themselves on both the LAN (so can be accessed via their hostnames) and VPN (optional). # from docker export DEBIAN_FRONTEND = noninteractive ; \\ sudo apt-get update -y ; \\ sudo apt-get -qq install -y avahi-daemon avahi-utils The main configuration is held in /etc/avahi/avahi-daemon.conf . We can modify the allow-interfaces line (to limit which interfaces we advertise on). This will be useful for when we want to only enable it on internal interfaces (e.g. our Cloud node shouldn't try and broadcast across the internet). We'll leave this for now. # file: /etc/avahi/avahi-daemon.conf allow-interfaces = bond0, wg0, docker0 # If we're using an internal domain then leave as `local` below, else change to tld domain-name = local Reload / restart the daemon with sudo systemctl daemon-reload && sudo systemctl restart avahi-daemon.service Network status After a vanilla install above, the network configuration and topology looks like the below. Network status $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 6 links listed. $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 192 .168.0.1 0 .0.0.0 UG 100 0 0 bond0 192 .168.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 bond0 192 .168.0.1 0 .0.0.0 255 .255.255.255 UH 100 0 0 bond0","title":"Compute (Banks)"},{"location":"01.infrastructure/01.hosts/01.banks/#basic-housekeeping","text":"First of all install the latest Ubuntu 18.04 LTS release and copy across your ssh key and then follow some simple hardening steps #TODO: link to ansible . Allow sudo without password (optional) You can allow your user to execute sudo commands without needing a password prompt for ease. echo \" $USER ALL=(ALL:ALL) NOPASSWD:ALL\" | sudo tee -a /etc/sudoers > /dev/null We'll configure our hostname too in order to reflect whatever the fqdn of our server is going to be. In my case this will be banks.local as it will just be an internal address. This is used by kubernetes later on. sudo hostnamectl set-hostname banks.local","title":"Basic housekeeping"},{"location":"01.infrastructure/01.hosts/01.banks/#install-wireguard","text":"See Wireguard vs OpenVPN on a local Gigabit Network for a performance comparison. As wireguard is now incorporated into the linux kernel we can install it on the host nodes and then kubernetes network plugins such as wormhole or kilo can then use the kernel module to configure a dynamic VPN mesh between nodes. sudo add-apt-repository -y ppa:wireguard/wireguard ; \\ sudo apt-get install -y wireguard ; \\ sudo modprobe wireguard # activate kernal module Check Kernel Module To check if the module is loaded use lsmod | grep wireguard . You should see something like the below. $ lsmod | grep wireguard wireguard 208896 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Enable forwarding of packets in the host kernel (there might be a slight delay in reconnecting if doing this over ssh). sudo tee /etc/sysctl.conf << EOF net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 EOF sudo sysctl -p","title":"Install wireguard"},{"location":"01.infrastructure/01.hosts/01.banks/#enable-avahi-discovery-on-vpn-and-lan","text":"As per Wikipedia . Avahi is a free zero-configuration networking (zeroconf) implementation, including a system for multicast DNS/DNS-SD service discovery. It is licensed under the GNU Lesser General Public License (LGPL). Avahi is a system which enables programs to publish and discover services and hosts running on a local network. For example, a user can plug a computer into a network and have Avahi automatically advertise the network services running on its machine, facilitating user access to those services. We will setup our nodes to publish themselves on both the LAN (so can be accessed via their hostnames) and VPN (optional). # from docker export DEBIAN_FRONTEND = noninteractive ; \\ sudo apt-get update -y ; \\ sudo apt-get -qq install -y avahi-daemon avahi-utils The main configuration is held in /etc/avahi/avahi-daemon.conf . We can modify the allow-interfaces line (to limit which interfaces we advertise on). This will be useful for when we want to only enable it on internal interfaces (e.g. our Cloud node shouldn't try and broadcast across the internet). We'll leave this for now. # file: /etc/avahi/avahi-daemon.conf allow-interfaces = bond0, wg0, docker0 # If we're using an internal domain then leave as `local` below, else change to tld domain-name = local Reload / restart the daemon with sudo systemctl daemon-reload && sudo systemctl restart avahi-daemon.service","title":"Enable Avahi (Discovery) on VPN and LAN"},{"location":"01.infrastructure/01.hosts/01.banks/#network-status","text":"After a vanilla install above, the network configuration and topology looks like the below. Network status $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 6 links listed. $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 192 .168.0.1 0 .0.0.0 UG 100 0 0 bond0 192 .168.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 bond0 192 .168.0.1 0 .0.0.0 255 .255.255.255 UH 100 0 0 bond0","title":"Network status"},{"location":"01.infrastructure/01.hosts/02.clarke/","text":"Clarke (storage node) I've got an old self-built frankenstein storage server that runs a lot of disks in it. Again, I've installed Ubuntu 18.04 LTS on it and will add as a node to the cluster to surface up file/block/object storage to workloads. Setup storage As I'll be using Ceph I'll leave all the disks as standalone (no RAID) and perform fulldisk encryption such that they have to be unlocked if removed from the server. Finding the relevant disks is done with lsblk . example lsblk adminlocal@banks:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID } Auto-mount encrypted devices at boot We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Storage (Clarke)"},{"location":"01.infrastructure/01.hosts/02.clarke/#clarke-storage-node","text":"I've got an old self-built frankenstein storage server that runs a lot of disks in it. Again, I've installed Ubuntu 18.04 LTS on it and will add as a node to the cluster to surface up file/block/object storage to workloads.","title":"Clarke (storage node)"},{"location":"01.infrastructure/01.hosts/02.clarke/#setup-storage","text":"As I'll be using Ceph I'll leave all the disks as standalone (no RAID) and perform fulldisk encryption such that they have to be unlocked if removed from the server. Finding the relevant disks is done with lsblk . example lsblk adminlocal@banks:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID }","title":"Setup storage"},{"location":"01.infrastructure/01.hosts/02.clarke/#auto-mount-encrypted-devices-at-boot","text":"We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Auto-mount encrypted devices at boot"},{"location":"01.infrastructure/01.hosts/03.donaldson/","text":"For the Kubernetes ingress node I'm going to use a cheap (<$5 per month) dedicated server from Kimsufi which has unmetered traffic and 100mb bandwidth. This will allow me to have a static IP4 address to point my DNS records at whilst then using the internal wireguard VPN to route traffic from here back home / elsewhere in a secure fashion. apt-get update ; \\ do -release-upgrade -f DistUpgradeViewNonInteractive # do-release-upgrade -f noninteractive","title":"Compute (Donaldson)"},{"location":"01.infrastructure/01.hosts/04.hamilton/","text":"","title":"Ingress (Hamilton)"},{"location":"01.infrastructure/01.hosts/rancheros/","text":"Configuration for RancherOS is performed by a cloud-init file. The main things we want to do is enable the kernel-headers services (so that we can run wireguard later on) and then install our configuration via a yaml file. sudo ros service enable kernel-headers sudo ros service enable kernel-headers-system-docker This can be added to a cloud-init file. Running sudo ros config export will show the actual settings to add. $ sudo ros config export rancher: environment: EXTRA_CMDLINE: /init services_include: kernel-headers: true kernel-headers-system-docker: true ssh_authorized_keys: [] Whilst each individual node will potentially have some different configurations for the likes of physcial networking we will have a section at the top that simply enables wireguard. rancher : # Load Kernel module for wireguard modules : [ wireguard ] We also want to install a compatible docker version rancher : # https://rancher.com/docs/os/v1.x/en/installation/configuration/switching-docker-versions/ docker : engine : docker-18.09.9","title":"Rancheros"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/","text":"In part 1 we setup and configured the physical nodes we are going to use for our kubernetes cluster, ensuring they could communicate between each other using a secure VPN. We'll now: install kubernetes implement a CNI (network solution for pod communication) add a load balancer so that services can obtain external IP addresses give your user them the ability to run commands on the cluster Install Kubernetes Docker As Kubernetes is an orchestration layer we will need to install a container runtime eninge. The below commands will install a compatible version of Docker. Kubernetes and Docker versions It's worth being aware that Kubernetes only supports specific versions of docker and, as a result, you should check for compatability in their changelog. At time of writing the latest stable version of Kubernetes was 1.16. The CHANGELOG-1.16 shows that they have validated the following docker versions: The list of validated docker versions remains unchanged. The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09 export K8S_DOCKER_VERSION = 18 .09 sudo apt-get update ; \\ sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common ; \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - ; \\ sudo add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" ; \\ sudo apt-get update && sudo apt-get install -y docker-ce = $( apt-cache madison docker-ce | grep ${ K8S_DOCKER_VERSION } | head -1 | awk '{print $3}' ) Network status With docker installed the network configuration and topology now looks like this. $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 7 links listed. $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 192 .168.0.1 0 .0.0.0 UG 100 0 0 bond0 172 .17.0.0 0 .0.0.0 255 .255.0.0 U 0 0 0 docker0 192 .168.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 bond0 192 .168.0.1 0 .0.0.0 255 .255.255.255 UH 100 0 0 bond0 Kubernetes With docker now setup as a runtime we'll install Kubernetes as well as jq (which will come in handy later). sudo apt-get update && sudo apt-get install -y apt-transport-https ; \\ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - ; \\ sudo tee /etc/apt/sources.list.d/kubernetes.list <<EOF deb http://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl jq Pre-flight fixes Before intialising the cluster we need to change the following: modify the cgroup driver from cgroupfs to systemd disable swap # Setup daemon. sudo tee /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo mkdir -p /etc/systemd/system/docker.service.d # Restart docker. sudo systemctl daemon-reload sudo systemctl restart docker The swap should be disabled both for the current session with sudo swapoff -a and then in /etc/fstab (just add a comment # at the start of the line) so that this persists across reboots. Initialise Initialise the node sudo kubeadm config images pull ; \\ sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 Listening on a specific address If you wanted to you can set the api for kubernetes to listen only on a specific address. This is useful if, for instance, you have a VPN connection between nodes. An example of how to initialise this is below. In this instance we're getting the IPv4 address of the wireguard interface. # Bind to VPN address sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) You'll see a load of scrolling log text followed by the following indicating success and giving some next step instructions. Success Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192 .168.0.101:6443 --token 64we2d.5jzsjzpa0ysqzagl \\ --discovery-token-ca-cert-hash sha256:9d1dda6163e0e539588e0209f06b37d209d230a373b0167ea5f881cd60537178 Give your user the ability to run kubectl . mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config To test this works we can run a couple of kubernetes commands. $ kubectl get nodes NAME STATUS ROLES AGE VERSION banks.local NotReady master 15s v1.17.0 The node will show as NotReady until we add a container network interface (CNI) in the next step. Remove the Control plane node isolation taint The official docs highlight: By default, your cluster will not schedule pods on the control-plane node for security reasons. This will be a problem if we only have one node running (e.g. homelab development) so we will remove the taint that prevents this. kubectl taint nodes --all node-role.kubernetes.io/master- This will remove the node-role.kubernetes.io/master taint from any nodes that have it, including the control-plane node, meaning that the scheduler will then be able to schedule pods everywhere. Add node labels For later on we will add a topology.kubernetes.io/region label to the node as this is used to indicate failure domains for DR and also assists with the setup of VPN links. For more informatio see Running in multiple zones and Well-Known Labels, Annotations and Taints in the official kubernetes docs. kubectl label node banks.local topology.kubernetes.io/region = mancave Wiping your Kubernetes installation On the off-chance that you ever want to completely uninstall kubernetes and associated resources you can run the commands below to purge from the system. kubeadm reset ; \\ sudo apt-get purge -y kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get autoremove -y ; \\ rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd * off-chance being the polite way of saying \"this happens quite a lot when learning\"... Network status With kubernetes installed the network configuration and topology still look the same until we apply a CNI for the pods to communicate with each other.","title":"Install Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#install-kubernetes","text":"","title":"Install Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#docker","text":"As Kubernetes is an orchestration layer we will need to install a container runtime eninge. The below commands will install a compatible version of Docker. Kubernetes and Docker versions It's worth being aware that Kubernetes only supports specific versions of docker and, as a result, you should check for compatability in their changelog. At time of writing the latest stable version of Kubernetes was 1.16. The CHANGELOG-1.16 shows that they have validated the following docker versions: The list of validated docker versions remains unchanged. The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09 export K8S_DOCKER_VERSION = 18 .09 sudo apt-get update ; \\ sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common ; \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - ; \\ sudo add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" ; \\ sudo apt-get update && sudo apt-get install -y docker-ce = $( apt-cache madison docker-ce | grep ${ K8S_DOCKER_VERSION } | head -1 | awk '{print $3}' ) Network status With docker installed the network configuration and topology now looks like this. $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 7 links listed. $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 192 .168.0.1 0 .0.0.0 UG 100 0 0 bond0 172 .17.0.0 0 .0.0.0 255 .255.0.0 U 0 0 0 docker0 192 .168.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 bond0 192 .168.0.1 0 .0.0.0 255 .255.255.255 UH 100 0 0 bond0","title":"Docker"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#kubernetes","text":"With docker now setup as a runtime we'll install Kubernetes as well as jq (which will come in handy later). sudo apt-get update && sudo apt-get install -y apt-transport-https ; \\ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - ; \\ sudo tee /etc/apt/sources.list.d/kubernetes.list <<EOF deb http://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl jq","title":"Kubernetes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#pre-flight-fixes","text":"Before intialising the cluster we need to change the following: modify the cgroup driver from cgroupfs to systemd disable swap # Setup daemon. sudo tee /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo mkdir -p /etc/systemd/system/docker.service.d # Restart docker. sudo systemctl daemon-reload sudo systemctl restart docker The swap should be disabled both for the current session with sudo swapoff -a and then in /etc/fstab (just add a comment # at the start of the line) so that this persists across reboots.","title":"Pre-flight fixes"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#initialise","text":"Initialise the node sudo kubeadm config images pull ; \\ sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 Listening on a specific address If you wanted to you can set the api for kubernetes to listen only on a specific address. This is useful if, for instance, you have a VPN connection between nodes. An example of how to initialise this is below. In this instance we're getting the IPv4 address of the wireguard interface. # Bind to VPN address sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) You'll see a load of scrolling log text followed by the following indicating success and giving some next step instructions. Success Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192 .168.0.101:6443 --token 64we2d.5jzsjzpa0ysqzagl \\ --discovery-token-ca-cert-hash sha256:9d1dda6163e0e539588e0209f06b37d209d230a373b0167ea5f881cd60537178 Give your user the ability to run kubectl . mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config To test this works we can run a couple of kubernetes commands. $ kubectl get nodes NAME STATUS ROLES AGE VERSION banks.local NotReady master 15s v1.17.0 The node will show as NotReady until we add a container network interface (CNI) in the next step.","title":"Initialise"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#remove-the-control-plane-node-isolation-taint","text":"The official docs highlight: By default, your cluster will not schedule pods on the control-plane node for security reasons. This will be a problem if we only have one node running (e.g. homelab development) so we will remove the taint that prevents this. kubectl taint nodes --all node-role.kubernetes.io/master- This will remove the node-role.kubernetes.io/master taint from any nodes that have it, including the control-plane node, meaning that the scheduler will then be able to schedule pods everywhere.","title":"Remove the Control plane node isolation taint"},{"location":"01.infrastructure/02.kubernetes/00.configuring.kubernetes/#add-node-labels","text":"For later on we will add a topology.kubernetes.io/region label to the node as this is used to indicate failure domains for DR and also assists with the setup of VPN links. For more informatio see Running in multiple zones and Well-Known Labels, Annotations and Taints in the official kubernetes docs. kubectl label node banks.local topology.kubernetes.io/region = mancave Wiping your Kubernetes installation On the off-chance that you ever want to completely uninstall kubernetes and associated resources you can run the commands below to purge from the system. kubeadm reset ; \\ sudo apt-get purge -y kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get autoremove -y ; \\ rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd * off-chance being the polite way of saying \"this happens quite a lot when learning\"... Network status With kubernetes installed the network configuration and topology still look the same until we apply a CNI for the pods to communicate with each other.","title":"Add node labels"},{"location":"01.infrastructure/02.kubernetes/01.cni/","text":"Navigating to the addons link provided above will show that we've got some options available. First we will focus on the Networking and Network Policy where we will use Canal as a CNI (networking solution between pods). Canal unites Flannel and Calico, providing networking and network policy. Canal Most of these manifests can be found in the official GitHub repo if required. # Role-based access control (RBAC) # Kubernetes API datastore with flannel networking: # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/integration#role-based-access-control-rbac kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-flannel.yaml # Installing Calico for policy and flannel for networking # Installing with the Kubernetes API datastore (recommended) # We can install directly as we're using the pod CIDR 10.244.0.0/16 # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/canal.yaml Network Interfaces Checking the available network interfaces should now show something similar to the below. You'll have a number of cali* interfaces, each one per pod. $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 8 cali9cce5775874 ether degraded unmanaged 11 cali8bf6cbe9a38 ether degraded unmanaged 12 flannel.1 ether routable unmanaged 13 cali4010097f9ae ether degraded unmanaged 14 calia9e20d251e6 ether degraded unmanaged Pure Calico mkdir -p ~/calico ; \\ cd ~/calico ; \\ wget https://docs.projectcalico.org/v3.10/manifests/calico.yaml export POD_CIDR = 10 .244.0.0/16 ; \\ sed -i -e \"s?192.168.0.0/16? $POD_CIDR ?g\" calico.yaml kubectl apply -f ~/calico/calico.yaml Network Interfaces Checking the available network interfaces should now show something similar to the below. You'll have a number of cali* interfaces, each one per pod. The main difference is a tunl0 as opposed to flannel.1 . $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 8 caliba1d20a20a6 ether degraded unmanaged 11 tunl0 tunnel routable unmanaged 12 cali6b7a2c35969 ether degraded unmanaged 13 cali7c0e5e55068 ether degraded unmanaged 14 calid6b03898a0b ether degraded unmanaged 32 cali887554ea8bf ether degraded unmanaged","title":"CNI"},{"location":"01.infrastructure/02.kubernetes/01.cni/#canal","text":"Most of these manifests can be found in the official GitHub repo if required. # Role-based access control (RBAC) # Kubernetes API datastore with flannel networking: # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/integration#role-based-access-control-rbac kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-flannel.yaml # Installing Calico for policy and flannel for networking # Installing with the Kubernetes API datastore (recommended) # We can install directly as we're using the pod CIDR 10.244.0.0/16 # https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/canal.yaml Network Interfaces Checking the available network interfaces should now show something similar to the below. You'll have a number of cali* interfaces, each one per pod. $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 8 cali9cce5775874 ether degraded unmanaged 11 cali8bf6cbe9a38 ether degraded unmanaged 12 flannel.1 ether routable unmanaged 13 cali4010097f9ae ether degraded unmanaged 14 calia9e20d251e6 ether degraded unmanaged","title":"Canal"},{"location":"01.infrastructure/02.kubernetes/01.cni/#pure-calico","text":"mkdir -p ~/calico ; \\ cd ~/calico ; \\ wget https://docs.projectcalico.org/v3.10/manifests/calico.yaml export POD_CIDR = 10 .244.0.0/16 ; \\ sed -i -e \"s?192.168.0.0/16? $POD_CIDR ?g\" calico.yaml kubectl apply -f ~/calico/calico.yaml Network Interfaces Checking the available network interfaces should now show something similar to the below. You'll have a number of cali* interfaces, each one per pod. The main difference is a tunl0 as opposed to flannel.1 . $ networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether carrier configured 3 eno2 ether carrier configured 4 eno3 ether carrier configured 5 eno4 ether carrier configured 6 bond0 ether routable configured 7 docker0 ether no-carrier unmanaged 8 caliba1d20a20a6 ether degraded unmanaged 11 tunl0 tunnel routable unmanaged 12 cali6b7a2c35969 ether degraded unmanaged 13 cali7c0e5e55068 ether degraded unmanaged 14 calid6b03898a0b ether degraded unmanaged 32 cali887554ea8bf ether degraded unmanaged","title":"Pure Calico"},{"location":"01.infrastructure/02.kubernetes/02.metallb/","text":"When you run Kubernetes services on supported cloud providers (GCP, AWS, Azure) obtaining an externally adressable endpoint for those you'd like to expose is abstracted away behind a call to a LoadBalancer in their infrastructre. This provisions an IP address and routes it to your service. As we're running on-prem or in our own cloud configuration we don't have this luxury. Luckily, someone at Google also likes bare metal... and so they created MetalLB as an alternative implementation of this functionality. MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols. Install MetalLB per the docs export METALLB_VERSION = v0.8.3 mkdir -p ~/metallb ; \\ cd ~/metallb wget https://raw.githubusercontent.com/google/metallb/ ${ METALLB_VERSION } /manifests/metallb.yaml ; \\ kubectl apply -f ~/metallb/metallb.yaml Create a configmap that allocates a valid address range on your local network which MetalLB will then hand out. # file: ~/metallb/config.yaml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 192.168.0.200-192.168.0.250 Apply this now with kubectl apply -f ~/metallb/config.yaml . Sample deployment We'll now deploy a simple whoami container and tell MetalLB to give it an external IP address from the cluster internal network. Further information on Service objects in kubernetes can be found in the official docs , specifically the Publishing Services (ServiceTypes) section. # file: ~/metallb/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : type : LoadBalancer selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Run kubectl apply -f ~/metallb/whoami.yaml to create the deployment and associated service. You should now be able to see a service whoami of type LoadBalancer being created and having an EXTERNAL-IP allocated to it by MetalLB. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 13h whoami LoadBalancer 10 .96.99.249 192 .168.0.201 80 :31314/TCP 8m47s You can query this as well with curl to confirm. $ curl 192 .168.0.201 Hostname: whoami-deployment-5b4bb9c787-fmb77 IP: 127 .0.0.1 IP: 192 .168.204.231 RemoteAddr: 192 .168.0.102:20516 GET / HTTP/1.1 Host: 192 .168.0.201 User-Agent: curl/7.58.0 Accept: */* The interesting pieces above are the IPs in the hops. IP: 127.0.0.1 : localhost IP: 192.168.204.231 : an address on the subnet of the Calico CNI tunl0 . You could access the pod directly using this as well if on the same network (pod to pod communication). RemoteAddr: 192.168.0.102:20516 : the client making the request (us, on the master node using curl). Host: 192.168.0.201 : the address allocated to the service by MetalLB, now accessible from anyone on the LAN. You can see some of this by running a describe on the pod with kubectl describe pod -l 'app=whoami' . This will show both the Node and assigned IP as well as the associated CNI annotation that allows Calico to route packets. Name : whoami-deployment-5b4bb9c787-fmb77 Namespace : default Priority : 0 Node : banks.local/172.17.0.1 Start Time : Thu, 12 Dec 2019 11:49:16 +0000 Labels : app=whoami pod-template-hash=5b4bb9c787 Annotations : cni.projectcalico.org/podIP : 192.168.204.231/32 Status : Running IP : 192.168.204.231 IPs : IP : 192.168.204.231 Controlled By : ReplicaSet/whoami-deployment-5b4bb9c787 Containers : whoami : Container ID : docker://5966c73d17c87df726b83db12a8227008dcfbccd4b8bd5cd0fda48bca153f415 Image : containous/whoami Image ID : docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b Port : 80/TCP Host Port : 0/TCP State : Running Started : Thu, 12 Dec 2019 11:49:19 +0000 Ready : True Restart Count : 0 Environment : <none> Mounts : /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro) Conditions : Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes : default-token-k9gvj : Type : Secret (a volume populated by a Secret) SecretName : default-token-k9gvj Optional : false QoS Class : BestEffort Node-Selectors : <none> Tolerations : node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 10m default-scheduler Successfully assigned default/whoami-deployment-5b4bb9c787-fmb77 to banks.local Normal Pulling 10m kubelet, banks.local Pulling image \"containous/whoami\" Normal Pulled 10m kubelet, banks.local Successfully pulled image \"containous/whoami\" Normal Created 10m kubelet, banks.local Created container whoami Normal Started 10m kubelet, banks.local Started container whoami Reviewing the service with kubectl describe svc whoami gives similarly useful information. Name : whoami Namespace : default Labels : <none> Annotations : kubectl.kubernetes.io/last-applied-configuration : { \"apiVersion\" : \"v1\" , \"kind\" : \"Service\" , \"metadata\" :{ \"annotations\" :{}, \"name\" : \"whoami\" , \"namespace\" : \"default\" }, \"spec\" :{ \"ports\" :[{ \"port\" : 80 , \"proto... Selector: app=whoami Type: LoadBalancer IP: 10.96.99.249 LoadBalancer Ingress: 192.168.0.201 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 31314/TCP Endpoints: 192.168.204.231:80 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 15m metallb-controller Assigned IP \" 192.168.0.201\" Normal nodeAssigned 15m metallb-speaker announcing from node \"banks.local\" Tear down Remove the deployment and example service with kubectl delete -f ~/metallb/whoami.yaml","title":"Load Balancer"},{"location":"01.infrastructure/02.kubernetes/02.metallb/#sample-deployment","text":"We'll now deploy a simple whoami container and tell MetalLB to give it an external IP address from the cluster internal network. Further information on Service objects in kubernetes can be found in the official docs , specifically the Publishing Services (ServiceTypes) section. # file: ~/metallb/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : type : LoadBalancer selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Run kubectl apply -f ~/metallb/whoami.yaml to create the deployment and associated service. You should now be able to see a service whoami of type LoadBalancer being created and having an EXTERNAL-IP allocated to it by MetalLB. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 13h whoami LoadBalancer 10 .96.99.249 192 .168.0.201 80 :31314/TCP 8m47s You can query this as well with curl to confirm. $ curl 192 .168.0.201 Hostname: whoami-deployment-5b4bb9c787-fmb77 IP: 127 .0.0.1 IP: 192 .168.204.231 RemoteAddr: 192 .168.0.102:20516 GET / HTTP/1.1 Host: 192 .168.0.201 User-Agent: curl/7.58.0 Accept: */* The interesting pieces above are the IPs in the hops. IP: 127.0.0.1 : localhost IP: 192.168.204.231 : an address on the subnet of the Calico CNI tunl0 . You could access the pod directly using this as well if on the same network (pod to pod communication). RemoteAddr: 192.168.0.102:20516 : the client making the request (us, on the master node using curl). Host: 192.168.0.201 : the address allocated to the service by MetalLB, now accessible from anyone on the LAN. You can see some of this by running a describe on the pod with kubectl describe pod -l 'app=whoami' . This will show both the Node and assigned IP as well as the associated CNI annotation that allows Calico to route packets. Name : whoami-deployment-5b4bb9c787-fmb77 Namespace : default Priority : 0 Node : banks.local/172.17.0.1 Start Time : Thu, 12 Dec 2019 11:49:16 +0000 Labels : app=whoami pod-template-hash=5b4bb9c787 Annotations : cni.projectcalico.org/podIP : 192.168.204.231/32 Status : Running IP : 192.168.204.231 IPs : IP : 192.168.204.231 Controlled By : ReplicaSet/whoami-deployment-5b4bb9c787 Containers : whoami : Container ID : docker://5966c73d17c87df726b83db12a8227008dcfbccd4b8bd5cd0fda48bca153f415 Image : containous/whoami Image ID : docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b Port : 80/TCP Host Port : 0/TCP State : Running Started : Thu, 12 Dec 2019 11:49:19 +0000 Ready : True Restart Count : 0 Environment : <none> Mounts : /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro) Conditions : Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes : default-token-k9gvj : Type : Secret (a volume populated by a Secret) SecretName : default-token-k9gvj Optional : false QoS Class : BestEffort Node-Selectors : <none> Tolerations : node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 10m default-scheduler Successfully assigned default/whoami-deployment-5b4bb9c787-fmb77 to banks.local Normal Pulling 10m kubelet, banks.local Pulling image \"containous/whoami\" Normal Pulled 10m kubelet, banks.local Successfully pulled image \"containous/whoami\" Normal Created 10m kubelet, banks.local Created container whoami Normal Started 10m kubelet, banks.local Started container whoami Reviewing the service with kubectl describe svc whoami gives similarly useful information. Name : whoami Namespace : default Labels : <none> Annotations : kubectl.kubernetes.io/last-applied-configuration : { \"apiVersion\" : \"v1\" , \"kind\" : \"Service\" , \"metadata\" :{ \"annotations\" :{}, \"name\" : \"whoami\" , \"namespace\" : \"default\" }, \"spec\" :{ \"ports\" :[{ \"port\" : 80 , \"proto... Selector: app=whoami Type: LoadBalancer IP: 10.96.99.249 LoadBalancer Ingress: 192.168.0.201 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 31314/TCP Endpoints: 192.168.204.231:80 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 15m metallb-controller Assigned IP \" 192.168.0.201\" Normal nodeAssigned 15m metallb-speaker announcing from node \"banks.local\"","title":"Sample deployment"},{"location":"01.infrastructure/02.kubernetes/02.metallb/#tear-down","text":"Remove the deployment and example service with kubectl delete -f ~/metallb/whoami.yaml","title":"Tear down"},{"location":"01.infrastructure/02.kubernetes/03.ingress/","text":"As described in the official kubernetes docs an Ingress object manages external access to services in a cluster, typically HTTP. They can provide load balancing, SSL termination and name-based virtual hosting. The easiest way to get setup with one is via Helm and we'll use the Nginx ingress for the moment. Install helm # Live on the edge curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Add the stable repo helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Now install the ingress. helm install my-ingress stable/nginx-ingress \\ --set controller.kind = DaemonSet \\ --set controller.service.type = LoadBalancer \\ --set controller.hostNetwork = true To check that all pods are running. $ kubectl get pod -l 'app=nginx-ingress' NAME READY STATUS RESTARTS AGE my-ingress-nginx-ingress-controller-n7mt6 1 /1 Running 0 3m17s my-ingress-nginx-ingress-default-backend-7469774fb6-wlxjh 1 /1 Running 0 3m17s As we deployed the ingress with both DaemonSet and hostNetwork=true these controllers will exist on every node in the cluster and be listening on the host ports of 80 and 443 . Editing one will show this. $ kubectl edit pod my-ingress-nginx-ingress-controller-n7mt6 ... affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchFields: - key: metadata.name operator: In values: - banks.local ... ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP - containerPort: 443 hostPort: 443 name: https protocol: TCP Deploy sample application Let's try to expose an application now via this ingress controlller. mkdir -p ~/ingress ; \\ cd ~/ingress First, create the deployment and associated service. Notice how we don't use LoadBalancer as the type. # file: ~/ingress/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : # type: LoadBalancer selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Apply with kubectl apply -f ~/ingress/whoami.yaml . We can now configure the ingress controller to route requests to this service. We'll use localhost for the moment to avoid any DNS setting requirements and just prove the routing works. As you can see from the comment this should change to the fqdn of the service ultimately. # file: ~/ingress/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami-ingress namespace : default annotations : kubernete.io/ingress.class : nginx spec : rules : - host : localhost # whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 Apply with kubectl apply -f ~/ingress/whoami-ingress.yaml . Now, running a curl from the node should give you something similar to the following. $ curl localhost Hostname: whoami-deployment-5b4bb9c787-zsv2h IP: 127 .0.0.1 IP: 10 .244.204.200 RemoteAddr: 192 .168.0.104:45478 GET / HTTP/1.1 Host: localhost User-Agent: curl/7.58.0 Accept: */* X-Forwarded-For: 127 .0.0.1 X-Forwarded-Host: localhost X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 127 .0.0.1 X-Request-Id: 678701531703cebe524d5803dd28d08b X-Scheme: http Teardown Remove the ingress resource and whoami application. kubectl delete -f ~/ingress/whoami-ingress.yaml ; \\ kubectl delete -f ~/ingress/whoami.yaml","title":"Ingress"},{"location":"01.infrastructure/02.kubernetes/03.ingress/#deploy-sample-application","text":"Let's try to expose an application now via this ingress controlller. mkdir -p ~/ingress ; \\ cd ~/ingress First, create the deployment and associated service. Notice how we don't use LoadBalancer as the type. # file: ~/ingress/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : # type: LoadBalancer selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Apply with kubectl apply -f ~/ingress/whoami.yaml . We can now configure the ingress controller to route requests to this service. We'll use localhost for the moment to avoid any DNS setting requirements and just prove the routing works. As you can see from the comment this should change to the fqdn of the service ultimately. # file: ~/ingress/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami-ingress namespace : default annotations : kubernete.io/ingress.class : nginx spec : rules : - host : localhost # whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 Apply with kubectl apply -f ~/ingress/whoami-ingress.yaml . Now, running a curl from the node should give you something similar to the following. $ curl localhost Hostname: whoami-deployment-5b4bb9c787-zsv2h IP: 127 .0.0.1 IP: 10 .244.204.200 RemoteAddr: 192 .168.0.104:45478 GET / HTTP/1.1 Host: localhost User-Agent: curl/7.58.0 Accept: */* X-Forwarded-For: 127 .0.0.1 X-Forwarded-Host: localhost X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 127 .0.0.1 X-Request-Id: 678701531703cebe524d5803dd28d08b X-Scheme: http","title":"Deploy sample application"},{"location":"01.infrastructure/02.kubernetes/03.ingress/#teardown","text":"Remove the ingress resource and whoami application. kubectl delete -f ~/ingress/whoami-ingress.yaml ; \\ kubectl delete -f ~/ingress/whoami.yaml","title":"Teardown"},{"location":"01.infrastructure/03.certificates/00.cert.manager/","text":"We're going to use the OpenSource Cert-Manager from JetStack to automate TLS within the cluster. cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources. It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry. The quickstart on Kubernetes guide is a good place to start. Wherever possible we'll look to use the same proposed default settings. Install with regular manifests With helm currently going through some significant changes with the move from v2 to v3 we'll stick with deploying cert-manager with regular manifests for now. Now download and apply the latest manifest. mkdir ~/cert-manager ; \\ cd ~/cert-manager export CERT_MANAGER_VERSION = v0.12.0 ; \\ wget https://github.com/jetstack/cert-manager/releases/download/ ${ CERT_MANAGER_VERSION } /cert-manager.yaml ; kubectl create -f ~/cert-manager/cert-manager.yaml Verify the Installation $ kubectl -n cert-manager get pods NAME READY STATUS RESTARTS AGE cert-manager-5c47f46f57-ldb5v 1 /1 Running 0 39s cert-manager-cainjector-6659d6844d-25mqs 1 /1 Running 0 39s cert-manager-webhook-547567b88f-kvw28 1 /1 Running 0 39s We should have three pods running cert-manager , cert-manager-cainjector , and cert-manager-webhook . DNS With an ingress now available, login to your DNS provider (in my case Cloudflare) and point an A record for whoami to the IP address of the first node. Wildcard DNS An alternative option (which is what I've gone for below) is to point your root A record at the IP address (e.g. jamesveitch.dev) and then add a * wildcard CNAME entry which points at the root . This way any arbitrary subdomain (e.g. myapp.jamesveitch.dev) that isn't specifically found as a standalone entry will route straight to wherever the root is pointed. The disadvantage of this is that anything not specifically highlighted as a standalone entry will not be proxied through their CDN (see the cloud). We will fix this later though with ExternalDNS on Kubernetes. For now it's fine. NB: I'm using a CNAME above for the root which points to an anondns.net address. This is so I can use a dynamic IP from home. We'll replace this with an A record and static IP later when we turn on the cloud node. If you want to stick with a single node at home though you can use my AnonDNS updater docker image to keep your home IP registered for free with anondns. We'll now deploy a simple whoami container and tell the ingress node how to route to it. # file: ~/cert-manager/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Create the container deployment and service with kubectl create -f ~/cert-manager/whoami.yaml . In order to route traffic to this container though we now need to create an ingress resource. # file: ~/cert-manager/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami annotations : kubernete.io/ingress.class : nginx spec : rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 Apply this with kubectl apply -f ~/cert-manager/whoami-ingress.yaml . (Note we use apply instead of create so we can edit for the TLS later). To check this is working we can curl the address. $ curl whoami.jamesveitch.dev Hostname: whoami-deployment-5b4bb9c787-xfkm2 IP: 127 .0.0.1 IP: 10 .244.0.104 RemoteAddr: 192 .168.0.99:19850 GET / HTTP/1.1 Host: whoami.jamesveitch.dev User-Agent: curl/7.58.0 Accept: */* X-Forwarded-For: 82 .19.212.223 X-Forwarded-Host: whoami.jamesveitch.dev X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 82 .19.212.223 X-Request-Id: 0747f4ec40d196f54151e48014f50383 X-Scheme: http Cloudflare CDN If you've used a manually created entry for whoami (as opposed to the wildcard) you can click the cloud icon in the Cloudflare DNS page to use their proxy/CDN. This will give you a slightly different output to the above with a couple of additional header keys indicating you're using their CDN. ... Cdn-Loop: cloudflare ... X-Original-Forwarded-For: 82 .19.222.223 Configuring an Issuer As per the official architecture diagram below Kubernetes has the concept of Issuers . Once issued, certificates are then stored in Kubernetes secrets. We're particularly interested in LetsEncrypt and Vault . We'll start off using the http challenge (which is generic for all providers) and then, later, move to dns for Cloudflare . # file: ~/cert-manager/letsencrypt-staging.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL and email address for ACME registration server : https://acme-staging-v02.api.letsencrypt.org/directory email : lol@cats.com # Name of the secret to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging-key solvers : # Enable HTTP01 validations - http01 : ingress : class : nginx kubectl create -f ~/cert-manager/letsencrypt-staging.yaml Obtaining a Certificate for an app We now need to create the equivalent of a certificate request for the whoami container. The highlighted line shows we're asking CertManager to use the staging issuer we configured earlier. We need to specify it's a cluster issuer (as opposed to a local namespace). # file: ~/cert-manager/whoami-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : whoami namespace : default spec : # Secret names are always required. secretName : whoami-jamesveitch-dev-tls duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : whoami.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - whoami.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt-staging # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io Request the certificate with a kubectl apply -f ~/cert-manager/whoami-certificate.yaml and then check it's been obtained. $ kubectl get certificates NAME READY SECRET AGE whoami True whoami-jamesveitch-dev-tls 28s We'll modify the ingress now to uncomment the following lines so that nginx knows to use the ssl cert. We'll also (optional) provide multiple routes to connect to the application via: whoami.jamesveitch.dev (using the CDN) as a proxied DNS record; or jamesveitch.dev/whoami (using the apex). # file: ~/cert-manager/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami annotations : kubernete.io/ingress.class : nginx nginx.ingress.kubernetes.io/rewrite-target : / cert-manager.io/issuer : \"letsencrypt-staging\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - secretName : whoami-jamesveitch-dev-tls hosts : - whoami.jamesveitch.dev rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 - host : jamesveitch.dev http : paths : - path : /whoami backend : serviceName : whoami servicePort : 80 Then apply these changes with a kubectl apply -f ~/cert-manager/whoami-ingress.yaml You may need to clear your cache but, navigating to the website should now redirect you automatically to https and present you with a LetsEncrypt certificate (albeit an untrusted one from the staging server) as opposed to the previous Kubernetes Ingress Controller Fake Certificate . # basic http gets a permanent redirect to https $ curl whoami.jamesveitch.dev <html> <head><title>308 Permanent Redirect</title></head> <body> <center><h1>308 Permanent Redirect</h1></center> <hr><center>openresty/1.15.8.2</center> </body> </html> Cloudflare CDN and too many redirects If you use the cloudflare proxy/cdn in your DNS then you can run into a too many redirects issue as a result of your TLS/SSL settings. This is because unencrypted traffic will enter the CDN; bounce around a number of nodes; and then get redirected straight back again through the same process to be encrypted. This need to be Full at a minimum. Once you're using production LetsEncrypt you could upgrade this further to Full (strict) if wanted. As with most things in life there's a good StackOverflow answer that explains in a bit more detail. Off : No visitors will be able to view your site over HTTPS; they will be redirected to HTTP. Flexible SSL : You cannot configure HTTPS support on your origin, even with a certificate that is not valid for your site. Visitors will be able to access your site over HTTPS, but connections to your origin will be made over HTTP. Note: You may encounter a redirect loop with some origin configurations. Full SSL : Your origin supports HTTPS, but the certificate installed does not match your domain or is self-signed. Cloudflare will connect to your origin over HTTPS, but will not validate the certificate. Full (strict) : Your origin has a valid certificate (not expired and signed by a trusted CA or Cloudflare Origin CA) installed. Cloudflare will connect over HTTPS and verify the cert on each request. # show https $ curl --insecure https://whoami.jamesveitch.dev Hostname: whoami-deployment-5b4bb9c787-bspd6 IP: 127 .0.0.1 IP: 10 .244.204.204 RemoteAddr: 192 .168.0.104:45694 GET / HTTP/1.1 Host: whoami.jamesveitch.dev User-Agent: curl/7.58.0 Accept: */* Accept-Encoding: gzip Cdn-Loop: cloudflare Cf-Connecting-Ip: 82 .19.222.223 Cf-Ipcountry: GB Cf-Ray: 5479b0729dc3ce53-LHR Cf-Visitor: { \"scheme\" : \"https\" } X-Forwarded-For: 162 .158.154.251 X-Forwarded-Host: whoami.jamesveitch.dev X-Forwarded-Port: 443 X-Forwarded-Proto: https X-Original-Forwarded-For: 82 .19.222.223 X-Real-Ip: 162 .158.154.251 X-Request-Id: 226c8aebc0f065303e9ece36b1433e5b X-Scheme: https LetsEncrypt Production Server With everything now setup and working we'll replace our staging implementation of LetsEncrypt with their production service such that our certificates are trusted by default in browsers. We need to create a new Issuer for LetsEncrypt and then change both the cert-request and ingress configurations. # file: ~/cert-manager/letsencrypt.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt namespace : default spec : acme : # The ACME server URL and email address for ACME registration server : https://acme-v02.api.letsencrypt.org/directory email : lol@cats.com # Name of the secret to store the ACME account private key privateKeySecretRef : name : letsencrypt-key solvers : # Enable HTTP01 validations - http01 : ingress : class : nginx kubectl create -f ~/cert-manager/letsencrypt.yaml We'll now need to modify the original certificate request to use the production service and then delete the staging certificate so it can be recreated. # file: ~/cert-manager/whoami-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : whoami namespace : default spec : # Secret names are always required. secretName : whoami-jamesveitch-dev-tls duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : whoami.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - whoami.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io kubectl delete certificate whoami ; \\ kubectl apply -f ~/cert-manager/whoami-certificate.yaml ; \\ watch -n 1 kubectl get certificate whoami NB: If you're using the Cloudflare CDN you might see their certificate instead.","title":"TLS Certificates Management"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#install-with-regular-manifests","text":"With helm currently going through some significant changes with the move from v2 to v3 we'll stick with deploying cert-manager with regular manifests for now. Now download and apply the latest manifest. mkdir ~/cert-manager ; \\ cd ~/cert-manager export CERT_MANAGER_VERSION = v0.12.0 ; \\ wget https://github.com/jetstack/cert-manager/releases/download/ ${ CERT_MANAGER_VERSION } /cert-manager.yaml ; kubectl create -f ~/cert-manager/cert-manager.yaml","title":"Install with regular manifests"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#verify-the-installation","text":"$ kubectl -n cert-manager get pods NAME READY STATUS RESTARTS AGE cert-manager-5c47f46f57-ldb5v 1 /1 Running 0 39s cert-manager-cainjector-6659d6844d-25mqs 1 /1 Running 0 39s cert-manager-webhook-547567b88f-kvw28 1 /1 Running 0 39s We should have three pods running cert-manager , cert-manager-cainjector , and cert-manager-webhook .","title":"Verify the Installation"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#dns","text":"With an ingress now available, login to your DNS provider (in my case Cloudflare) and point an A record for whoami to the IP address of the first node. Wildcard DNS An alternative option (which is what I've gone for below) is to point your root A record at the IP address (e.g. jamesveitch.dev) and then add a * wildcard CNAME entry which points at the root . This way any arbitrary subdomain (e.g. myapp.jamesveitch.dev) that isn't specifically found as a standalone entry will route straight to wherever the root is pointed. The disadvantage of this is that anything not specifically highlighted as a standalone entry will not be proxied through their CDN (see the cloud). We will fix this later though with ExternalDNS on Kubernetes. For now it's fine. NB: I'm using a CNAME above for the root which points to an anondns.net address. This is so I can use a dynamic IP from home. We'll replace this with an A record and static IP later when we turn on the cloud node. If you want to stick with a single node at home though you can use my AnonDNS updater docker image to keep your home IP registered for free with anondns. We'll now deploy a simple whoami container and tell the ingress node how to route to it. # file: ~/cert-manager/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : whoami spec : selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 Create the container deployment and service with kubectl create -f ~/cert-manager/whoami.yaml . In order to route traffic to this container though we now need to create an ingress resource. # file: ~/cert-manager/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami annotations : kubernete.io/ingress.class : nginx spec : rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 Apply this with kubectl apply -f ~/cert-manager/whoami-ingress.yaml . (Note we use apply instead of create so we can edit for the TLS later). To check this is working we can curl the address. $ curl whoami.jamesveitch.dev Hostname: whoami-deployment-5b4bb9c787-xfkm2 IP: 127 .0.0.1 IP: 10 .244.0.104 RemoteAddr: 192 .168.0.99:19850 GET / HTTP/1.1 Host: whoami.jamesveitch.dev User-Agent: curl/7.58.0 Accept: */* X-Forwarded-For: 82 .19.212.223 X-Forwarded-Host: whoami.jamesveitch.dev X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 82 .19.212.223 X-Request-Id: 0747f4ec40d196f54151e48014f50383 X-Scheme: http Cloudflare CDN If you've used a manually created entry for whoami (as opposed to the wildcard) you can click the cloud icon in the Cloudflare DNS page to use their proxy/CDN. This will give you a slightly different output to the above with a couple of additional header keys indicating you're using their CDN. ... Cdn-Loop: cloudflare ... X-Original-Forwarded-For: 82 .19.222.223","title":"DNS"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#configuring-an-issuer","text":"As per the official architecture diagram below Kubernetes has the concept of Issuers . Once issued, certificates are then stored in Kubernetes secrets. We're particularly interested in LetsEncrypt and Vault . We'll start off using the http challenge (which is generic for all providers) and then, later, move to dns for Cloudflare . # file: ~/cert-manager/letsencrypt-staging.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL and email address for ACME registration server : https://acme-staging-v02.api.letsencrypt.org/directory email : lol@cats.com # Name of the secret to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging-key solvers : # Enable HTTP01 validations - http01 : ingress : class : nginx kubectl create -f ~/cert-manager/letsencrypt-staging.yaml","title":"Configuring an Issuer"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#obtaining-a-certificate-for-an-app","text":"We now need to create the equivalent of a certificate request for the whoami container. The highlighted line shows we're asking CertManager to use the staging issuer we configured earlier. We need to specify it's a cluster issuer (as opposed to a local namespace). # file: ~/cert-manager/whoami-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : whoami namespace : default spec : # Secret names are always required. secretName : whoami-jamesveitch-dev-tls duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : whoami.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - whoami.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt-staging # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io Request the certificate with a kubectl apply -f ~/cert-manager/whoami-certificate.yaml and then check it's been obtained. $ kubectl get certificates NAME READY SECRET AGE whoami True whoami-jamesveitch-dev-tls 28s We'll modify the ingress now to uncomment the following lines so that nginx knows to use the ssl cert. We'll also (optional) provide multiple routes to connect to the application via: whoami.jamesveitch.dev (using the CDN) as a proxied DNS record; or jamesveitch.dev/whoami (using the apex). # file: ~/cert-manager/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : whoami annotations : kubernete.io/ingress.class : nginx nginx.ingress.kubernetes.io/rewrite-target : / cert-manager.io/issuer : \"letsencrypt-staging\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - secretName : whoami-jamesveitch-dev-tls hosts : - whoami.jamesveitch.dev rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 - host : jamesveitch.dev http : paths : - path : /whoami backend : serviceName : whoami servicePort : 80 Then apply these changes with a kubectl apply -f ~/cert-manager/whoami-ingress.yaml You may need to clear your cache but, navigating to the website should now redirect you automatically to https and present you with a LetsEncrypt certificate (albeit an untrusted one from the staging server) as opposed to the previous Kubernetes Ingress Controller Fake Certificate . # basic http gets a permanent redirect to https $ curl whoami.jamesveitch.dev <html> <head><title>308 Permanent Redirect</title></head> <body> <center><h1>308 Permanent Redirect</h1></center> <hr><center>openresty/1.15.8.2</center> </body> </html> Cloudflare CDN and too many redirects If you use the cloudflare proxy/cdn in your DNS then you can run into a too many redirects issue as a result of your TLS/SSL settings. This is because unencrypted traffic will enter the CDN; bounce around a number of nodes; and then get redirected straight back again through the same process to be encrypted. This need to be Full at a minimum. Once you're using production LetsEncrypt you could upgrade this further to Full (strict) if wanted. As with most things in life there's a good StackOverflow answer that explains in a bit more detail. Off : No visitors will be able to view your site over HTTPS; they will be redirected to HTTP. Flexible SSL : You cannot configure HTTPS support on your origin, even with a certificate that is not valid for your site. Visitors will be able to access your site over HTTPS, but connections to your origin will be made over HTTP. Note: You may encounter a redirect loop with some origin configurations. Full SSL : Your origin supports HTTPS, but the certificate installed does not match your domain or is self-signed. Cloudflare will connect to your origin over HTTPS, but will not validate the certificate. Full (strict) : Your origin has a valid certificate (not expired and signed by a trusted CA or Cloudflare Origin CA) installed. Cloudflare will connect over HTTPS and verify the cert on each request. # show https $ curl --insecure https://whoami.jamesveitch.dev Hostname: whoami-deployment-5b4bb9c787-bspd6 IP: 127 .0.0.1 IP: 10 .244.204.204 RemoteAddr: 192 .168.0.104:45694 GET / HTTP/1.1 Host: whoami.jamesveitch.dev User-Agent: curl/7.58.0 Accept: */* Accept-Encoding: gzip Cdn-Loop: cloudflare Cf-Connecting-Ip: 82 .19.222.223 Cf-Ipcountry: GB Cf-Ray: 5479b0729dc3ce53-LHR Cf-Visitor: { \"scheme\" : \"https\" } X-Forwarded-For: 162 .158.154.251 X-Forwarded-Host: whoami.jamesveitch.dev X-Forwarded-Port: 443 X-Forwarded-Proto: https X-Original-Forwarded-For: 82 .19.222.223 X-Real-Ip: 162 .158.154.251 X-Request-Id: 226c8aebc0f065303e9ece36b1433e5b X-Scheme: https","title":"Obtaining a Certificate for an app"},{"location":"01.infrastructure/03.certificates/00.cert.manager/#letsencrypt-production-server","text":"With everything now setup and working we'll replace our staging implementation of LetsEncrypt with their production service such that our certificates are trusted by default in browsers. We need to create a new Issuer for LetsEncrypt and then change both the cert-request and ingress configurations. # file: ~/cert-manager/letsencrypt.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt namespace : default spec : acme : # The ACME server URL and email address for ACME registration server : https://acme-v02.api.letsencrypt.org/directory email : lol@cats.com # Name of the secret to store the ACME account private key privateKeySecretRef : name : letsencrypt-key solvers : # Enable HTTP01 validations - http01 : ingress : class : nginx kubectl create -f ~/cert-manager/letsencrypt.yaml We'll now need to modify the original certificate request to use the production service and then delete the staging certificate so it can be recreated. # file: ~/cert-manager/whoami-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : whoami namespace : default spec : # Secret names are always required. secretName : whoami-jamesveitch-dev-tls duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : whoami.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - whoami.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io kubectl delete certificate whoami ; \\ kubectl apply -f ~/cert-manager/whoami-certificate.yaml ; \\ watch -n 1 kubectl get certificate whoami NB: If you're using the Cloudflare CDN you might see their certificate instead.","title":"LetsEncrypt Production Server"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/","text":"In part 2 we installed Kubernetes and setup a user (in my case adminlocal ) on our cluster with the ability to run administrative kubernetes commands. etcd (key/value store), Rook, Promethues and Vault are all examples of technologies we will be using in our cluster and are deployed using Kubernetes Operators . In this section we'll be deploying the Rook storage orchestrator with Ceph as a storage provider. Components Rook Rook allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider. More information on the architecure can be found in the docs Ceph Ceph provides three types of storage: object : compatible with S3 API file : files and directories (incl. NFS); and block : replicate a hard drive There a 4 key components of the architecture to be aware of (shown above in the Rook diagram): Monitor (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication Manager daemon: keeps track of state in cluster and metrics OSDs (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage. MSDs (Metatdata Server): concerned with filesystem storage type only Under the hood everything is stored as an object in logical storage pools. Installation and Setup Deployment of the Rook operator See the Rook quickstart We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple mon components onto a single node. Similar to when we removed the the control plane node taint Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes). # create a working directory mkdir -p ~/rook && \\ cd ~/rook # download the sample files # all of these can be found here: https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/common.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/operator.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/cluster.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/toolbox.yaml # modify the cluster spec # - allow multiple mons per node sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml In addition, because we've setup our encrypted data storage to be mounted at /data/${BLKID} we will edit the default storage options to remove the useAllDevices selection and, instead, specify the directories. See the docs for more details. # set default storage location and remove default `useAllDevices: true` sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml # any devices starting with 'sd' (but not sda as that's our root filesystem) sed -i.bak 's/deviceFilter:/deviceFilter: ^sd[^a]/' cluster.yaml # encrypt them with LUKS # see conversation https://github.com/rook/rook/issues/923#issuecomment-557651052 sed -i.bak 's/# encryptedDevice: \"true\"/encryptedDevice: \"true\"/' cluster.yaml Wiping disks for usage Because Rook will fail if it finds an existing \"in use\" filesystem or disk we need to wipe the disks in the host which we want to use. For example. In use disks below (from a previous cluster). in use $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk \u2514\u2500ceph--04176411--2495--4944--abae--4ac6cedd8b46-osd--data--20a17864--c8d4--48f7--8aa0--e9d393fbb9e4 253 :1 0 1 .8T 0 lvm sdc 8 :32 0 1 .8T 0 disk \u2514\u2500ceph--38ed1893--7746--477a--a8c8--dae66f8360e5-osd--data--0940f2cf--d1c2--489f--9c8a--31d75d759be8 253 :0 0 1 .8T 0 lvm sdd 8 :48 0 2T 0 disk sde 8 :64 0 2T 0 disk sr0 11 :0 1 1024M 0 rom We can delete and wipe the disk of partition maps. # replicate the rook commands (and use regex to exclude sda like the manifest) export DISKS = $( lsblk --all --noheadings --list --output KNAME | grep sd [ ^a ] ) ; \\ for d in $DISKS ; do \\ export DISK = /dev/ $d ; \\ sudo wipefs $DISK ; \\ sudo vgremove -y $( sudo pvscan | grep $DISK | awk '{print $4}' ) ; \\ sudo dd if = /dev/zero of = $DISK bs = 512 count = 10 ; \\ done Now, if appropriate, remove the LVM with the appropriate combination of pvremove , lvremove , vgremove etc. $ sudo vgremove -y $( sudo pvscan | grep $DISK | awk '{print $4}' ) Logical volume \"osd-data-20a17864-c8d4-48f7-8aa0-e9d393fbb9e4\" successfully removed Volume group \"ceph-04176411-2495-4944-abae-4ac6cedd8b46\" successfully removed With these configurations now downloaded we'll apply them in the following order. kubectl create -f ~/rook/common.yaml ; \\ kubectl create -f ~/rook/operator.yaml Verify the rook-ceph-operator is in the Running state Use kubectl -n rook-ceph get pod to check we have a running state. root@banks:~# kubectl -n rook-ceph get pod NAME READY STATUS RESTARTS AGE ... rook-ceph-operator-c8ff6447d-tbh5c 1 /1 Running 0 6m18s Create the Rook cluster Assuming the operator looks ok we can now create the cluster kubectl create -f ~/rook/cluster.yaml To verify the state of the cluster we will connect to the Rook Toolbox kubectl create -f ~/rook/toolbox.yaml Wait for the toolbox pod to enter a running state: kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" Once the rook-ceph-tools pod is running, you can connect to it with: kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash When inside the toolbox run ceph status after setting a custom prompt so we don't forget where we are. export PS1 = \"ceph-toolbox# \" ceph status [ root@banks / ] # ceph status cluster: id: 06da5ebc-d2f3-4366-a51c-db759d8bc664 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c ( age 2m ) mgr: a ( active, since 102s ) osd: 2 osds: 2 up ( since 33s ) , 2 in ( since 33s ) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2 .0 GiB used, 3 .6 TiB / 3 .6 TiB avail pgs: All mons should be in quorum A mgr should be active At least one OSD should be active If the health is not HEALTH_OK, the warnings or errors should be investigated Toubleshooting: Not all OSDs (disks) are created The task to prepare a disk can vary in duration based on it's size and a number of other factors. Start off by checking that the prepare has actually finished. $ watch kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-banks.local-dlvk7 0/1 Completed 0 2m31s If this doesn't show Completed then it's still performing the tasks. Wait for it to complete and then go back into the toolbox and check the ceph status output again. Troubleshooting: [errno 2] NB: You might get an error unable to get monitor info from DNS SRV with service name: ceph-mon or [errno 2] error connecting to the cluster when running ceph status in the toolbox if you've typed all of the above commands very quickly. This is usually because the cluster is still starting and waiting for all the monitors to come up and establish connections. Go get a cup of tea / wait a couple of minutes and try again. In the cluster.yaml spec the default number of mon instances is 3 . As a result if you don't have three of these pods running then your cluster is still initialising. You can run kubectl -n rook-ceph logs -l \"app=rook-ceph-operator\" to see an output of the logs from the operator and search for mons running . As you can see below it took mine around a minute to initialise all 3. To see what monitors you have run kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" . $ kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" NAME READY STATUS RESTARTS AGE rook-ceph-mon-a-5d677b5849-t4xct 1 /1 Running 0 82s rook-ceph-mon-b-6cfbcf8db4-7cwxp 1 /1 Running 0 66s rook-ceph-mon-c-8f858c585-c9z5b 1 /1 Running 0 50s When you are done with the toolbox, you can remove the deployment: kubectl -n rook-ceph delete deployment rook-ceph-tools If you want to delete the cluster and start again... Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse. There are some additional pointers here in the docs. kubectl delete -f toolbox.yaml ; \\ kubectl delete -f cluster.yaml ; \\ kubectl delete -f operator.yaml ; \\ kubectl delete -f common.yaml ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/* Dashboard and Storage We now have a cluster running but no configured storage or an ability to review status (other than logging into the toolbox).","title":"Install the Cluster"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#components","text":"","title":"Components"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#rook","text":"Rook allows us to use storage systems in a cloud-agnostic way and replicate the feel of a public cloud where you attach a storage volume to a container for application data persistence (e.g. EBS on AWS). We're going to configure it to use Ceph as a storage provider. More information on the architecure can be found in the docs","title":"Rook"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#ceph","text":"Ceph provides three types of storage: object : compatible with S3 API file : files and directories (incl. NFS); and block : replicate a hard drive There a 4 key components of the architecture to be aware of (shown above in the Rook diagram): Monitor (min 3): keeps a map of state in cluster for components to communicate with each other and handles authentication Manager daemon: keeps track of state in cluster and metrics OSDs (Object Storage daemon, min 3): stores the data. These will run on multiple nodes and handle the read/write operations to their underlying storage. MSDs (Metatdata Server): concerned with filesystem storage type only Under the hood everything is stored as an object in logical storage pools.","title":"Ceph"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#deployment-of-the-rook-operator","text":"See the Rook quickstart We're going to download some sample files from the main repo and make a tweak so that we can deploy multiple mon components onto a single node. Similar to when we removed the the control plane node taint Ceph will fail to run otherwise (as it wants a quorum of mons across multiple nodes). # create a working directory mkdir -p ~/rook && \\ cd ~/rook # download the sample files # all of these can be found here: https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/common.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/operator.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/cluster.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/toolbox.yaml # modify the cluster spec # - allow multiple mons per node sed -i.bak 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' cluster.yaml In addition, because we've setup our encrypted data storage to be mounted at /data/${BLKID} we will edit the default storage options to remove the useAllDevices selection and, instead, specify the directories. See the docs for more details. # set default storage location and remove default `useAllDevices: true` sed -i.bak 's/useAllDevices: true/useAllDevices: false/' cluster.yaml # any devices starting with 'sd' (but not sda as that's our root filesystem) sed -i.bak 's/deviceFilter:/deviceFilter: ^sd[^a]/' cluster.yaml # encrypt them with LUKS # see conversation https://github.com/rook/rook/issues/923#issuecomment-557651052 sed -i.bak 's/# encryptedDevice: \"true\"/encryptedDevice: \"true\"/' cluster.yaml Wiping disks for usage Because Rook will fail if it finds an existing \"in use\" filesystem or disk we need to wipe the disks in the host which we want to use. For example. In use disks below (from a previous cluster). in use $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk \u2514\u2500ceph--04176411--2495--4944--abae--4ac6cedd8b46-osd--data--20a17864--c8d4--48f7--8aa0--e9d393fbb9e4 253 :1 0 1 .8T 0 lvm sdc 8 :32 0 1 .8T 0 disk \u2514\u2500ceph--38ed1893--7746--477a--a8c8--dae66f8360e5-osd--data--0940f2cf--d1c2--489f--9c8a--31d75d759be8 253 :0 0 1 .8T 0 lvm sdd 8 :48 0 2T 0 disk sde 8 :64 0 2T 0 disk sr0 11 :0 1 1024M 0 rom We can delete and wipe the disk of partition maps. # replicate the rook commands (and use regex to exclude sda like the manifest) export DISKS = $( lsblk --all --noheadings --list --output KNAME | grep sd [ ^a ] ) ; \\ for d in $DISKS ; do \\ export DISK = /dev/ $d ; \\ sudo wipefs $DISK ; \\ sudo vgremove -y $( sudo pvscan | grep $DISK | awk '{print $4}' ) ; \\ sudo dd if = /dev/zero of = $DISK bs = 512 count = 10 ; \\ done Now, if appropriate, remove the LVM with the appropriate combination of pvremove , lvremove , vgremove etc. $ sudo vgremove -y $( sudo pvscan | grep $DISK | awk '{print $4}' ) Logical volume \"osd-data-20a17864-c8d4-48f7-8aa0-e9d393fbb9e4\" successfully removed Volume group \"ceph-04176411-2495-4944-abae-4ac6cedd8b46\" successfully removed With these configurations now downloaded we'll apply them in the following order. kubectl create -f ~/rook/common.yaml ; \\ kubectl create -f ~/rook/operator.yaml Verify the rook-ceph-operator is in the Running state Use kubectl -n rook-ceph get pod to check we have a running state. root@banks:~# kubectl -n rook-ceph get pod NAME READY STATUS RESTARTS AGE ... rook-ceph-operator-c8ff6447d-tbh5c 1 /1 Running 0 6m18s","title":"Deployment of the Rook operator"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#create-the-rook-cluster","text":"Assuming the operator looks ok we can now create the cluster kubectl create -f ~/rook/cluster.yaml To verify the state of the cluster we will connect to the Rook Toolbox kubectl create -f ~/rook/toolbox.yaml Wait for the toolbox pod to enter a running state: kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" Once the rook-ceph-tools pod is running, you can connect to it with: kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash When inside the toolbox run ceph status after setting a custom prompt so we don't forget where we are. export PS1 = \"ceph-toolbox# \" ceph status [ root@banks / ] # ceph status cluster: id: 06da5ebc-d2f3-4366-a51c-db759d8bc664 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c ( age 2m ) mgr: a ( active, since 102s ) osd: 2 osds: 2 up ( since 33s ) , 2 in ( since 33s ) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2 .0 GiB used, 3 .6 TiB / 3 .6 TiB avail pgs: All mons should be in quorum A mgr should be active At least one OSD should be active If the health is not HEALTH_OK, the warnings or errors should be investigated","title":"Create the Rook cluster"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#toubleshooting-not-all-osds-disks-are-created","text":"The task to prepare a disk can vary in duration based on it's size and a number of other factors. Start off by checking that the prepare has actually finished. $ watch kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-banks.local-dlvk7 0/1 Completed 0 2m31s If this doesn't show Completed then it's still performing the tasks. Wait for it to complete and then go back into the toolbox and check the ceph status output again.","title":"Toubleshooting: Not all OSDs (disks) are created"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#troubleshooting-errno-2","text":"NB: You might get an error unable to get monitor info from DNS SRV with service name: ceph-mon or [errno 2] error connecting to the cluster when running ceph status in the toolbox if you've typed all of the above commands very quickly. This is usually because the cluster is still starting and waiting for all the monitors to come up and establish connections. Go get a cup of tea / wait a couple of minutes and try again. In the cluster.yaml spec the default number of mon instances is 3 . As a result if you don't have three of these pods running then your cluster is still initialising. You can run kubectl -n rook-ceph logs -l \"app=rook-ceph-operator\" to see an output of the logs from the operator and search for mons running . As you can see below it took mine around a minute to initialise all 3. To see what monitors you have run kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" . $ kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" NAME READY STATUS RESTARTS AGE rook-ceph-mon-a-5d677b5849-t4xct 1 /1 Running 0 82s rook-ceph-mon-b-6cfbcf8db4-7cwxp 1 /1 Running 0 66s rook-ceph-mon-c-8f858c585-c9z5b 1 /1 Running 0 50s When you are done with the toolbox, you can remove the deployment: kubectl -n rook-ceph delete deployment rook-ceph-tools If you want to delete the cluster and start again... Obviously everything worked first time... But, if it didn't, you can always delete everything and start again with the following commands. Essentially undoing what we applied in the yaml configs earlier in reverse. There are some additional pointers here in the docs. kubectl delete -f toolbox.yaml ; \\ kubectl delete -f cluster.yaml ; \\ kubectl delete -f operator.yaml ; \\ kubectl delete -f common.yaml ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Troubleshooting: [errno 2]"},{"location":"01.infrastructure/04.storage/00.setup.ceph.storage.with.rook/#dashboard-and-storage","text":"We now have a cluster running but no configured storage or an ability to review status (other than logging into the toolbox).","title":"Dashboard and Storage"},{"location":"01.infrastructure/04.storage/01.dashboard/","text":"Ceph comes with it's own dashboard for management purposes. Let's check the dashboard status. For full details see the docs root@banks:~/rook# kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-cephfsplugin-metrics ClusterIP 10 .107.29.251 <none> 8080 /TCP,8081/TCP 14m csi-rbdplugin-metrics ClusterIP 10 .102.151.196 <none> 8080 /TCP,8081/TCP 14m rook-ceph-mgr ClusterIP 10 .110.58.77 <none> 9283 /TCP 12m rook-ceph-mgr-dashboard ClusterIP 10 .104.71.31 <none> 8443 /TCP 12m rook-ceph-mon-a ClusterIP 10 .101.139.217 <none> 6789 /TCP,3300/TCP 13m rook-ceph-mon-b ClusterIP 10 .109.239.104 <none> 6789 /TCP,3300/TCP 13m rook-ceph-mon-c ClusterIP 10 .99.14.79 <none> 6789 /TCP,3300/TCP 13m So it looks like our rook-ceph-mgr-dashboard is running on port 8843 . Describing the service indicates it's not yet exposed via a NodePort etc. so we can't access it external to the cluster. root@banks:~/rook# kubectl -n rook-ceph describe services rook-ceph-mgr-dashboard Name: rook-ceph-mgr-dashboard Namespace: rook-ceph Labels: app = rook-ceph-mgr rook_cluster = rook-ceph Annotations: <none> Selector: app = rook-ceph-mgr,rook_cluster = rook-ceph Type: ClusterIP IP: 10 .104.71.31 Port: https-dashboard 8443 /TCP TargetPort: 8443 /TCP Endpoints: 10 .244.0.80:8443 Session Affinity: None Events: <none> You now have options for how to expose this: Ingress NodePort NodePort We'll enable this via a NodePort now. cd ~/rook wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml kubectl create -f ~/rook/dashboard-external-https.yaml You will see the new service rook-ceph-mgr-dashboard-external-https created: $ kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-cephfsplugin-metrics ClusterIP 10 .102.185.98 <none> 8080 /TCP,8081/TCP 18h csi-rbdplugin-metrics ClusterIP 10 .108.114.119 <none> 8080 /TCP,8081/TCP 18h rook-ceph-mgr ClusterIP 10 .107.152.33 <none> 9283 /TCP 18h rook-ceph-mgr-dashboard ClusterIP 10 .105.220.219 <none> 8443 /TCP 18h rook-ceph-mgr-dashboard-external-https NodePort 10 .98.207.115 <none> 8443 :30995/TCP 9s rook-ceph-mon-a ClusterIP 10 .101.90.100 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-b ClusterIP 10 .107.14.178 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-c ClusterIP 10 .101.91.134 <none> 6789 /TCP,3300/TCP 18h In this example, port 30995 will be opened to expose port 8443 from the ceph-mgr pod . Find the ip address of the node or, if you've used Avahi as per the physcial node installation you could probably access the node directly using the hostname (Kubernetes will listen on that fqdn). Finding the IP address So this isn't that obvious... We've exposed our service via a NodePort but now need to find the IP address of the actual node in order to hit the required port to see the service... $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-mgr\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-mgr-a-59cc7fb98-7wfxf 1 /1 Running 0 19h 10 .244.0.112 banks <none> <none> Our service is running on the banks node. Let's find the IP of that then. $ kubectl describe node banks | grep InternalIP InternalIP: 192 .168.0.95 Or you could find the IP address with: kubectl get node banks -o jsonpath = '{.status.addresses[0].address}' Now you can enter the URL in your browser as https://192.168.0.95:30995 or https://banks.local:30995 and the dashboard will appear. Credentials As described in the docs Rook creates a default user named admin and generates a secret called rook-ceph-dashboard-admin-password in the namespace where rook is running. To retrieve the generated password, you can run the following: kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo This is a good thing to see... Ingress Given all the work we went through to give our services proper external access with TLS certificates earlier it would be a shame to waste this...! Let's teardown the NodePort. kubectl delete -f ~/rook/dashboard-external-https.yaml The docs have an example using an ingress controller we will borrow from. Quote If you have a cluster with an nginx Ingress Controller and a Certificate Manager (e.g. cert-manager) then you can create an Ingress like the one below. This example achieves four things: Exposes the dashboard on the Internet (using an reverse proxy) Issues an valid TLS Certificate for the specified domain name (using ACME) Tells the reverse proxy that the dashboard itself uses HTTPS Tells the reverse proxy that the dashboard itself does not have a valid certificate (it is self-signed) We need to modify the example though as some of the syntax and extensions used are out of date versus our version of kubernetes and we want to use our specific TLS settings. # file: ~/rook/dashboard-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rook-ceph-mgr-dashboard namespace : rook-ceph annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTPS\" nginx.ingress.kubernetes.io/server-snippet : | proxy_ssl_verify off; spec : tls : - hosts : - rook-ceph.jamesveitch.dev secretName : rook-ceph.jamesveitch.dev rules : - host : rook-ceph.jamesveitch.dev http : paths : - path : / backend : serviceName : rook-ceph-mgr-dashboard servicePort : https-dashboard --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : rook-ceph namespace : rook-ceph spec : secretName : rook-ceph.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : rook-ceph.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - rook-ceph.jamesveitch.dev issuerRef : name : letsencrypt-staging kind : ClusterIssuer group : cert-manager.io --- apiVersion : v1 kind : Service metadata : name : rook-ceph-mgr-dashboard-external-https namespace : rook-ceph labels : app : rook-ceph-mgr rook_cluster : rook-ceph spec : ports : - name : dashboard port : 8443 protocol : TCP targetPort : 8443 selector : app : rook-ceph-mgr rook_cluster : rook-ceph sessionAffinity : None Once you've confirmed this works with openssl we can change the issuer for the production LetsEncrypt issuer. Check Certificate with OpenSSL With the above manifest we should have a Fake LE certificate authority from the LetsEncrypt staging servers. $ echo | openssl s_client -connect rook-ceph.jamesveitch.dev:443 2 > & 1 | sed --quiet '/CN/p' depth = 1 CN = Fake LE Intermediate X1 0 s:CN = rook-ceph.jamesveitch.dev i:CN = Fake LE Intermediate X1 1 s:CN = Fake LE Intermediate X1 i:CN = Fake LE Root X1 subject = CN = rook-ceph.jamesveitch.dev issuer = CN = Fake LE Intermediate X1 kubectl -n rook-ceph delete certificate rook-ceph ; \\ sed -i.bak 's/letsencrypt-staging/letsencrypt/g' ~/rook/dashboard-ingress-https.yaml ; \\ kubectl apply -f ~/rook/dashboard-ingress-https.yaml Assuming this all works you should be able to navigate straight to the dashboard with your external dns settings.","title":"Setup Dashboard"},{"location":"01.infrastructure/04.storage/01.dashboard/#nodeport","text":"We'll enable this via a NodePort now. cd ~/rook wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml kubectl create -f ~/rook/dashboard-external-https.yaml You will see the new service rook-ceph-mgr-dashboard-external-https created: $ kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-cephfsplugin-metrics ClusterIP 10 .102.185.98 <none> 8080 /TCP,8081/TCP 18h csi-rbdplugin-metrics ClusterIP 10 .108.114.119 <none> 8080 /TCP,8081/TCP 18h rook-ceph-mgr ClusterIP 10 .107.152.33 <none> 9283 /TCP 18h rook-ceph-mgr-dashboard ClusterIP 10 .105.220.219 <none> 8443 /TCP 18h rook-ceph-mgr-dashboard-external-https NodePort 10 .98.207.115 <none> 8443 :30995/TCP 9s rook-ceph-mon-a ClusterIP 10 .101.90.100 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-b ClusterIP 10 .107.14.178 <none> 6789 /TCP,3300/TCP 18h rook-ceph-mon-c ClusterIP 10 .101.91.134 <none> 6789 /TCP,3300/TCP 18h In this example, port 30995 will be opened to expose port 8443 from the ceph-mgr pod . Find the ip address of the node or, if you've used Avahi as per the physcial node installation you could probably access the node directly using the hostname (Kubernetes will listen on that fqdn). Finding the IP address So this isn't that obvious... We've exposed our service via a NodePort but now need to find the IP address of the actual node in order to hit the required port to see the service... $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-mgr\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-mgr-a-59cc7fb98-7wfxf 1 /1 Running 0 19h 10 .244.0.112 banks <none> <none> Our service is running on the banks node. Let's find the IP of that then. $ kubectl describe node banks | grep InternalIP InternalIP: 192 .168.0.95 Or you could find the IP address with: kubectl get node banks -o jsonpath = '{.status.addresses[0].address}' Now you can enter the URL in your browser as https://192.168.0.95:30995 or https://banks.local:30995 and the dashboard will appear.","title":"NodePort"},{"location":"01.infrastructure/04.storage/01.dashboard/#credentials","text":"As described in the docs Rook creates a default user named admin and generates a secret called rook-ceph-dashboard-admin-password in the namespace where rook is running. To retrieve the generated password, you can run the following: kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo This is a good thing to see...","title":"Credentials"},{"location":"01.infrastructure/04.storage/01.dashboard/#ingress","text":"Given all the work we went through to give our services proper external access with TLS certificates earlier it would be a shame to waste this...! Let's teardown the NodePort. kubectl delete -f ~/rook/dashboard-external-https.yaml The docs have an example using an ingress controller we will borrow from. Quote If you have a cluster with an nginx Ingress Controller and a Certificate Manager (e.g. cert-manager) then you can create an Ingress like the one below. This example achieves four things: Exposes the dashboard on the Internet (using an reverse proxy) Issues an valid TLS Certificate for the specified domain name (using ACME) Tells the reverse proxy that the dashboard itself uses HTTPS Tells the reverse proxy that the dashboard itself does not have a valid certificate (it is self-signed) We need to modify the example though as some of the syntax and extensions used are out of date versus our version of kubernetes and we want to use our specific TLS settings. # file: ~/rook/dashboard-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rook-ceph-mgr-dashboard namespace : rook-ceph annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTPS\" nginx.ingress.kubernetes.io/server-snippet : | proxy_ssl_verify off; spec : tls : - hosts : - rook-ceph.jamesveitch.dev secretName : rook-ceph.jamesveitch.dev rules : - host : rook-ceph.jamesveitch.dev http : paths : - path : / backend : serviceName : rook-ceph-mgr-dashboard servicePort : https-dashboard --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : rook-ceph namespace : rook-ceph spec : secretName : rook-ceph.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : rook-ceph.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - rook-ceph.jamesveitch.dev issuerRef : name : letsencrypt-staging kind : ClusterIssuer group : cert-manager.io --- apiVersion : v1 kind : Service metadata : name : rook-ceph-mgr-dashboard-external-https namespace : rook-ceph labels : app : rook-ceph-mgr rook_cluster : rook-ceph spec : ports : - name : dashboard port : 8443 protocol : TCP targetPort : 8443 selector : app : rook-ceph-mgr rook_cluster : rook-ceph sessionAffinity : None Once you've confirmed this works with openssl we can change the issuer for the production LetsEncrypt issuer. Check Certificate with OpenSSL With the above manifest we should have a Fake LE certificate authority from the LetsEncrypt staging servers. $ echo | openssl s_client -connect rook-ceph.jamesveitch.dev:443 2 > & 1 | sed --quiet '/CN/p' depth = 1 CN = Fake LE Intermediate X1 0 s:CN = rook-ceph.jamesveitch.dev i:CN = Fake LE Intermediate X1 1 s:CN = Fake LE Intermediate X1 i:CN = Fake LE Root X1 subject = CN = rook-ceph.jamesveitch.dev issuer = CN = Fake LE Intermediate X1 kubectl -n rook-ceph delete certificate rook-ceph ; \\ sed -i.bak 's/letsencrypt-staging/letsencrypt/g' ~/rook/dashboard-ingress-https.yaml ; \\ kubectl apply -f ~/rook/dashboard-ingress-https.yaml Assuming this all works you should be able to navigate straight to the dashboard with your external dns settings.","title":"Ingress"},{"location":"01.infrastructure/04.storage/02.storage/","text":"We now have Ceph running and managed via Rook on our Kubernetes cluster (with a nice graphical dashboard) but don't yet have any storage configured. There are some good documented examples we can walk through to get started.","title":"Storage"},{"location":"01.infrastructure/04.storage/0200.block/","text":"As per the docs : Block storage allows a single pod to mount storage We need to create both a StorageClass and a CephBlockPool in order to use black storage on our cluster. mkdir -p ~/rook/storage cd ~/rook/storage wget -O \"storageclass-rbd.yaml\" https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' storageclass-rbd.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' storageclass-rbd.yaml ; \\ kubectl create -f ~/rook/storage/storageclass-rbd.yaml Test this new storage out with a Wordpress installation (including MySQL) which requires the usage of Volume and PersistentVolumeClaim . mkdir -p ~/rook/examples cd ~/rook/examples wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/wordpress.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/mysql.yaml kubectl create -f ~/rook/examples/mysql.yaml ; \\ kubectl create -f ~/rook/examples/wordpress.yaml To review the volumes that have been created run kubectl get pvc $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-29aa4aba-4029-487d-8e7e-b2eb08400382 20Gi RWO rook-ceph-block 74s wp-pv-claim Bound pvc-7251e045-a174-49e1-9393-fbd8dbcfeaa6 20Gi RWO rook-ceph-block 73s Those PVCs can also be seen in the ceph dashboard under Block >> Images Once the pods for Wordpress and MySQL are running get the cluster IP for the wordpress app and navigate to it. You should be presented with the installation wizard. We'll tear this down now before proceeding (but leave the storage class for later usage). $ kubectl delete -f wordpress.yaml service \"wordpress\" deleted persistentvolumeclaim \"wp-pv-claim\" deleted deployment.apps \"wordpress\" deleted $ kubectl delete -f mysql.yaml service \"wordpress-mysql\" deleted persistentvolumeclaim \"mysql-pv-claim\" deleted deployment.apps \"wordpress-mysql\" deleted # kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool # kubectl delete storageclass rook-ceph-block Setting as the default storage class Rather than needing to specify the storage everytime we deploy a new application we can tell kubernetes to, by default, use this storage if not given. Edit the ~/rook/storage/storageclass-rbd.yaml manifest and add both an annotation and the ability to expand the volume dynamically (added in kubernetes v1.16 ). # file: ~/rook/storage/storageclass-rbd.yaml apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block annotations : storageclass.kubernetes.io/is-default-class : \"true\" provisioner : rook-ceph.rbd.csi.ceph.com ... allowVolumeExpansion : true Now apply the modifications with kubectl apply -f ~/rook/storage/storageclass-rbd.yaml","title":"Block"},{"location":"01.infrastructure/04.storage/0200.block/#setting-as-the-default-storage-class","text":"Rather than needing to specify the storage everytime we deploy a new application we can tell kubernetes to, by default, use this storage if not given. Edit the ~/rook/storage/storageclass-rbd.yaml manifest and add both an annotation and the ability to expand the volume dynamically (added in kubernetes v1.16 ). # file: ~/rook/storage/storageclass-rbd.yaml apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block annotations : storageclass.kubernetes.io/is-default-class : \"true\" provisioner : rook-ceph.rbd.csi.ceph.com ... allowVolumeExpansion : true Now apply the modifications with kubectl apply -f ~/rook/storage/storageclass-rbd.yaml","title":"Setting as the default storage class"},{"location":"01.infrastructure/04.storage/0201.filesystem/","text":"As per the docs A shared file system can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared filesystem. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/filesystem.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' filesystem.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' filesystem.yaml ; \\ kubectl create -f ~/rook/storage/filesystem.yaml Wait for the mds pods to start adminlocal@banks:~/rook/storage$ kubectl -n rook-ceph get pod -l app = rook-ceph-mds NAME READY STATUS RESTARTS AGE rook-ceph-mds-myfs-a-84ccb448b5-9jbds 1 /1 Running 0 21s rook-ceph-mds-myfs-b-7d85c48b4c-72q9d 0 /1 Pending 0 20s NB: Because there is a podAntiAffinity spec in the filesystem.yaml placement section you may only see one pod running if we have a single node cluster. This is fine. Running a ceph status via the toolbox will reinforce this. # ceph status ... services: mon: 3 daemons, quorum a,b,c ( age 3d ) mgr: a ( active, since 3d ) mds: myfs:1 { 0 = myfs-a = up:active } osd: 2 osds: 2 up ( since 3d ) , 2 in ( since 3d ) rgw: 1 daemon active ( my.store.a ) ... As with other storage options, we need to create a StorageClass for a Filesystem. This will use the CSI Driver (which is the preferred driver going forward for K8s 1.13 and newer). cd ~/rook/storage wget -O \"storageclass-cephfs.yaml\" https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml kubectl create -f ~/rook/storage/storageclass-cephfs.yaml Consume the Shared File System: K8s Registry Sample We'll deploy a private docker registry that uses this shared filesystem via a PersistentVolumeClaim . cd ~/rook/ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml kubectl create -f ~/rook/kube-registry.yaml Configure registry See Github docs for further details. mkdir -p ~/registry cd ~/registry Now create a service.yaml file. # file: ~/registry/service.yaml apiVersion: v1 kind: Service metadata: name: kube-registry namespace: kube-system labels: k8s-app: kube-registry-upstream kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"KubeRegistry\" spec: selector: k8s-app: kube-registry ports: - name: registry port: 5000 protocol: TCP Apply this with kubectl create -f service.yaml . With the service created we'll use a DaemonSet to deploy a pod onto every node in the cluster (so that Dokcer sees it as localhost ). # file: ~/registry/daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-registry-proxy namespace: kube-system labels: k8s-app: kube-registry-proxy kubernetes.io/cluster-service: \"true\" version: v0.4 spec: selector: matchLabels: name: kube-registry template: metadata: labels: k8s-app: kube-registry-proxy kubernetes.io/name: \"kube-registry-proxy\" kubernetes.io/cluster-service: \"true\" version: v0.4 name: kube-registry spec: containers: - name: kube-registry-proxy image: gcr.io/google_containers/kube-registry-proxy:0.4 resources: limits: cpu: 100m memory: 50Mi env: - name: REGISTRY_HOST value: kube-registry.kube-system.svc.cluster.local - name: REGISTRY_PORT value: \"5000\" ports: - name: registry containerPort: 80 hostPort: 5000 Apply with a kubectl create -f ~/registry/daemonset.yaml and then check for completion of pods. $ kubectl -n kube-system get pod -l 'name=kube-registry' NAME READY STATUS RESTARTS AGE kube-registry-proxy-vtd56 1 /1 Running 0 5m34s We can check the registry has been deployed by running curl localhost:5000/image and expecting a 404 response. $ curl localhost:5000/image 404 page not found Push an image to the registry As per Docker docs we will push a small docker alpine image to our new local private repository. sudo docker pull alpine sudo docker images | grep alpine | grep latest sudo docker tag 965ea09ff2eb 127 .0.0.1:5000/alpine sudo docker push 127 .0.0.1:5000/alpine Mount the filesystem in toolbox to confirm # Create the directory mkdir /tmp/registry # Detect the mon endpoints and the user secret for the connection mon_endpoints = $( grep mon_host /etc/ceph/ceph.conf | awk '{print $3}' ) my_secret = $( grep key /etc/ceph/keyring | awk '{print $3}' ) # Mount the file system mount -t ceph -o mds_namespace = myfs,name = admin,secret = $my_secret $mon_endpoints :/ /tmp/registry # See your mounted file system df -h With the filesystem mounted we'll confirm there's an alpine repository now after our push above. # find /tmp/registry -name \"alpine\" /tmp/registry/volumes/csi/csi-vol-77b79f13-11ee-11ea-9848-7a3d11f24466/docker/registry/v2/repositories/alpine Teardown kubectl delete -f ~/registry/daemonset.yaml ; \\ kubectl delete -f ~/registry/service.yaml ; \\ kubectl delete -f ~/rook/kube-registry.yaml To delete the filesystem components and backing data, delete the Filesystem CRD. Warning: Data will be deleted kubectl -n rook-ceph delete cephfilesystem myfs","title":"Filesystem"},{"location":"01.infrastructure/04.storage/0201.filesystem/#consume-the-shared-file-system-k8s-registry-sample","text":"We'll deploy a private docker registry that uses this shared filesystem via a PersistentVolumeClaim . cd ~/rook/ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml kubectl create -f ~/rook/kube-registry.yaml","title":"Consume the Shared File System: K8s Registry Sample"},{"location":"01.infrastructure/04.storage/0201.filesystem/#configure-registry","text":"See Github docs for further details. mkdir -p ~/registry cd ~/registry Now create a service.yaml file. # file: ~/registry/service.yaml apiVersion: v1 kind: Service metadata: name: kube-registry namespace: kube-system labels: k8s-app: kube-registry-upstream kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"KubeRegistry\" spec: selector: k8s-app: kube-registry ports: - name: registry port: 5000 protocol: TCP Apply this with kubectl create -f service.yaml . With the service created we'll use a DaemonSet to deploy a pod onto every node in the cluster (so that Dokcer sees it as localhost ). # file: ~/registry/daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-registry-proxy namespace: kube-system labels: k8s-app: kube-registry-proxy kubernetes.io/cluster-service: \"true\" version: v0.4 spec: selector: matchLabels: name: kube-registry template: metadata: labels: k8s-app: kube-registry-proxy kubernetes.io/name: \"kube-registry-proxy\" kubernetes.io/cluster-service: \"true\" version: v0.4 name: kube-registry spec: containers: - name: kube-registry-proxy image: gcr.io/google_containers/kube-registry-proxy:0.4 resources: limits: cpu: 100m memory: 50Mi env: - name: REGISTRY_HOST value: kube-registry.kube-system.svc.cluster.local - name: REGISTRY_PORT value: \"5000\" ports: - name: registry containerPort: 80 hostPort: 5000 Apply with a kubectl create -f ~/registry/daemonset.yaml and then check for completion of pods. $ kubectl -n kube-system get pod -l 'name=kube-registry' NAME READY STATUS RESTARTS AGE kube-registry-proxy-vtd56 1 /1 Running 0 5m34s We can check the registry has been deployed by running curl localhost:5000/image and expecting a 404 response. $ curl localhost:5000/image 404 page not found","title":"Configure registry"},{"location":"01.infrastructure/04.storage/0201.filesystem/#push-an-image-to-the-registry","text":"As per Docker docs we will push a small docker alpine image to our new local private repository. sudo docker pull alpine sudo docker images | grep alpine | grep latest sudo docker tag 965ea09ff2eb 127 .0.0.1:5000/alpine sudo docker push 127 .0.0.1:5000/alpine","title":"Push an image to the registry"},{"location":"01.infrastructure/04.storage/0201.filesystem/#mount-the-filesystem-in-toolbox-to-confirm","text":"# Create the directory mkdir /tmp/registry # Detect the mon endpoints and the user secret for the connection mon_endpoints = $( grep mon_host /etc/ceph/ceph.conf | awk '{print $3}' ) my_secret = $( grep key /etc/ceph/keyring | awk '{print $3}' ) # Mount the file system mount -t ceph -o mds_namespace = myfs,name = admin,secret = $my_secret $mon_endpoints :/ /tmp/registry # See your mounted file system df -h With the filesystem mounted we'll confirm there's an alpine repository now after our push above. # find /tmp/registry -name \"alpine\" /tmp/registry/volumes/csi/csi-vol-77b79f13-11ee-11ea-9848-7a3d11f24466/docker/registry/v2/repositories/alpine","title":"Mount the filesystem in toolbox to confirm"},{"location":"01.infrastructure/04.storage/0201.filesystem/#teardown","text":"kubectl delete -f ~/registry/daemonset.yaml ; \\ kubectl delete -f ~/registry/service.yaml ; \\ kubectl delete -f ~/rook/kube-registry.yaml To delete the filesystem components and backing data, delete the Filesystem CRD. Warning: Data will be deleted kubectl -n rook-ceph delete cephfilesystem myfs","title":"Teardown"},{"location":"01.infrastructure/04.storage/0202.object/","text":"As per the docs Object storage exposes an S3 API to the storage cluster for applications to put and get data. We'll first create a CephObjectStore followed by a StorageClass for the bucket. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-ec.yaml # - replicas: 1 # we dont want to replicate this # - failureDomain: osd # we don't want it to require multiple nodes sed -i.bak 's/size: 3/size: 1/g' object-ec.yaml ; \\ sed -i.bak 's/failureDomain: host/failureDomain: osd/g' object-ec.yaml ; \\ kubectl create -f ~/rook/storage/object-ec.yaml Check that the object store is configured and a rgw pod has started. $ kubectl -n rook-ceph get pod -l app = rook-ceph-rgw NAME READY STATUS RESTARTS AGE rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 27s Enable dashboard for the Object Gateway As per the docs we need to specifically enable access to the object gateway for it to be registered in the Ceph dashboard. # Connect to the toolbox first kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash # Create a system user radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system Make note of the keys # radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system { \"user_id\" : \"666\" , \"display_name\" : \"dashboard\" , \"email\" : \"\" , \"suspended\" : 0 , \"max_buckets\" : 1000 , \"subusers\" : [] , \"keys\" : [ { \"user\" : \"666\" , \"access_key\" : \"MUNSZSY7LF2E202MW1H6\" , \"secret_key\" : \"OF1za2LvibBpYjb6mw0umYDePfBkzfWSRNMeIwL0\" } ] , \"swift_keys\" : [] , \"caps\" : [] , \"op_mask\" : \"read, write, delete\" , \"system\" : \"true\" , \"default_placement\" : \"\" , \"default_storage_class\" : \"\" , \"placement_tags\" : [] , \"bucket_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"user_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"temp_url_keys\" : [] , \"type\" : \"rgw\" , \"mfa_ids\" : [] } Get the access_key and secret_access_key radosgw-admin user info --uid = 666 Now apply these credentials to the dashboard ceph dashboard set-rgw-api-access-key <access_key> ceph dashboard set-rgw-api-secret-key <secret_key> Set the host. You can get the service details with kubectl -n rook-ceph describe svc -l \"app=rook-ceph-rgw\" # use the format `service`.`namespace` as per docs # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ ceph dashboard set-rgw-api-host rook-ceph-rgw-my-store.rook-ceph ceph dashboard set-rgw-api-port 80 Create a bucket With an object store configured we can create a bucket. A bucket is created by defining a storage class and then registering an associated claim. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml kubectl create -f ~/rook/storage/storageclass-bucket-delete.yaml ; \\ kubectl create -f ~/rook/storage/object-bucket-claim-delete.yaml We should now see something like this when navigating on the dashboard to Object Gateway >> Buckets Enable external access Much like the Ceph Dashboard we want to expose the bucket to services that potentially live outside of the cluster. As with the dashboard we can either use a NodePort or Ingress to do this. NodePort We'll create a new service for external access. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/rgw-external.yaml kubectl create -f rgw-external.yaml We should now have a service running listening on a NodePort $ kubectl get svc -n rook-ceph -l 'app=rook-ceph-rgw' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-ceph-rgw-my-store ClusterIP 10 .97.98.170 <none> 80 /TCP 95m rook-ceph-rgw-my-store-external NodePort 10 .105.1.131 <none> 80 :32039/TCP 30s Connecting to the bucket with a client As the API is S3 compatible we can connect to the bucket with a variety of tools. In order to to do we need to obtain the HOST , ACCESS_KEY and SECRET_ACCESS_KEY variables. # config-map, secret, OBC will part of default if no specific name space mentioned # NB: You need to use the `metadata: name` for the bucket as defined in the claim export AWS_HOST = $( kubectl -n default get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}' ) export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) The AWS_HOST should also match the details provided abvove as the set-rgw-api-host command for the dashboard. We'll install s3md on the host to check. sudo apt-get update && sudo apt-get install -y s3cmd We'll need to find the node that our service is running on: $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-rgw,rook_object_store=my-store\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 107m 10 .244.0.87 banks.local <none> <none> We'll set the AWS_HOST to banks.local and the NodePort in use (in this case 32039 ). export AWS_HOST = banks.local:32039 Check the buckets our use has access to. $ s3cmd ls --no-ssl --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -11-28 12 :33 s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 Now upload a new file to this bucket and download it again to confirm. # Create object echo \"Hello Rook\" > /tmp/rookObj # Upload s3cmd put /tmp/rookObj \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } \\ s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 # Download s3cmd get s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3/rookObj \\ /tmp/rookObj-download \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } Check the contents $ cat /tmp/rookObj-download Hello Rook $ md5sum /tmp/rookObj* dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj-download Ingress As with the dasboard we'll configure this to have a valid SSL certificate and be accessible under a subdomain (e.g. s3.jamesveitch.dev). # file: ~/rook/storage/bucket-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rook-ceph-rgw-my-store-external-ingress namespace : rook-ceph annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - s3.jamesveitch.dev secretName : s3.jamesveitch.dev rules : - host : s3.jamesveitch.dev http : paths : - path : / backend : serviceName : rook-ceph-rgw-my-store servicePort : http --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : s3 namespace : rook-ceph spec : secretName : s3.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : s3.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - s3.jamesveitch.dev issuerRef : name : letsencrypt-staging kind : ClusterIssuer group : cert-manager.io --- apiVersion : v1 kind : Service metadata : name : rook-ceph-rgw-my-store-external-ingress namespace : rook-ceph labels : app : rook-ceph-rgw rook_cluster : rook-ceph rook_object_store : my-store spec : ports : - name : rgw port : 80 protocol : TCP targetPort : 80 selector : app : rook-ceph-rgw rook_cluster : rook-ceph rook_object_store : my-store sessionAffinity : None Apply this manifest with kubectl apply -f ~/rook/storage/bucket-ingress-https.yaml and, once the certificate has been provisioned, check that you get the Fake LE certificate before modifying and using the production server. kubectl -n rook-ceph delete certificate s3 ; \\ sed -i.bak 's/letsencrypt-staging/letsencrypt/g' ~/rook/storage/bucket-ingress-https.yaml ; \\ kubectl apply -f ~/rook/storage/bucket-ingress-https.yaml ; \\ watch kubectl -n rook-ceph get certificates Once the certificate has been recreated you should be able to navigate to the address and see the below. Connecting to the bucket with a client (external) Using s3cmd we can now connect to our bucket over SSL. Install the client (if not already available) sudo apt-get update && sudo apt-get install -y s3cmd Set some environment variables. export AWS_HOST = s3.jamesveitch.dev export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) Now connect and list out all the buckets. $ s3cmd ls --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -12-20 11 :24 s3://ceph-bkt-adc7524d-3dc6-400d-9a33-74171d2a4786 Teardown See Removing buckets in radosgw (and their contents) kubectl delete -f ~/rook/storage/object-bucket-claim-delete.yaml ; \\ kubectl delete -f ~/rook/storage/storageclass-bucket-delete.yaml And then, within the toolbox radosgw-admin bucket rm --bucket = ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 --purge-objects","title":"Object"},{"location":"01.infrastructure/04.storage/0202.object/#enable-dashboard-for-the-object-gateway","text":"As per the docs we need to specifically enable access to the object gateway for it to be registered in the Ceph dashboard. # Connect to the toolbox first kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash # Create a system user radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system Make note of the keys # radosgw-admin user create \\ --uid = 666 \\ --display-name = dashboard \\ --system { \"user_id\" : \"666\" , \"display_name\" : \"dashboard\" , \"email\" : \"\" , \"suspended\" : 0 , \"max_buckets\" : 1000 , \"subusers\" : [] , \"keys\" : [ { \"user\" : \"666\" , \"access_key\" : \"MUNSZSY7LF2E202MW1H6\" , \"secret_key\" : \"OF1za2LvibBpYjb6mw0umYDePfBkzfWSRNMeIwL0\" } ] , \"swift_keys\" : [] , \"caps\" : [] , \"op_mask\" : \"read, write, delete\" , \"system\" : \"true\" , \"default_placement\" : \"\" , \"default_storage_class\" : \"\" , \"placement_tags\" : [] , \"bucket_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"user_quota\" : { \"enabled\" : false, \"check_on_raw\" : false, \"max_size\" : -1, \"max_size_kb\" : 0 , \"max_objects\" : -1 } , \"temp_url_keys\" : [] , \"type\" : \"rgw\" , \"mfa_ids\" : [] } Get the access_key and secret_access_key radosgw-admin user info --uid = 666 Now apply these credentials to the dashboard ceph dashboard set-rgw-api-access-key <access_key> ceph dashboard set-rgw-api-secret-key <secret_key> Set the host. You can get the service details with kubectl -n rook-ceph describe svc -l \"app=rook-ceph-rgw\" # use the format `service`.`namespace` as per docs # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ ceph dashboard set-rgw-api-host rook-ceph-rgw-my-store.rook-ceph ceph dashboard set-rgw-api-port 80","title":"Enable dashboard for the Object Gateway"},{"location":"01.infrastructure/04.storage/0202.object/#create-a-bucket","text":"With an object store configured we can create a bucket. A bucket is created by defining a storage class and then registering an associated claim. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml ; \\ wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml kubectl create -f ~/rook/storage/storageclass-bucket-delete.yaml ; \\ kubectl create -f ~/rook/storage/object-bucket-claim-delete.yaml We should now see something like this when navigating on the dashboard to Object Gateway >> Buckets","title":"Create a bucket"},{"location":"01.infrastructure/04.storage/0202.object/#enable-external-access","text":"Much like the Ceph Dashboard we want to expose the bucket to services that potentially live outside of the cluster. As with the dashboard we can either use a NodePort or Ingress to do this.","title":"Enable external access"},{"location":"01.infrastructure/04.storage/0202.object/#nodeport","text":"We'll create a new service for external access. cd ~/rook/storage wget https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/rgw-external.yaml kubectl create -f rgw-external.yaml We should now have a service running listening on a NodePort $ kubectl get svc -n rook-ceph -l 'app=rook-ceph-rgw' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-ceph-rgw-my-store ClusterIP 10 .97.98.170 <none> 80 /TCP 95m rook-ceph-rgw-my-store-external NodePort 10 .105.1.131 <none> 80 :32039/TCP 30s","title":"NodePort"},{"location":"01.infrastructure/04.storage/0202.object/#connecting-to-the-bucket-with-a-client","text":"As the API is S3 compatible we can connect to the bucket with a variety of tools. In order to to do we need to obtain the HOST , ACCESS_KEY and SECRET_ACCESS_KEY variables. # config-map, secret, OBC will part of default if no specific name space mentioned # NB: You need to use the `metadata: name` for the bucket as defined in the claim export AWS_HOST = $( kubectl -n default get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}' ) export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) The AWS_HOST should also match the details provided abvove as the set-rgw-api-host command for the dashboard. We'll install s3md on the host to check. sudo apt-get update && sudo apt-get install -y s3cmd We'll need to find the node that our service is running on: $ kubectl -n rook-ceph get pods --selector = \"app=rook-ceph-rgw,rook_object_store=my-store\" --output = wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rook-ceph-rgw-my-store-a-86d4f98658-tfrj9 1 /1 Running 0 107m 10 .244.0.87 banks.local <none> <none> We'll set the AWS_HOST to banks.local and the NodePort in use (in this case 32039 ). export AWS_HOST = banks.local:32039 Check the buckets our use has access to. $ s3cmd ls --no-ssl --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -11-28 12 :33 s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 Now upload a new file to this bucket and download it again to confirm. # Create object echo \"Hello Rook\" > /tmp/rookObj # Upload s3cmd put /tmp/rookObj \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } \\ s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 # Download s3cmd get s3://ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3/rookObj \\ /tmp/rookObj-download \\ --no-ssl \\ --host = ${ AWS_HOST } \\ --host-bucket = \\ --access_key = ${ AWS_ACCESS_KEY_ID } \\ --secret_key = ${ AWS_SECRET_ACCESS_KEY } Check the contents $ cat /tmp/rookObj-download Hello Rook $ md5sum /tmp/rookObj* dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj dd2f8a37e3bd769458faef03c0e4610d /tmp/rookObj-download","title":"Connecting to the bucket with a client"},{"location":"01.infrastructure/04.storage/0202.object/#ingress","text":"As with the dasboard we'll configure this to have a valid SSL certificate and be accessible under a subdomain (e.g. s3.jamesveitch.dev). # file: ~/rook/storage/bucket-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rook-ceph-rgw-my-store-external-ingress namespace : rook-ceph annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - s3.jamesveitch.dev secretName : s3.jamesveitch.dev rules : - host : s3.jamesveitch.dev http : paths : - path : / backend : serviceName : rook-ceph-rgw-my-store servicePort : http --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : s3 namespace : rook-ceph spec : secretName : s3.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : s3.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - s3.jamesveitch.dev issuerRef : name : letsencrypt-staging kind : ClusterIssuer group : cert-manager.io --- apiVersion : v1 kind : Service metadata : name : rook-ceph-rgw-my-store-external-ingress namespace : rook-ceph labels : app : rook-ceph-rgw rook_cluster : rook-ceph rook_object_store : my-store spec : ports : - name : rgw port : 80 protocol : TCP targetPort : 80 selector : app : rook-ceph-rgw rook_cluster : rook-ceph rook_object_store : my-store sessionAffinity : None Apply this manifest with kubectl apply -f ~/rook/storage/bucket-ingress-https.yaml and, once the certificate has been provisioned, check that you get the Fake LE certificate before modifying and using the production server. kubectl -n rook-ceph delete certificate s3 ; \\ sed -i.bak 's/letsencrypt-staging/letsencrypt/g' ~/rook/storage/bucket-ingress-https.yaml ; \\ kubectl apply -f ~/rook/storage/bucket-ingress-https.yaml ; \\ watch kubectl -n rook-ceph get certificates Once the certificate has been recreated you should be able to navigate to the address and see the below.","title":"Ingress"},{"location":"01.infrastructure/04.storage/0202.object/#connecting-to-the-bucket-with-a-client-external","text":"Using s3cmd we can now connect to our bucket over SSL. Install the client (if not already available) sudo apt-get update && sudo apt-get install -y s3cmd Set some environment variables. export AWS_HOST = s3.jamesveitch.dev export AWS_ACCESS_KEY_ID = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode ) Now connect and list out all the buckets. $ s3cmd ls --host = ${ AWS_HOST } --host-bucket = --access_key = ${ AWS_ACCESS_KEY_ID } --secret_key = ${ AWS_SECRET_ACCESS_KEY } s3:// 2019 -12-20 11 :24 s3://ceph-bkt-adc7524d-3dc6-400d-9a33-74171d2a4786","title":"Connecting to the bucket with a client (external)"},{"location":"01.infrastructure/04.storage/0202.object/#teardown","text":"See Removing buckets in radosgw (and their contents) kubectl delete -f ~/rook/storage/object-bucket-claim-delete.yaml ; \\ kubectl delete -f ~/rook/storage/storageclass-bucket-delete.yaml And then, within the toolbox radosgw-admin bucket rm --bucket = ceph-bkt-5d1f6a77-3206-4140-8c6c-04533e4f8cb3 --purge-objects","title":"Teardown"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/","text":"As we've got Ceph (via Rook) installed we'll initially setup Prometheus to monitor metrics and then display them in Deploy and configure Prometheus mkdir -p ~/monitoring ; \\ cd ~/monitoring export OPERATOR_VERSION = v0.34.0 wget https://raw.githubusercontent.com/coreos/prometheus-operator/ ${ OPERATOR_VERSION } /bundle.yaml kubectl apply -f ~/monitoring/bundle.yaml Then wait for the prometheus-operator pod to be Running with kubectl get pods -w . Then we need to configure the Ceph specific configuration: monitoring endpoints, alarm levels etc... cd ~/monitoring ; \\ wget https://raw.githubusercontent.com/packet-labs/Rook-on-Bare-Metal-Workshop/master/configs/ceph-monitoring.yml ; \\ kubectl apply -f ~/monitoring/ceph-monitoring.yml At this point we should be able to reach the Prometheus UI at: IP = $( kubectl get nodes -o jsonpath = '{.items[0].status.addresses[].address}' ) PORT = $( kubectl -n rook-ceph get svc rook-prometheus -o jsonpath = '{.spec.ports[].nodePort}' ) echo \"Your Prometheus UI is available at: http:// $IP : $PORT /\" NB: This, by default, sets up a NodePort service. We'll fix this later. If you're node is not immediately acessible via the $IP then you can check the available addresses for it, maybe choosing to access via the Hostname instead. $ kubectl get nodes -o json | jq '.items[0].status.addresses[]' { \"address\" : \"172.17.0.1\" , \"type\" : \"InternalIP\" } { \"address\" : \"banks.local\" , \"type\" : \"Hostname\" } Head over to Status >> Target and make sure that the ceph-mgr target is UP . Then go to Graph and graph following query ceph_cluster_total_used_bytes/(1024^3) to show the total space used in gigabyte over time. Another query of (ceph_cluster_total_used_bytes / ceph_cluster_total_used_raw_bytes) * 100 will show the % of available space used. Install Helm Irrespective of the method for ultimately exposing the service we'll use helm to install grafana from the stable charts repository. # Install helm export HELM_VERSION = v3.0.0 wget https://get.helm.sh/helm- ${ HELM_VERSION } -linux-amd64.tar.gz tar -xvzf helm- ${ HELM_VERSION } -linux-amd64.tar.gz chmod +x linux-amd64/helm sudo mv linux-amd64/helm /usr/local/bin/ rm -rf { helm*,linux-amd64 } # Add repository helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm repo update Install Grafana As with other services, we can choose to use either a NodePort or Ingress (via a ClusterIP ) to expose our services. My preference is the ingress. ClusterIP helm install grafana stable/grafana \\ --set service.type = ClusterIP \\ --set persistence.enabled = true \\ --set persistence.type = pvc \\ --set persistence.size = 10Gi \\ --set persistence.storageClassName = rook-ceph-block NodePort helm install grafana stable/grafana \\ --set service.type = NodePort \\ --set persistence.enabled = true \\ --set persistence.type = pvc \\ --set persistence.size = 10Gi \\ --set persistence.storageClassName = rook-ceph-block As can be seen we're using persistence with a pvc and telling it to use our rook-ceph-block storage. You'll get something that look like the below as an output. Follow the instructions. Info NAME: grafana LAST DEPLOYED: Fri Dec 20 12 :28:11 2019 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1 . Get your 'admin' user password by running: kubectl get secret --namespace default grafana -o jsonpath = \"{.data.admin-password}\" | base64 --decode ; echo 2 . The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.default.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME = $( kubectl get pods --namespace default -l \"app=grafana,release=grafana\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace default port-forward $POD_NAME 3000 3 . Login with the password from step 1 and the username: admin Once logged in you'll see a screen similar to below. Hit the Add data source and select Prometheus . The url needs to be the details we identified above. We'll need the either the Cluster-IP or an internal dns reference (i.e rook-prometheus.rook-ceph ) and Port of the service (choose the container port, not the externally exposed one). $ kubectl -n rook-ceph get svc rook-prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-prometheus NodePort 10 .110.175.22 <none> 9090 :30900/TCP 16h Hit Save & Test and you should hopefully see a Data source is working check appear. Hit Back to go back to the main screen. Back on the main screen click on the + and select Import . Ceph has published some open dashboards with the IDs 2842 , 5336 and 5342 . NB: On two of the dashboards you need to select Prometheus as the datasource. Ingress As the helm chart has already created the Service we need to just create an Ingress and map it to this and then ensure we request a valid certificate from LetsEncrypt. # file: ~/monitoring/grafana-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : grafana-external-ingress namespace : default annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - grafana.jamesveitch.dev secretName : grafana.jamesveitch.dev rules : - host : grafana.jamesveitch.dev http : paths : - path : / backend : serviceName : grafana servicePort : service --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : grafana namespace : default spec : secretName : grafana.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : grafana.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - grafana.jamesveitch.dev issuerRef : name : letsencrypt kind : ClusterIssuer group : cert-manager.io","title":"Prometheus and Grafana"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/#deploy-and-configure-prometheus","text":"mkdir -p ~/monitoring ; \\ cd ~/monitoring export OPERATOR_VERSION = v0.34.0 wget https://raw.githubusercontent.com/coreos/prometheus-operator/ ${ OPERATOR_VERSION } /bundle.yaml kubectl apply -f ~/monitoring/bundle.yaml Then wait for the prometheus-operator pod to be Running with kubectl get pods -w . Then we need to configure the Ceph specific configuration: monitoring endpoints, alarm levels etc... cd ~/monitoring ; \\ wget https://raw.githubusercontent.com/packet-labs/Rook-on-Bare-Metal-Workshop/master/configs/ceph-monitoring.yml ; \\ kubectl apply -f ~/monitoring/ceph-monitoring.yml At this point we should be able to reach the Prometheus UI at: IP = $( kubectl get nodes -o jsonpath = '{.items[0].status.addresses[].address}' ) PORT = $( kubectl -n rook-ceph get svc rook-prometheus -o jsonpath = '{.spec.ports[].nodePort}' ) echo \"Your Prometheus UI is available at: http:// $IP : $PORT /\" NB: This, by default, sets up a NodePort service. We'll fix this later. If you're node is not immediately acessible via the $IP then you can check the available addresses for it, maybe choosing to access via the Hostname instead. $ kubectl get nodes -o json | jq '.items[0].status.addresses[]' { \"address\" : \"172.17.0.1\" , \"type\" : \"InternalIP\" } { \"address\" : \"banks.local\" , \"type\" : \"Hostname\" } Head over to Status >> Target and make sure that the ceph-mgr target is UP . Then go to Graph and graph following query ceph_cluster_total_used_bytes/(1024^3) to show the total space used in gigabyte over time. Another query of (ceph_cluster_total_used_bytes / ceph_cluster_total_used_raw_bytes) * 100 will show the % of available space used.","title":"Deploy and configure Prometheus"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/#install-helm","text":"Irrespective of the method for ultimately exposing the service we'll use helm to install grafana from the stable charts repository. # Install helm export HELM_VERSION = v3.0.0 wget https://get.helm.sh/helm- ${ HELM_VERSION } -linux-amd64.tar.gz tar -xvzf helm- ${ HELM_VERSION } -linux-amd64.tar.gz chmod +x linux-amd64/helm sudo mv linux-amd64/helm /usr/local/bin/ rm -rf { helm*,linux-amd64 } # Add repository helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm repo update","title":"Install Helm"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/#install-grafana","text":"As with other services, we can choose to use either a NodePort or Ingress (via a ClusterIP ) to expose our services. My preference is the ingress. ClusterIP helm install grafana stable/grafana \\ --set service.type = ClusterIP \\ --set persistence.enabled = true \\ --set persistence.type = pvc \\ --set persistence.size = 10Gi \\ --set persistence.storageClassName = rook-ceph-block NodePort helm install grafana stable/grafana \\ --set service.type = NodePort \\ --set persistence.enabled = true \\ --set persistence.type = pvc \\ --set persistence.size = 10Gi \\ --set persistence.storageClassName = rook-ceph-block As can be seen we're using persistence with a pvc and telling it to use our rook-ceph-block storage. You'll get something that look like the below as an output. Follow the instructions. Info NAME: grafana LAST DEPLOYED: Fri Dec 20 12 :28:11 2019 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1 . Get your 'admin' user password by running: kubectl get secret --namespace default grafana -o jsonpath = \"{.data.admin-password}\" | base64 --decode ; echo 2 . The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.default.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME = $( kubectl get pods --namespace default -l \"app=grafana,release=grafana\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace default port-forward $POD_NAME 3000 3 . Login with the password from step 1 and the username: admin Once logged in you'll see a screen similar to below. Hit the Add data source and select Prometheus . The url needs to be the details we identified above. We'll need the either the Cluster-IP or an internal dns reference (i.e rook-prometheus.rook-ceph ) and Port of the service (choose the container port, not the externally exposed one). $ kubectl -n rook-ceph get svc rook-prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rook-prometheus NodePort 10 .110.175.22 <none> 9090 :30900/TCP 16h Hit Save & Test and you should hopefully see a Data source is working check appear. Hit Back to go back to the main screen. Back on the main screen click on the + and select Import . Ceph has published some open dashboards with the IDs 2842 , 5336 and 5342 . NB: On two of the dashboards you need to select Prometheus as the datasource.","title":"Install Grafana"},{"location":"01.infrastructure/05.monitoring/00.monitoring.with.prometheus.and.grafana/#ingress","text":"As the helm chart has already created the Service we need to just create an Ingress and map it to this and then ensure we request a valid certificate from LetsEncrypt. # file: ~/monitoring/grafana-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : grafana-external-ingress namespace : default annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - grafana.jamesveitch.dev secretName : grafana.jamesveitch.dev rules : - host : grafana.jamesveitch.dev http : paths : - path : / backend : serviceName : grafana servicePort : service --- apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : grafana namespace : default spec : secretName : grafana.jamesveitch.dev duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : grafana.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - grafana.jamesveitch.dev issuerRef : name : letsencrypt kind : ClusterIssuer group : cert-manager.io","title":"Ingress"},{"location":"01.infrastructure/05.monitoring/01.kubernetes.dashboard/","text":"Kubernetes has a Web UI that can be turned on and looks like the below. As it's not deployed by default we'll turn it on. Further docs are available on GitHub . We need to customise the steps though as by default the documentation installs in an insecure fashion (self-signed certs etc.) Notes from repo Custom certificates have to be stored in a secret named kubernetes-dashboard-certs in the same namespace as Kubernetes Dashboard. By default the recommended manifests are the aio ones and are found here . The recommended.yaml is the one the README.md will propose. If we want to deploy with authorisation thoguh the alternative.yaml should be used. We need to review the Access control part of the docs. First we should create a namespace for all of our monitoring solutions and then get a LetsEncrypt certificate for the dashboard to consume (as opposed to using some self-signed ones). kubectl create namespace monitoring ~/monitoring/kubernetes-dashboard-certificate.yaml # file: ~/monitoring/kubernetes-dashboard-certificate.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : dashboard namespace : monitoring spec : # Secret names are always required. # We should use the name in the docs # https://github.com/kubernetes/dashboard/blob/master/docs/user/installation.md#recommended-setup secretName : kubernetes-dashboard-certs duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName : dashboard.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth # At least one of a DNS Name, USI SAN, or IP address is required. dnsNames : - dashboard.jamesveitch.dev # uriSANs: # - spiffe://cluster.local/ns/sandbox/sa/example # ipAddresses: # - 192.168.0.5 # Issuer references are always required. issuerRef : name : letsencrypt # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : ClusterIssuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group : cert-manager.io Obtain with a kubectl apply -f ~/monitoring/kubernetes-dashboard-certificate.yaml . Once the certificate has been obtained you should be able to see it as a secret. certificate $ kubectl get certificate -n monitoring NAME READY SECRET AGE dashboard True kubernetes-dashboard-certs 9m18s $ kubectl describe secret kubernetes-dashboard-certs -n monitoring Name: kubernetes-dashboard-certs Namespace: monitoring Labels: <none> Annotations: cert-manager.io/alt-names: dashboard.jamesveitch.dev cert-manager.io/certificate-name: dashboard cert-manager.io/common-name: dashboard.jamesveitch.dev cert-manager.io/ip-sans: cert-manager.io/issuer-kind: ClusterIssuer cert-manager.io/issuer-name: letsencrypt cert-manager.io/uri-sans: Type: kubernetes.io/tls Data ==== ca.crt: 0 bytes tls.crt: 3586 bytes tls.key: 1679 bytes Now grab the manifest and, under the Deployment, section add arguments with the pod definition for the certificates as well as changing the namespace to our monitoring one. export DASHBOARD_VERSION = v2.0.0-beta8 ; \\ cd ~/monitoring ; \\ wget https://raw.githubusercontent.com/kubernetes/dashboard/ ${ DASHBOARD_VERSION } /aio/deploy/recommended.yaml # Replace namespace sed -i.bak 's/namespace: kubernetes-dashboard/namespace: monitoring/g' ~/monitoring/recommended.yaml # Replace deployment options for namespace and insert tls sed -i.bak '/- --auto-generate-certificates/i\\ - --tls-cert-file=/tls.crt\\n - --tls-key-file=/tls.key' ~/monitoring/recommended.yaml sed -i.bak 's/- --namespace=kubernetes-dashboard/- --namespace=monitoring/g' ~/monitoring/recommended.yaml We'll apply this now (ignore any errors around The Secret \"kubernetes-dashboard-certs\" is invalid ). It creates a namespace for kubernetes-dashboard by default so we'll also delete that afterwards as it's not needed. kubectl apply -f ~/monitoring/recommended.yaml ; \\ kubectl delete ns kubernetes-dashboard And now create an ingress and matching certificate # file: ~/monitoring/dashboard-ingress-https.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : dashboard-external-ingress namespace : monitoring annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTPS\" nginx.ingress.kubernetes.io/server-snippet : | proxy_ssl_verify off; spec : tls : - hosts : - dashboard.jamesveitch.dev secretName : kubernetes-dashboard-certs rules : - host : dashboard.jamesveitch.dev http : paths : - path : / backend : serviceName : kubernetes-dashboard servicePort : 443 kubectl apply -f ~/monitoring/dashboard-ingress-https.yaml If all goes well you should now be able to navigate to your dashboard at dashboard.jamesveitch.dev . Get your token for logging in. $ kubectl -n monitoring get secret NAME TYPE DATA AGE dashboard-jamesveitch-dev kubernetes.io/tls 3 40m dashboard.jamesveitch.dev kubernetes.io/tls 3 12m default-token-xwv2j kubernetes.io/service-account-token 3 41m kubernetes-dashboard-certs kubernetes.io/tls 3 6m17s kubernetes-dashboard-csrf Opaque 1 3m kubernetes-dashboard-key-holder Opaque 2 3m kubernetes-dashboard-token-gdf67 kubernetes.io/service-account-token 3 3m $ kubectl -n monitoring describe secret kubernetes-dashboard-token-gdf67 Name: kubernetes-dashboard-token-gdf67 Namespace: monitoring Labels: <none> Annotations: kubernetes.io/service-account.name: kubernetes-dashboard kubernetes.io/service-account.uid: 798c2145-63df-4852-b539-00f897b61e4a Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 10 bytes token: aVeryLongTokenString On logging in there's some good news and bad news... Good: We can login with a token and have a nice padlock for TLS via LetsEncrypt Bad: It tells us there's nothing running...?! Creating an Admin user Following the guidance in the docs we will create a sample admin user. I say sample because we want, wherever possible, our users to be defined in our central Identity Provider (which is going to be setup later on). # file: ~/monitoring/admin-user.yaml apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : monitoring --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : monitoring kubectl apply -f ~/monitoring/admin-user.yaml We can now find the token directly from the commandline. kubectl -n monitoring describe secret $( kubectl -n monitoring get secret | grep admin-user | awk '{print $1}' ) Logging back in as this user we can now see everything in the cluster.","title":"Kubernetes Dashboard"},{"location":"01.infrastructure/05.monitoring/01.kubernetes.dashboard/#creating-an-admin-user","text":"Following the guidance in the docs we will create a sample admin user. I say sample because we want, wherever possible, our users to be defined in our central Identity Provider (which is going to be setup later on). # file: ~/monitoring/admin-user.yaml apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : monitoring --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : monitoring kubectl apply -f ~/monitoring/admin-user.yaml We can now find the token directly from the commandline. kubectl -n monitoring describe secret $( kubectl -n monitoring get secret | grep admin-user | awk '{print $1}' ) Logging back in as this user we can now see everything in the cluster.","title":"Creating an Admin user"},{"location":"02.idam/00.idam/","text":"Create the namespace kubectl create ns idam","title":"IDAM Overview"},{"location":"02.idam/01.openldap/","text":"We're going to follow a previous tutorial I wrote for setting up an initial OpenLDAP installation on a VM and then seed an initial admin user. Dockerising this is done now with the osixia/openldap base image which can be found on GitHub . mkdir -p ~/idam ; \\ cd ~/idam OpenLDAP manifest # file: ~/idam/ldap.yaml apiVersion : v1 kind : ConfigMap metadata : name : ldap-config namespace : idam labels : app : ldap tier : backend data : LDAP_ORGANISATION : James Veitch LDAP_DOMAIN : jamesveitch.dev LDAP_ADMIN_PASSWORD : admin LDAP_CONFIG_PASSWORD : config --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-data-pv-claim namespace : idam labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 20Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-config-pv-claim namespace : idam labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : ldap-certs-pv-claim namespace : idam labels : app : ldap tier : backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Service metadata : name : ldap namespace : idam labels : app : ldap tier : backend spec : selector : app : ldap tier : backend ports : - name : ldap protocol : TCP port : 389 targetPort : 389 - name : ldaps-tcp protocol : TCP port : 636 targetPort : 636 - name : ldaps protocol : UDP port : 636 targetPort : 636 --- apiVersion : apps/v1 kind : Deployment metadata : name : ldap-deployment namespace : idam labels : app : ldap tier : backend spec : replicas : 1 selector : matchLabels : app : ldap tier : backend strategy : type : Recreate template : metadata : labels : app : ldap tier : backend spec : containers : - name : ldap image : osixia/openldap envFrom : - configMapRef : name : ldap-config ports : - containerPort : 389 name : ldap - containerPort : 636 name : ldaps volumeMounts : - name : ldap-data mountPath : /var/lib/ldap - name : ldap-config mountPath : /etc/ldap/slapd.d - name : ldap-certs mountPath : /container/service/slapd/assets/certs volumes : - name : ldap-data persistentVolumeClaim : claimName : ldap-data-pv-claim - name : ldap-config persistentVolumeClaim : claimName : ldap-config-pv-claim - name : ldap-certs persistentVolumeClaim : claimName : ldap-certs-pv-claim Apply this manifest with a ~/idam/ldap.yaml and then wait for the services and pods to spin up. Admin If you haven't used a pre-canned LDIF file to seed the initial database we can use a friendly admin tool to quickly add some content into the LDAP backend. LDAP Admin manifest # file: ~/idam/ldap-admin.yaml apiVersion : v1 kind : ConfigMap metadata : name : ldapadmin-config namespace : idam labels : app : ldap tier : frontend data : PHPLDAPADMIN_LDAP_HOSTS : \"ldap.idam\" PHPLDAPADMIN_HTTPS : \"false\" --- apiVersion : v1 kind : Service metadata : name : ldapadmin namespace : idam labels : app : ldap tier : frontend spec : type : LoadBalancer selector : app : ldap tier : frontend ports : - name : http protocol : TCP port : 80 targetPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : ldapadmin-deployment namespace : idam labels : app : ldap tier : frontend spec : replicas : 1 selector : matchLabels : app : ldap tier : frontend strategy : type : Recreate template : metadata : labels : app : ldap tier : frontend spec : containers : - name : ldapadmin image : osixia/phpldapadmin envFrom : - configMapRef : name : ldapadmin-config ports : - containerPort : 80 name : http This creates an internal (to the LAN) Service using a LoadBalancer so it appears on the network via MetalLB. To get the External-IP query the services in the namespace. $ kubectl get svc -n idam NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ldap ClusterIP 10 .96.236.102 <none> 389 /TCP,636/TCP,636/UDP 14h ldapadmin LoadBalancer 10 .96.165.63 192 .168.0.201 80 :30158/TCP 14h To login to the admin panel you'll need to use the username of cn=admin,dc=jamesveitch,dc=dev and then the password. One the left you should see something similar to the below. Click to Create new entry here so we can add the necessary Organisational Units we'll require later on. In the screen that appears choose Generic: Organisational Unit and then call it People . Click through the screens and commit to the database. Generic: Organisational Unit If you click on this new ou=People icon and then Add new attribute you'll see down the bottom we can select description from a drop-down box. Type in something descriptive. LDAP tree where users are Add description Additional entries After adding the above, we need to create the following structure. Organisational Units : Create these in the same way as above ( Generic: Organisational Unit and then add a description attribute). People : LDAP tree where users are Groups : LDAP tree for Groups that users belong to RealmRoles : Keycloak roles that users have Groups : These sit inside the Groups OU tree and should be created as a Posix Group . users : Standard group for all users admins : Holds general administrators for the domain auth-admins : Sits inside RealmRoles OU. Administrators for Keycloak and LDAP Users : These sit inside the People OU tree and should be created as a Generic: User Account jamesveitch : Obviously replace with another name... This will be our standard admin user going forwards. Add them to the users group initially (where it says GID ). Make note of your user's Distinguished Name (DN) as we need this. It will look something like cn=James Veitch,ou=People,dc=jamesveitch,dc=dev . Now go into each of users , admins and auth-admins Groups and add our user into them with Add new attribute \u2192 memberUid and use the DN. Export configuration as LDIF The really helpful thing about this admin interface is that it contains an Export functionality we can use to save down a copy of our databse for subsequent seeding / backup. Make sure you set the following: Base DN : use the root dc=jamesveitch,dc=dev Search Scope : Select the entire subtree Attributes : Keep the wildcard * Save as file : Select this Export format : LDIF Line ends : UNIX Export TODO: Seed the database with an LDIF See seed-ldap-database-with-ldif from the docs.","title":"OpenLDAP"},{"location":"02.idam/01.openldap/#admin","text":"If you haven't used a pre-canned LDIF file to seed the initial database we can use a friendly admin tool to quickly add some content into the LDAP backend. LDAP Admin manifest # file: ~/idam/ldap-admin.yaml apiVersion : v1 kind : ConfigMap metadata : name : ldapadmin-config namespace : idam labels : app : ldap tier : frontend data : PHPLDAPADMIN_LDAP_HOSTS : \"ldap.idam\" PHPLDAPADMIN_HTTPS : \"false\" --- apiVersion : v1 kind : Service metadata : name : ldapadmin namespace : idam labels : app : ldap tier : frontend spec : type : LoadBalancer selector : app : ldap tier : frontend ports : - name : http protocol : TCP port : 80 targetPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : ldapadmin-deployment namespace : idam labels : app : ldap tier : frontend spec : replicas : 1 selector : matchLabels : app : ldap tier : frontend strategy : type : Recreate template : metadata : labels : app : ldap tier : frontend spec : containers : - name : ldapadmin image : osixia/phpldapadmin envFrom : - configMapRef : name : ldapadmin-config ports : - containerPort : 80 name : http This creates an internal (to the LAN) Service using a LoadBalancer so it appears on the network via MetalLB. To get the External-IP query the services in the namespace. $ kubectl get svc -n idam NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ldap ClusterIP 10 .96.236.102 <none> 389 /TCP,636/TCP,636/UDP 14h ldapadmin LoadBalancer 10 .96.165.63 192 .168.0.201 80 :30158/TCP 14h To login to the admin panel you'll need to use the username of cn=admin,dc=jamesveitch,dc=dev and then the password. One the left you should see something similar to the below. Click to Create new entry here so we can add the necessary Organisational Units we'll require later on. In the screen that appears choose Generic: Organisational Unit and then call it People . Click through the screens and commit to the database. Generic: Organisational Unit If you click on this new ou=People icon and then Add new attribute you'll see down the bottom we can select description from a drop-down box. Type in something descriptive. LDAP tree where users are Add description","title":"Admin"},{"location":"02.idam/01.openldap/#additional-entries","text":"After adding the above, we need to create the following structure. Organisational Units : Create these in the same way as above ( Generic: Organisational Unit and then add a description attribute). People : LDAP tree where users are Groups : LDAP tree for Groups that users belong to RealmRoles : Keycloak roles that users have Groups : These sit inside the Groups OU tree and should be created as a Posix Group . users : Standard group for all users admins : Holds general administrators for the domain auth-admins : Sits inside RealmRoles OU. Administrators for Keycloak and LDAP Users : These sit inside the People OU tree and should be created as a Generic: User Account jamesveitch : Obviously replace with another name... This will be our standard admin user going forwards. Add them to the users group initially (where it says GID ). Make note of your user's Distinguished Name (DN) as we need this. It will look something like cn=James Veitch,ou=People,dc=jamesveitch,dc=dev . Now go into each of users , admins and auth-admins Groups and add our user into them with Add new attribute \u2192 memberUid and use the DN.","title":"Additional entries"},{"location":"02.idam/01.openldap/#export-configuration-as-ldif","text":"The really helpful thing about this admin interface is that it contains an Export functionality we can use to save down a copy of our databse for subsequent seeding / backup. Make sure you set the following: Base DN : use the root dc=jamesveitch,dc=dev Search Scope : Select the entire subtree Attributes : Keep the wildcard * Save as file : Select this Export format : LDIF Line ends : UNIX Export","title":"Export configuration as LDIF"},{"location":"02.idam/01.openldap/#todo-seed-the-database-with-an-ldif","text":"See seed-ldap-database-with-ldif from the docs.","title":"TODO: Seed the database with an LDIF"},{"location":"02.idam/02.keycloak/","text":"We need the following: keycloak postgres postgres (backup) Documentation is available on Docker Hub Deploy Keycloak Keycloak manifest # file: ~/idam/keycloak.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : keycloak namespace : idam spec : secretName : keycloak-certs duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : auth.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - auth.jamesveitch.dev issuerRef : name : letsencrypt kind : ClusterIssuer group : cert-manager.io --- apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : keycloak-external-ingress namespace : idam labels : app : keycloak tier : frontend annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - auth.jamesveitch.dev secretName : keycloak-certs rules : - host : auth.jamesveitch.dev http : paths : - path : / backend : serviceName : keycloak servicePort : 8080 --- apiVersion : v1 kind : Service metadata : name : keycloak namespace : idam labels : app : keycloak tier : frontend spec : selector : app : keycloak tier : frontend ports : - name : http protocol : TCP port : 8080 targetPort : 8080 --- apiVersion : v1 kind : ConfigMap metadata : name : keycloak-config namespace : idam labels : app : keycloak tier : frontend data : DB_ADDR : keycloak-db.idam DB_USER : postgres DB_PASSWORD : postgres KEYCLOAK_USER : admin KEYCLOAK_PASSWORD : password # This is required to run keycloak behind a service,ingress etc PROXY_ADDRESS_FORWARDING : \"true\" --- apiVersion : apps/v1 kind : Deployment metadata : name : keycloak-deployment namespace : idam labels : app : keycloak tier : frontend spec : replicas : 1 selector : matchLabels : app : keycloak tier : frontend strategy : type : Recreate template : metadata : labels : app : keycloak tier : frontend spec : containers : - name : keycloak image : jboss/keycloak envFrom : - configMapRef : name : keycloak-config ports : - containerPort : 8080 name : keycloak --- apiVersion : v1 kind : ConfigMap metadata : name : keycloak-db-config namespace : idam labels : app : keycloak tier : postgres data : POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres POSTGRES_DB : keycloak PGDATA : /var/lib/postgresql/data/pgdata --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : keycloak-db-data-pv-claim namespace : idam labels : app : keycloak tier : postgres spec : accessModes : - ReadWriteOnce resources : requests : storage : 20Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : keycloak-db-config-pv-claim namespace : idam labels : app : keycloak tier : postgres spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Service metadata : name : keycloak-db namespace : idam labels : app : keycloak tier : postgres spec : selector : app : keycloak tier : postgres ports : - name : postgres protocol : TCP port : 5432 targetPort : 5432 --- apiVersion : apps/v1 kind : Deployment metadata : name : keycloak-db-deployment namespace : idam labels : app : keycloak tier : postgres spec : replicas : 1 selector : matchLabels : app : keycloak tier : postgres strategy : type : Recreate template : metadata : labels : app : keycloak tier : postgres spec : containers : - name : keycloak-db image : postgres:alpine envFrom : - configMapRef : name : keycloak-db-config ports : - containerPort : 5432 name : postgres volumeMounts : - name : keycloak-db-data mountPath : /var/lib/postgresql/data - name : keycloak-db-config mountPath : /usr/share/postgresql volumes : - name : keycloak-db-data persistentVolumeClaim : claimName : keycloak-db-data-pv-claim - name : keycloak-db-config persistentVolumeClaim : claimName : keycloak-db-config-pv-claim Apply the manifest with kubectl apply -f ~/idam/keycloak.yaml and then wait for the pods, services, ingress and certificate to be provisioned. The keycloak container can take up to a minute to properly start up so you can monitor this be looking at the logs for the keycloak-deployment* pod. $ kubectl get pod -n idam NAME READY STATUS RESTARTS AGE keycloak-db-deployment-5d8846dbf9-2dfdh 1 /1 Running 0 14h keycloak-deployment-88bc75877-f8cbr 1 /1 Running 0 14h ldap-deployment-7864dd96cf-9jvmz 1 /1 Running 0 15h ldapadmin-deployment-7575c6d9dc-vb47b 1 /1 Running 0 15h $ kubectl -n idam logs keycloak-deployment-88bc75877-f8cbr User Federation Identity Provider (LDAP) To configure Keycloak to work with OpenLDAP we need to login and setup our ldap container as a User Federation provider. For the moment we will use the Master realm but, if wanted, you could create a new realm and perform this inside. Settings Key settings you'll need are as follows: Vendor : Other Edit Mode : WRITABLE Connection URL : ldap://ldap.idam Users DN : ou=People,dc=jamesveitch,dc=dev Bind DN : cn=admin,dc=jamesveitch,dc=dev Bind Credential : Identity Provider Configuration (Settings) Mappers Mappers help to provide a translation layer between how data is stored in the provider and how we'd like to use it in Keycloak. For example, there are some standard ones created for you automatically based on your selection of the Vendor in the previous Settings tab. Clicking on the username will show you that: Type : It's a user-attribute mapper (used to map a single attribute from a LDAP user to the Keycloak user model) Model Attribute : This is what the attribute will be called in Keycloak LDAP Attribute : What the attribute is called in LDAP Username attribute-ldap-mapper We are going to add some of our own custom mappers so that we can do things like identify what security groups a user is part of and, therefore, what resources they should be able to access inside Kubernetes. First though a couple of the defaults need tweaking. First Name : Currently points at our cn for our users and this will pull back James Veitch . We need to modify, per our LDAP schema, to use givenName instead as the LDAP Attribute . Create Groups and Mappers One of the great things about Keycloak is you can also use it as a frontend to edit back into the LDAP database (without needing a separate phpldapadmin deployment - you'll still need this for managing things like OUs and other structural items). As we created a number of Groups in our previous step (or have seeded with an LDIF) we need to tell Keycloak how to find and interpret them. Go back into our User Federation \u2192 Ldap \u2192 Mappers and create a new one. Name : posix groups Type : group-ldap-mapper LDAP Groups DN : ou=Groups,dc=jamesveitch,dc=dev Group Object Classes : posixGroup Membership LDAP Attribute : memberuid Click on the Sync LDAP Groups To Keycloak button down the bottom and ensure you get a success overlay message that it's now imported the Groups we created earlier (3 of them). Go to the Settings tab and then, down the bottom, click Synchronize all users to get a message about one user being updated (because it's now spotted some groups we're part of). Groups Go to Groups . You should see 4 items for our People OU and then the 3 groups we created. Double-click on auth-admins to edit it and select Role Mappings . Add the admin role from Available Roles to the Assigned Roles . You'll see this now gives use some Effective Roles to create a new realm or administrate one. Group Role Mappings Users If you now go to Users and select you user yo should note on the Groups tab you have a Group Membership identified for all of the groups you're part of. In addition you have admin as an Effective Role in the Role Mappings tab because it's mapped this through from your membership of the auth-admins group. Go to Users and then click View all users to force it to perform an initial sync. You should see something similar to the below with an initial admin user (if using Master realm). User: Group Membership User: Role Mapping Bootstrapping We'll, later, bootstrap this into Keycloak via the Importing a realm option. The easiest way to get your configuration back out of the system is to go the Export setting and then get the JSON output. Unfortunately, due to the way in which the image is configured, the given method in the docs doesn't work (as volumes are mounted by root yet the application executes as the jboss user and therefore can't access the files). As a result we'll inherit from and build a custom image with a /realms folder that we can mount the JSON files into. #TODO: update with custom image. Enabling MFA Stealing with pride from documentation elsewhere we're going to enable TOTP based MFA for our initial user. In the GUI you would navigate to Authentication \u2192 OTP Policy and then update the following settings as required. The below are those we're using: OTP Type : Time Based OTP Hash Algorith : SHA512 Number of Digits : 8 Look Ahead Window : 3 OTP Token Period : 30 Depending on appetite we can also navigate to the Authentication \u2192 Required Actions tab and tick the Default Action box against Configure OTP if we want to enforce this for everyone by default. Impersonate user Navigate to Users and then select Impersonate next to ours. This should change and give you a different screen where we can now setup MFA for our user. Select the Authenticator option and follow the instructions to get setup. Login Logout using the Sign Out button in the top right of the screen and then attempt to sign back in with your new user. You'll be presented with the below requiring you to input a code from your app. Accessing account and admin console The admin console and account details can be accessed from the following urls: Security Admin Console [Account]( http://keycloak.jamesveitch.local:8080/auth/realms/homelab/accountdev","title":"Keycloak"},{"location":"02.idam/02.keycloak/#deploy-keycloak","text":"Keycloak manifest # file: ~/idam/keycloak.yaml apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : keycloak namespace : idam spec : secretName : keycloak-certs duration : 2160h # 90d renewBefore : 360h # 15d organization : - jamesveitch commonName : auth.jamesveitch.dev isCA : false keySize : 2048 keyAlgorithm : rsa keyEncoding : pkcs1 usages : - server auth - client auth dnsNames : - auth.jamesveitch.dev issuerRef : name : letsencrypt kind : ClusterIssuer group : cert-manager.io --- apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : keycloak-external-ingress namespace : idam labels : app : keycloak tier : frontend annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"letsencrypt\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" spec : tls : - hosts : - auth.jamesveitch.dev secretName : keycloak-certs rules : - host : auth.jamesveitch.dev http : paths : - path : / backend : serviceName : keycloak servicePort : 8080 --- apiVersion : v1 kind : Service metadata : name : keycloak namespace : idam labels : app : keycloak tier : frontend spec : selector : app : keycloak tier : frontend ports : - name : http protocol : TCP port : 8080 targetPort : 8080 --- apiVersion : v1 kind : ConfigMap metadata : name : keycloak-config namespace : idam labels : app : keycloak tier : frontend data : DB_ADDR : keycloak-db.idam DB_USER : postgres DB_PASSWORD : postgres KEYCLOAK_USER : admin KEYCLOAK_PASSWORD : password # This is required to run keycloak behind a service,ingress etc PROXY_ADDRESS_FORWARDING : \"true\" --- apiVersion : apps/v1 kind : Deployment metadata : name : keycloak-deployment namespace : idam labels : app : keycloak tier : frontend spec : replicas : 1 selector : matchLabels : app : keycloak tier : frontend strategy : type : Recreate template : metadata : labels : app : keycloak tier : frontend spec : containers : - name : keycloak image : jboss/keycloak envFrom : - configMapRef : name : keycloak-config ports : - containerPort : 8080 name : keycloak --- apiVersion : v1 kind : ConfigMap metadata : name : keycloak-db-config namespace : idam labels : app : keycloak tier : postgres data : POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres POSTGRES_DB : keycloak PGDATA : /var/lib/postgresql/data/pgdata --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : keycloak-db-data-pv-claim namespace : idam labels : app : keycloak tier : postgres spec : accessModes : - ReadWriteOnce resources : requests : storage : 20Gi --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : keycloak-db-config-pv-claim namespace : idam labels : app : keycloak tier : postgres spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Service metadata : name : keycloak-db namespace : idam labels : app : keycloak tier : postgres spec : selector : app : keycloak tier : postgres ports : - name : postgres protocol : TCP port : 5432 targetPort : 5432 --- apiVersion : apps/v1 kind : Deployment metadata : name : keycloak-db-deployment namespace : idam labels : app : keycloak tier : postgres spec : replicas : 1 selector : matchLabels : app : keycloak tier : postgres strategy : type : Recreate template : metadata : labels : app : keycloak tier : postgres spec : containers : - name : keycloak-db image : postgres:alpine envFrom : - configMapRef : name : keycloak-db-config ports : - containerPort : 5432 name : postgres volumeMounts : - name : keycloak-db-data mountPath : /var/lib/postgresql/data - name : keycloak-db-config mountPath : /usr/share/postgresql volumes : - name : keycloak-db-data persistentVolumeClaim : claimName : keycloak-db-data-pv-claim - name : keycloak-db-config persistentVolumeClaim : claimName : keycloak-db-config-pv-claim Apply the manifest with kubectl apply -f ~/idam/keycloak.yaml and then wait for the pods, services, ingress and certificate to be provisioned. The keycloak container can take up to a minute to properly start up so you can monitor this be looking at the logs for the keycloak-deployment* pod. $ kubectl get pod -n idam NAME READY STATUS RESTARTS AGE keycloak-db-deployment-5d8846dbf9-2dfdh 1 /1 Running 0 14h keycloak-deployment-88bc75877-f8cbr 1 /1 Running 0 14h ldap-deployment-7864dd96cf-9jvmz 1 /1 Running 0 15h ldapadmin-deployment-7575c6d9dc-vb47b 1 /1 Running 0 15h $ kubectl -n idam logs keycloak-deployment-88bc75877-f8cbr","title":"Deploy Keycloak"},{"location":"02.idam/02.keycloak/#user-federation-identity-provider-ldap","text":"To configure Keycloak to work with OpenLDAP we need to login and setup our ldap container as a User Federation provider. For the moment we will use the Master realm but, if wanted, you could create a new realm and perform this inside.","title":"User Federation Identity Provider (LDAP)"},{"location":"02.idam/02.keycloak/#settings","text":"Key settings you'll need are as follows: Vendor : Other Edit Mode : WRITABLE Connection URL : ldap://ldap.idam Users DN : ou=People,dc=jamesveitch,dc=dev Bind DN : cn=admin,dc=jamesveitch,dc=dev Bind Credential : Identity Provider Configuration (Settings)","title":"Settings"},{"location":"02.idam/02.keycloak/#mappers","text":"Mappers help to provide a translation layer between how data is stored in the provider and how we'd like to use it in Keycloak. For example, there are some standard ones created for you automatically based on your selection of the Vendor in the previous Settings tab. Clicking on the username will show you that: Type : It's a user-attribute mapper (used to map a single attribute from a LDAP user to the Keycloak user model) Model Attribute : This is what the attribute will be called in Keycloak LDAP Attribute : What the attribute is called in LDAP Username attribute-ldap-mapper We are going to add some of our own custom mappers so that we can do things like identify what security groups a user is part of and, therefore, what resources they should be able to access inside Kubernetes. First though a couple of the defaults need tweaking. First Name : Currently points at our cn for our users and this will pull back James Veitch . We need to modify, per our LDAP schema, to use givenName instead as the LDAP Attribute .","title":"Mappers"},{"location":"02.idam/02.keycloak/#create-groups-and-mappers","text":"One of the great things about Keycloak is you can also use it as a frontend to edit back into the LDAP database (without needing a separate phpldapadmin deployment - you'll still need this for managing things like OUs and other structural items). As we created a number of Groups in our previous step (or have seeded with an LDIF) we need to tell Keycloak how to find and interpret them. Go back into our User Federation \u2192 Ldap \u2192 Mappers and create a new one. Name : posix groups Type : group-ldap-mapper LDAP Groups DN : ou=Groups,dc=jamesveitch,dc=dev Group Object Classes : posixGroup Membership LDAP Attribute : memberuid Click on the Sync LDAP Groups To Keycloak button down the bottom and ensure you get a success overlay message that it's now imported the Groups we created earlier (3 of them). Go to the Settings tab and then, down the bottom, click Synchronize all users to get a message about one user being updated (because it's now spotted some groups we're part of).","title":"Create Groups and Mappers"},{"location":"02.idam/02.keycloak/#groups","text":"Go to Groups . You should see 4 items for our People OU and then the 3 groups we created. Double-click on auth-admins to edit it and select Role Mappings . Add the admin role from Available Roles to the Assigned Roles . You'll see this now gives use some Effective Roles to create a new realm or administrate one. Group Role Mappings","title":"Groups"},{"location":"02.idam/02.keycloak/#users","text":"If you now go to Users and select you user yo should note on the Groups tab you have a Group Membership identified for all of the groups you're part of. In addition you have admin as an Effective Role in the Role Mappings tab because it's mapped this through from your membership of the auth-admins group. Go to Users and then click View all users to force it to perform an initial sync. You should see something similar to the below with an initial admin user (if using Master realm). User: Group Membership User: Role Mapping","title":"Users"},{"location":"02.idam/02.keycloak/#bootstrapping","text":"We'll, later, bootstrap this into Keycloak via the Importing a realm option. The easiest way to get your configuration back out of the system is to go the Export setting and then get the JSON output. Unfortunately, due to the way in which the image is configured, the given method in the docs doesn't work (as volumes are mounted by root yet the application executes as the jboss user and therefore can't access the files). As a result we'll inherit from and build a custom image with a /realms folder that we can mount the JSON files into. #TODO: update with custom image.","title":"Bootstrapping"},{"location":"02.idam/02.keycloak/#enabling-mfa","text":"Stealing with pride from documentation elsewhere we're going to enable TOTP based MFA for our initial user. In the GUI you would navigate to Authentication \u2192 OTP Policy and then update the following settings as required. The below are those we're using: OTP Type : Time Based OTP Hash Algorith : SHA512 Number of Digits : 8 Look Ahead Window : 3 OTP Token Period : 30 Depending on appetite we can also navigate to the Authentication \u2192 Required Actions tab and tick the Default Action box against Configure OTP if we want to enforce this for everyone by default.","title":"Enabling MFA"},{"location":"02.idam/02.keycloak/#impersonate-user","text":"Navigate to Users and then select Impersonate next to ours. This should change and give you a different screen where we can now setup MFA for our user. Select the Authenticator option and follow the instructions to get setup.","title":"Impersonate user"},{"location":"02.idam/02.keycloak/#login","text":"Logout using the Sign Out button in the top right of the screen and then attempt to sign back in with your new user. You'll be presented with the below requiring you to input a code from your app.","title":"Login"},{"location":"02.idam/02.keycloak/#accessing-account-and-admin-console","text":"The admin console and account details can be accessed from the following urls: Security Admin Console [Account]( http://keycloak.jamesveitch.local:8080/auth/realms/homelab/accountdev","title":"Accessing account and admin console"},{"location":"98.wip/00.external.dns/","text":"We're going to install the ExternalDNS addon for Kubernetes in order to make our services discoverable externally via public DNS. ... it retrieves a list of resources (Services, Ingresses, etc.) from the Kubernetes API to determine a desired list of DNS records. So it uses annotations inside Kubernetes in order to identify the services you;d like to expose. ... allows you to control DNS records dynamically via Kubernetes resources in a DNS provider-agnostic way. And then, using these annotations, will configure an external DNS provider of your choice to populate the necessary records to expose your services. At time of writing the current version is v0.5 . This provides Beta support for Cloudflare . The Setting up ExternalDNS for Services on Cloudflare tutorial will be used as a starting point. Because though we're using an ingress we will need to adapt it slightly to use an ingress as opposed to service . Cloudflare Login to Cloudflare and generate an API Token. As per the docs: When using API Token authentication the token should be granted Zone Read and DNS Edit privileges API Token will be preferred for authentication if CF_API_TOKEN environment variable is set. Otherwise CF_API_KEY and CF_API_EMAIL should be set to run ExternalDNS with Cloudflare. Make note of this token as we'll need it below. In addition there's an open issue #1127 which highlights. for now, the API token must be granted for all zone, cannot only to a specific zone. We're going to use Kubernetes secrets in order to store our api credentials from Cloudflare. kubectl create secret generic cloudflare-api-token --from-literal = CF_API_TOKEN = Aq6g43899Am0567SURF2diL2DSZBGEYIAUHdmluaB This should now be encoded and stored. To check, run a describe. $ kubectl get secret cloudflare-api-token -o yaml apiVersion: v1 data: CF_API_TOKEN: QXE2Z21VRUFtMFN3dkhTVVJGWGRpTDJEU1pCR0VZSUFVSGRtbHVhQg == kind: Secret metadata: creationTimestamp: \"2019-12-02T09:00:15Z\" name: cloudflare-api-token namespace: default resourceVersion: \"2983185\" selfLink: /api/v1/namespaces/default/secrets/cloudflare-api-token uid: 2917b431-0bd9-4fcc-8c2b-a401c8a3a3c3 type: Opaque # file: ~/external-dns/external-dns.yaml apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest args : # - --source=service # ingress is also possible - --source=ingress - --domain-filter=jamesveitch.dev # (optional) limit to only jamesveitch.dev domains; change to match the zone created above. - --provider=cloudflare - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...) # - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --log-level=debug - --interval=5s env : # - name: CF_API_KEY # value: \"YOUR_CLOUDFLARE_API_KEY\" # - name: CF_API_EMAIL # value: \"YOUR_CLOUDFLARE_EMAIL\" - name : CF_API_TOKEN valueFrom : secretKeyRef : name : cloudflare-api-token key : CF_API_TOKEN # now apply the manifest kubectl create -f ~/external-dns/external-dns.yaml As we've set this up to monitor --source=service (as opposed to ingress ) we now need to modify the service manifest for applications we'd like to expose. Let's modify the whoami spec and add the necessary annotations. For those of you who have used Traefik with Docker or DockerCompose in the past this is a very similar concept. apiVersion : v1 kind : Service metadata : name : whoami annotations : external-dns.alpha.kubernetes.io/hostname : whoami.jamesveitch.dev external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 kubectl logs -l 'app=external-dns'","title":"00.external.dns"},{"location":"98.wip/00.external.dns/#cloudflare","text":"Login to Cloudflare and generate an API Token. As per the docs: When using API Token authentication the token should be granted Zone Read and DNS Edit privileges API Token will be preferred for authentication if CF_API_TOKEN environment variable is set. Otherwise CF_API_KEY and CF_API_EMAIL should be set to run ExternalDNS with Cloudflare. Make note of this token as we'll need it below. In addition there's an open issue #1127 which highlights. for now, the API token must be granted for all zone, cannot only to a specific zone. We're going to use Kubernetes secrets in order to store our api credentials from Cloudflare. kubectl create secret generic cloudflare-api-token --from-literal = CF_API_TOKEN = Aq6g43899Am0567SURF2diL2DSZBGEYIAUHdmluaB This should now be encoded and stored. To check, run a describe. $ kubectl get secret cloudflare-api-token -o yaml apiVersion: v1 data: CF_API_TOKEN: QXE2Z21VRUFtMFN3dkhTVVJGWGRpTDJEU1pCR0VZSUFVSGRtbHVhQg == kind: Secret metadata: creationTimestamp: \"2019-12-02T09:00:15Z\" name: cloudflare-api-token namespace: default resourceVersion: \"2983185\" selfLink: /api/v1/namespaces/default/secrets/cloudflare-api-token uid: 2917b431-0bd9-4fcc-8c2b-a401c8a3a3c3 type: Opaque # file: ~/external-dns/external-dns.yaml apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest args : # - --source=service # ingress is also possible - --source=ingress - --domain-filter=jamesveitch.dev # (optional) limit to only jamesveitch.dev domains; change to match the zone created above. - --provider=cloudflare - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...) # - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --log-level=debug - --interval=5s env : # - name: CF_API_KEY # value: \"YOUR_CLOUDFLARE_API_KEY\" # - name: CF_API_EMAIL # value: \"YOUR_CLOUDFLARE_EMAIL\" - name : CF_API_TOKEN valueFrom : secretKeyRef : name : cloudflare-api-token key : CF_API_TOKEN # now apply the manifest kubectl create -f ~/external-dns/external-dns.yaml As we've set this up to monitor --source=service (as opposed to ingress ) we now need to modify the service manifest for applications we'd like to expose. Let's modify the whoami spec and add the necessary annotations. For those of you who have used Traefik with Docker or DockerCompose in the past this is a very similar concept. apiVersion : v1 kind : Service metadata : name : whoami annotations : external-dns.alpha.kubernetes.io/hostname : whoami.jamesveitch.dev external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : whoami ports : - protocol : TCP port : 80 targetPort : 80 kubectl logs -l 'app=external-dns'","title":"Cloudflare"},{"location":"98.wip/03.ingress.and.service.mesh/","text":"As described in the official kubernetes docs an Ingress object manages external access to services in a cluster, typically HTTP. They can provide load balancing, SSL termination and name-based virtual hosting. We're going to combine a couple of sections of the official docs to deploy the following: Istio ingress with LetsEncrypt via cert-manager ( docs ); Mutual TLS between pods ( docs ) Istio's CNI, so that privileged injection for sidecars no longer needed, ( docs ) Install mkdir -p ~/istio ; \\ cd ~/istio curl -L https://git.io/getLatestIstio | sh - export ISTIO_DIR = $( ls -d istio-* ) # enable autocompletion and add to path tee >> ~/.bash_profile <<EOL # add istio to path export PATH=~/istio/$ISTIO_DIR/bin:\\$PATH # add istioctl autocompletion test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash EOL source ~/.bash_profile Now we'll install the new Istio CNI plugin as per the docs with a couple of additional tweaks as outlined for ingress . Istio comes with a number of Installation Configuration Profiles that we want to customise slightly before deploying. First generate a manifest with the settings we want. We're deploying the default profile and adding in the necessary additional components for sds and cni . istioctl manifest generate \\ --set values.gateways.istio-ingressgateway.sds.enabled = true \\ --set values.global.k8sIngress.enabled = true \\ --set values.global.k8sIngress.enableHttps = true \\ --set values.global.k8sIngress.gatewayName = ingressgateway \\ # --set cni.enabled=true \\ > ~/istio/generated-manifest.yaml Now apply istioctl manifest apply -f ~/istio/generated-manifest.yaml the required ClusterRole:istio-cni is not ready Currently using the --set cni-enabled=true flag doesn't seem to work. When you run an istioctl verify-installation -f ~/istio/generated-manifest.yaml it will present you with an error of the required ClusterRole:istio-cni is not ready . To be investigated further... Reviewing the list of services on the cluster we should now see an ingress-gateway created with an EXTERNAL-IP allocated by MetalLB in the istio-system namespace. Running Services $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE default kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 12h istio-system istio-citadel ClusterIP 10 .96.218.252 <none> 8060 /TCP,15014/TCP 82s istio-system istio-galley ClusterIP 10 .96.101.76 <none> 443 /TCP,15014/TCP,9901/TCP,15019/TCP 82s istio-system istio-ingressgateway LoadBalancer 10 .96.113.231 192 .168.0.200 15020 :30088/TCP,80:32168/TCP,443:31899/TCP,.... 80s istio-system istio-pilot ClusterIP 10 .96.232.65 <none> 15010 /TCP,15011/TCP,8080/TCP,15014/TCP 81s istio-system istio-policy ClusterIP 10 .96.42.154 <none> 9091 /TCP,15004/TCP,15014/TCP 80s istio-system istio-sidecar-injector ClusterIP 10 .96.147.243 <none> 443 /TCP 82s istio-system istio-telemetry ClusterIP 10 .96.8.149 <none> 9091 /TCP,15004/TCP,15014/TCP,42422/TCP 80s istio-system prometheus ClusterIP 10 .96.144.9 <none> 9090 /TCP 82s kube-system kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP,9153/TCP 12h We'll now create a deployment and see how many pods are created. # file: ~/istio/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 Apply the manifest with kubectl apply -f ~/istio/whoami.yaml and check the deployment and the number of pods created as a result. $ kubectl get deployment -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR whoami-deployment 1 /1 1 1 13s whoami containous/whoami app = whoami $ kubectl get pod -l app = whoami NAME READY STATUS RESTARTS AGE whoami-deployment-5b4bb9c787-9cwg8 1 /1 Running 0 10s There's 1/1 ready for the pods which means we only have the single container running and sidecar injection is not yet configured. We'll enable it for the default namespace and then delete the pod (forcing it to recreate with a sidecar). $ kubectl label namespace default istio-injection = enabled $ kubectl delete pod -l app = whoami $ kubectl get pod -l app = whoami NAME READY STATUS RESTARTS AGE whoami-deployment-5b4bb9c787-snm9h 2 /2 Running 0 8s The 2/2 shows that we now have an injected sidecar container. We can inspect some details to see more with kubectl describe pod -l app=whoami . You should see the injected istio-proxy container and corresponding volumes. Sidecar Proxy kubectl describe pod -l app = whoami ... Containers : whoami : Container ID : docker://5bbf03517d378e3e615a32396239f9704092f22e81fedcd00907d38891baaf07 Image : containous/whoami Image ID : docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b Port : 80/TCP Host Port : 0/TCP State : Running Started : Fri, 13 Dec 2019 06:47:57 +0000 Ready : True Restart Count : 0 Environment : <none> Mounts : /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro) istio-proxy : Container ID : docker://3e93f0370eecf8a5686e90013e3681c44f3b152891561bde8c56c3defd240d4b Image : docker.io/istio/proxyv2:1.4.2 Image ID : docker-pullable://istio/proxyv2@sha256:c98b4d277b724a2ad0c6bf22008bd02ddeb2525f19e35cdefe8a3181313716e7 Port : 15090/TCP Host Port : 0/TCP ... ... Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 90s default-scheduler Successfully assigned default/whoami-deployment-5b4bb9c787-snm9h to banks.local Normal Pulled 89s kubelet, banks.local Container image \"docker.io/istio/proxyv2:1.4.2\" already present on machine Normal Created 89s kubelet, banks.local Created container istio-init Normal Started 88s kubelet, banks.local Started container istio-init Normal Pulling 87s kubelet, banks.local Pulling image \"containous/whoami\" Normal Pulled 85s kubelet, banks.local Successfully pulled image \"containous/whoami\" Normal Created 85s kubelet, banks.local Created container whoami Normal Started 85s kubelet, banks.local Started container whoami Normal Pulled 85s kubelet, banks.local Container image \"docker.io/istio/proxyv2:1.4.2\" already present on machine Normal Created 84s kubelet, banks.local Created container istio-proxy Normal Started 84s kubelet, banks.local Started container istio-proxy We can now create a Service and Ingress for the deployment so that we can access it externally. # file: ~/istio/whoami-svc.yaml apiVersion : v1 kind : Service metadata : name : whoami labels : app : whoami spec : ports : - protocol : TCP port : 80 name : http selector : app : whoami create it with kubectl apply -f ~/istio/whoami-svc.yaml # file: ~/istio/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : labels : app : whoami annotations : kubernetes.io/ingress.class : istio # cert-manager.io/issuer: \"letsencrypt\" # external-dns.alpha.kubernetes.io/hostname: whoami.jamesveitch.dev name : whoami-ingress spec : rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80 TLS via Cert-Manager Setup a secret to hold our cloudflare api key (dns validation) kubectl create secret generic cloudflare-apikey-secret --from-literal = apikey = MYKEYFROMCLOUDA","title":"03.ingress.and.service.mesh"},{"location":"98.wip/03.ingress.and.service.mesh/#install","text":"mkdir -p ~/istio ; \\ cd ~/istio curl -L https://git.io/getLatestIstio | sh - export ISTIO_DIR = $( ls -d istio-* ) # enable autocompletion and add to path tee >> ~/.bash_profile <<EOL # add istio to path export PATH=~/istio/$ISTIO_DIR/bin:\\$PATH # add istioctl autocompletion test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash EOL source ~/.bash_profile Now we'll install the new Istio CNI plugin as per the docs with a couple of additional tweaks as outlined for ingress . Istio comes with a number of Installation Configuration Profiles that we want to customise slightly before deploying. First generate a manifest with the settings we want. We're deploying the default profile and adding in the necessary additional components for sds and cni . istioctl manifest generate \\ --set values.gateways.istio-ingressgateway.sds.enabled = true \\ --set values.global.k8sIngress.enabled = true \\ --set values.global.k8sIngress.enableHttps = true \\ --set values.global.k8sIngress.gatewayName = ingressgateway \\ # --set cni.enabled=true \\ > ~/istio/generated-manifest.yaml Now apply istioctl manifest apply -f ~/istio/generated-manifest.yaml the required ClusterRole:istio-cni is not ready Currently using the --set cni-enabled=true flag doesn't seem to work. When you run an istioctl verify-installation -f ~/istio/generated-manifest.yaml it will present you with an error of the required ClusterRole:istio-cni is not ready . To be investigated further... Reviewing the list of services on the cluster we should now see an ingress-gateway created with an EXTERNAL-IP allocated by MetalLB in the istio-system namespace. Running Services $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE default kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 12h istio-system istio-citadel ClusterIP 10 .96.218.252 <none> 8060 /TCP,15014/TCP 82s istio-system istio-galley ClusterIP 10 .96.101.76 <none> 443 /TCP,15014/TCP,9901/TCP,15019/TCP 82s istio-system istio-ingressgateway LoadBalancer 10 .96.113.231 192 .168.0.200 15020 :30088/TCP,80:32168/TCP,443:31899/TCP,.... 80s istio-system istio-pilot ClusterIP 10 .96.232.65 <none> 15010 /TCP,15011/TCP,8080/TCP,15014/TCP 81s istio-system istio-policy ClusterIP 10 .96.42.154 <none> 9091 /TCP,15004/TCP,15014/TCP 80s istio-system istio-sidecar-injector ClusterIP 10 .96.147.243 <none> 443 /TCP 82s istio-system istio-telemetry ClusterIP 10 .96.8.149 <none> 9091 /TCP,15004/TCP,15014/TCP,42422/TCP 80s istio-system prometheus ClusterIP 10 .96.144.9 <none> 9090 /TCP 82s kube-system kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP,9153/TCP 12h We'll now create a deployment and see how many pods are created. # file: ~/istio/whoami.yaml apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment labels : app : whoami spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami image : containous/whoami ports : - containerPort : 80 Apply the manifest with kubectl apply -f ~/istio/whoami.yaml and check the deployment and the number of pods created as a result. $ kubectl get deployment -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR whoami-deployment 1 /1 1 1 13s whoami containous/whoami app = whoami $ kubectl get pod -l app = whoami NAME READY STATUS RESTARTS AGE whoami-deployment-5b4bb9c787-9cwg8 1 /1 Running 0 10s There's 1/1 ready for the pods which means we only have the single container running and sidecar injection is not yet configured. We'll enable it for the default namespace and then delete the pod (forcing it to recreate with a sidecar). $ kubectl label namespace default istio-injection = enabled $ kubectl delete pod -l app = whoami $ kubectl get pod -l app = whoami NAME READY STATUS RESTARTS AGE whoami-deployment-5b4bb9c787-snm9h 2 /2 Running 0 8s The 2/2 shows that we now have an injected sidecar container. We can inspect some details to see more with kubectl describe pod -l app=whoami . You should see the injected istio-proxy container and corresponding volumes. Sidecar Proxy kubectl describe pod -l app = whoami ... Containers : whoami : Container ID : docker://5bbf03517d378e3e615a32396239f9704092f22e81fedcd00907d38891baaf07 Image : containous/whoami Image ID : docker-pullable://containous/whoami@sha256:c0d68a0f9acde95c5214bd057fd3ff1c871b2ef12dae2a9e2d2a3240fdd9214b Port : 80/TCP Host Port : 0/TCP State : Running Started : Fri, 13 Dec 2019 06:47:57 +0000 Ready : True Restart Count : 0 Environment : <none> Mounts : /var/run/secrets/kubernetes.io/serviceaccount from default-token-k9gvj (ro) istio-proxy : Container ID : docker://3e93f0370eecf8a5686e90013e3681c44f3b152891561bde8c56c3defd240d4b Image : docker.io/istio/proxyv2:1.4.2 Image ID : docker-pullable://istio/proxyv2@sha256:c98b4d277b724a2ad0c6bf22008bd02ddeb2525f19e35cdefe8a3181313716e7 Port : 15090/TCP Host Port : 0/TCP ... ... Events : Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 90s default-scheduler Successfully assigned default/whoami-deployment-5b4bb9c787-snm9h to banks.local Normal Pulled 89s kubelet, banks.local Container image \"docker.io/istio/proxyv2:1.4.2\" already present on machine Normal Created 89s kubelet, banks.local Created container istio-init Normal Started 88s kubelet, banks.local Started container istio-init Normal Pulling 87s kubelet, banks.local Pulling image \"containous/whoami\" Normal Pulled 85s kubelet, banks.local Successfully pulled image \"containous/whoami\" Normal Created 85s kubelet, banks.local Created container whoami Normal Started 85s kubelet, banks.local Started container whoami Normal Pulled 85s kubelet, banks.local Container image \"docker.io/istio/proxyv2:1.4.2\" already present on machine Normal Created 84s kubelet, banks.local Created container istio-proxy Normal Started 84s kubelet, banks.local Started container istio-proxy We can now create a Service and Ingress for the deployment so that we can access it externally. # file: ~/istio/whoami-svc.yaml apiVersion : v1 kind : Service metadata : name : whoami labels : app : whoami spec : ports : - protocol : TCP port : 80 name : http selector : app : whoami create it with kubectl apply -f ~/istio/whoami-svc.yaml # file: ~/istio/whoami-ingress.yaml apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : labels : app : whoami annotations : kubernetes.io/ingress.class : istio # cert-manager.io/issuer: \"letsencrypt\" # external-dns.alpha.kubernetes.io/hostname: whoami.jamesveitch.dev name : whoami-ingress spec : rules : - host : whoami.jamesveitch.dev http : paths : - path : / backend : serviceName : whoami servicePort : 80","title":"Install"},{"location":"98.wip/03.ingress.and.service.mesh/#tls-via-cert-manager","text":"Setup a secret to hold our cloudflare api key (dns validation) kubectl create secret generic cloudflare-apikey-secret --from-literal = apikey = MYKEYFROMCLOUDA","title":"TLS via Cert-Manager"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/","text":"Installing Calicoctl See the calico docs for more details. mkdir -p ~/calico ; \\ cd ~/calico # Install as a pod wget https://docs.projectcalico.org/v3.10/manifests/calicoctl.yaml ; \\ kubectl apply -f ~/calico/calicoctl.yaml You can then run commands using kubectl as shown below. alicoctl $ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get profiles -o wide NAME LABELS kns.default kns.kube-node-lease kns.kube-public kns.kube-system ksa.default.default ksa.kube-node-lease.default ksa.kube-public.default ksa.kube-system.attachdetach-controller ksa.kube-system.bootstrap-signer ksa.kube-system.calicoctl ksa.kube-system.canal ksa.kube-system.certificate-controller ksa.kube-system.clusterrole-aggregation-controller ksa.kube-system.coredns ksa.kube-system.cronjob-controller ksa.kube-system.daemon-set-controller ksa.kube-system.default ksa.kube-system.deployment-controller ksa.kube-system.disruption-controller ksa.kube-system.endpoint-controller ksa.kube-system.expand-controller ksa.kube-system.generic-garbage-collector ksa.kube-system.horizontal-pod-autoscaler ksa.kube-system.job-controller ksa.kube-system.kube-proxy ksa.kube-system.namespace-controller ksa.kube-system.node-controller ksa.kube-system.persistent-volume-binder ksa.kube-system.pod-garbage-collector ksa.kube-system.pv-protection-controller ksa.kube-system.pvc-protection-controller ksa.kube-system.replicaset-controller ksa.kube-system.replication-controller ksa.kube-system.resourcequota-controller ksa.kube-system.service-account-controller ksa.kube-system.service-controller ksa.kube-system.statefulset-controller ksa.kube-system.token-cleaner ksa.kube-system.ttl-controller We'll create an alias of the kubectl/calicoctl command for sanity. tee ~/.bash_profile <<EOF # Load .bashrc if it exists test -f ~/.bashrc && source ~/.bashrc # Load .bash_aliases if it exists test -f ~/.bash_aliases && source ~/.bash_aliases EOF tee ~/.bash_aliases <<EOF # Calico alias alias calicoctl=\"kubectl exec -i -n kube-system calicoctl /calicoctl -- \" EOF Note: In order to use manifests with this alias you need to redirect them. E.g. calicoctl create -f - < my_manifest.yaml Enable application layer policy (ALP) Application layer policy for Calico allows you to write policies that enforce against application layer attributes like HTTP methods or paths as well as against cryptographically secure identities. Support for application layer policy is not enabled by default in Calico installs, since it requires extra CPU and memory resources to operate. Issue #2943 As detailed in the calico docs Istio 1.1.7 does not support Kubernetes 1.16+. This issue documents the current workarounds and status. As a result the below is left for record of how to implement ALP when a fix is available. Now enable the application layer policy. cd ~/calico # Run calicoctl and output the existing config calicoctl get felixconfiguration default \\ --export -o yaml | \\ sed -e '/ policySyncPathPrefix:/d' \\ -e '$ a\\ policySyncPathPrefix: /var/run/nodeagent' > felix-config.yaml calicoctl apply -f - < felix-config.yaml Install Istio mkdir ~/istio ; \\ cd ~/istio curl -L https://git.io/getLatestIstio | sh - export ISTIO_DIR = $( ls -d istio-* ) export PATH = ~/istio/ $ISTIO_DIR /bin: $PATH # enable autocompletion and add to path tee >> ~/.bash_profile <<EOL # add istio to path export PATH=~/istio/$ISTIO_DIR/bin:\\$PATH # add istioctl autocompletion test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash EOL source ~/.bash_profile Now we'll install the new Istio CNI plugin as per the docs # file: ~/istio/cni.yaml apiVersion : install.istio.io/v1alpha2 kind : IstioControlPlane spec : cni : enabled : true values : cni : excludeNamespaces : - istio-system - kube-system unvalidatedValues : cni : logLevel : info and apply with istioctl manifest apply -f ~/istio/cni.yaml . Now we should enable sidecar injection on the namespaces. kubectl label namespace default istio-injection = enabled Mutual TLS See docs for further details. We'll now create a mesh-wide authentication policy that enables mutual TLS. #file: ~/istio/map.yaml apiVersion : \"authentication.istio.io/v1alpha1\" kind : \"MeshPolicy\" metadata : name : \"default\" spec : peers : - mtls : {} and apply with kubectl apply -f ~/istio/map.yaml And now create a policy which will destination rules. # file: ~/istio/destinations.yaml apiVersion : \"networking.istio.io/v1alpha3\" kind : \"DestinationRule\" metadata : name : \"default\" namespace : \"istio-system\" spec : host : \"*.local\" trafficPolicy : tls : mode : ISTIO_MUTUAL annd apply with kubectl apply -f ~/istio/destinations.yaml kubectl apply -f ${ISTIO_DIR}/nstall/kubernetes/istio-demo-auth.yaml","title":"Installing Calicoctl"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/#installing-calicoctl","text":"See the calico docs for more details. mkdir -p ~/calico ; \\ cd ~/calico # Install as a pod wget https://docs.projectcalico.org/v3.10/manifests/calicoctl.yaml ; \\ kubectl apply -f ~/calico/calicoctl.yaml You can then run commands using kubectl as shown below. alicoctl $ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get profiles -o wide NAME LABELS kns.default kns.kube-node-lease kns.kube-public kns.kube-system ksa.default.default ksa.kube-node-lease.default ksa.kube-public.default ksa.kube-system.attachdetach-controller ksa.kube-system.bootstrap-signer ksa.kube-system.calicoctl ksa.kube-system.canal ksa.kube-system.certificate-controller ksa.kube-system.clusterrole-aggregation-controller ksa.kube-system.coredns ksa.kube-system.cronjob-controller ksa.kube-system.daemon-set-controller ksa.kube-system.default ksa.kube-system.deployment-controller ksa.kube-system.disruption-controller ksa.kube-system.endpoint-controller ksa.kube-system.expand-controller ksa.kube-system.generic-garbage-collector ksa.kube-system.horizontal-pod-autoscaler ksa.kube-system.job-controller ksa.kube-system.kube-proxy ksa.kube-system.namespace-controller ksa.kube-system.node-controller ksa.kube-system.persistent-volume-binder ksa.kube-system.pod-garbage-collector ksa.kube-system.pv-protection-controller ksa.kube-system.pvc-protection-controller ksa.kube-system.replicaset-controller ksa.kube-system.replication-controller ksa.kube-system.resourcequota-controller ksa.kube-system.service-account-controller ksa.kube-system.service-controller ksa.kube-system.statefulset-controller ksa.kube-system.token-cleaner ksa.kube-system.ttl-controller We'll create an alias of the kubectl/calicoctl command for sanity. tee ~/.bash_profile <<EOF # Load .bashrc if it exists test -f ~/.bashrc && source ~/.bashrc # Load .bash_aliases if it exists test -f ~/.bash_aliases && source ~/.bash_aliases EOF tee ~/.bash_aliases <<EOF # Calico alias alias calicoctl=\"kubectl exec -i -n kube-system calicoctl /calicoctl -- \" EOF Note: In order to use manifests with this alias you need to redirect them. E.g. calicoctl create -f - < my_manifest.yaml","title":"Installing Calicoctl"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/#enable-application-layer-policy-alp","text":"Application layer policy for Calico allows you to write policies that enforce against application layer attributes like HTTP methods or paths as well as against cryptographically secure identities. Support for application layer policy is not enabled by default in Calico installs, since it requires extra CPU and memory resources to operate. Issue #2943 As detailed in the calico docs Istio 1.1.7 does not support Kubernetes 1.16+. This issue documents the current workarounds and status. As a result the below is left for record of how to implement ALP when a fix is available. Now enable the application layer policy. cd ~/calico # Run calicoctl and output the existing config calicoctl get felixconfiguration default \\ --export -o yaml | \\ sed -e '/ policySyncPathPrefix:/d' \\ -e '$ a\\ policySyncPathPrefix: /var/run/nodeagent' > felix-config.yaml calicoctl apply -f - < felix-config.yaml","title":"Enable application layer policy (ALP)"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/#install-istio","text":"mkdir ~/istio ; \\ cd ~/istio curl -L https://git.io/getLatestIstio | sh - export ISTIO_DIR = $( ls -d istio-* ) export PATH = ~/istio/ $ISTIO_DIR /bin: $PATH # enable autocompletion and add to path tee >> ~/.bash_profile <<EOL # add istio to path export PATH=~/istio/$ISTIO_DIR/bin:\\$PATH # add istioctl autocompletion test -f ~/istio/${ISTIO_DIR}/tools/istioctl.bash && source ~/istio/${ISTIO_DIR}/tools/istioctl.bash EOL source ~/.bash_profile Now we'll install the new Istio CNI plugin as per the docs # file: ~/istio/cni.yaml apiVersion : install.istio.io/v1alpha2 kind : IstioControlPlane spec : cni : enabled : true values : cni : excludeNamespaces : - istio-system - kube-system unvalidatedValues : cni : logLevel : info and apply with istioctl manifest apply -f ~/istio/cni.yaml . Now we should enable sidecar injection on the namespaces. kubectl label namespace default istio-injection = enabled","title":"Install Istio"},{"location":"98.wip/04.application.layer.policy.and.istio.mesh/#mutual-tls","text":"See docs for further details. We'll now create a mesh-wide authentication policy that enables mutual TLS. #file: ~/istio/map.yaml apiVersion : \"authentication.istio.io/v1alpha1\" kind : \"MeshPolicy\" metadata : name : \"default\" spec : peers : - mtls : {} and apply with kubectl apply -f ~/istio/map.yaml And now create a policy which will destination rules. # file: ~/istio/destinations.yaml apiVersion : \"networking.istio.io/v1alpha3\" kind : \"DestinationRule\" metadata : name : \"default\" namespace : \"istio-system\" spec : host : \"*.local\" trafficPolicy : tls : mode : ISTIO_MUTUAL annd apply with kubectl apply -f ~/istio/destinations.yaml kubectl apply -f ${ISTIO_DIR}/nstall/kubernetes/istio-demo-auth.yaml","title":"Mutual TLS"},{"location":"99.snippets/ceph/","text":"Useful snippets for interacting with Ceph. Toolbox Connect to the toolbox kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash You can then run commands such as ceph device ls Normal If you don't have any devices or OSDs created check what's happening during startup. See docs # get the prepare pods in the cluster $ kubectl -n rook-ceph get pod -l app = rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-node1-fvmrp 0 /1 Completed 0 18m rook-ceph-osd-prepare-node2-w9xv9 0 /1 Completed 0 22m rook-ceph-osd-prepare-node3-7rgnv 0 /1 Completed 0 22m # view the logs for the node of interest in the \"provision\" container $ kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision Remove and wipe kubectl delete -f ~/rook/toolbox.yaml ; \\ kubectl delete -f ~/rook/cluster.yaml ; \\ kubectl delete -f ~/rook/operator.yaml ; \\ kubectl delete -f ~/rook/common.yaml ; \\ kubectl delete namespace rook-ceph ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Ceph"},{"location":"99.snippets/ceph/#toolbox","text":"Connect to the toolbox kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash You can then run commands such as ceph device ls","title":"Toolbox"},{"location":"99.snippets/ceph/#normal","text":"If you don't have any devices or OSDs created check what's happening during startup. See docs # get the prepare pods in the cluster $ kubectl -n rook-ceph get pod -l app = rook-ceph-osd-prepare NAME READY STATUS RESTARTS AGE rook-ceph-osd-prepare-node1-fvmrp 0 /1 Completed 0 18m rook-ceph-osd-prepare-node2-w9xv9 0 /1 Completed 0 22m rook-ceph-osd-prepare-node3-7rgnv 0 /1 Completed 0 22m # view the logs for the node of interest in the \"provision\" container $ kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision","title":"Normal"},{"location":"99.snippets/ceph/#remove-and-wipe","text":"kubectl delete -f ~/rook/toolbox.yaml ; \\ kubectl delete -f ~/rook/cluster.yaml ; \\ kubectl delete -f ~/rook/operator.yaml ; \\ kubectl delete -f ~/rook/common.yaml ; \\ kubectl delete namespace rook-ceph ; \\ rm -rf ~/rook ; \\ sudo rm -rf /var/lib/rook/*","title":"Remove and wipe"},{"location":"99.snippets/kubernetes/","text":"Useful snippets of code for interfacing with Kubernetes Wiping a kubernetes installation and starting again Because sometimes stuff goes wrong... kubeadm reset ; \\ sudo apt-get -y purge kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get -y autoremove ; \\ sudo rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd sudo shutdown -r now sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl sudo kubeadm init \\ --pod-network-cidr = 10 .244.0.0/16 \\ --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) To also ensure that rook configuration has been removed sudo rm -rf /var/lib/rook/* Getting the IP address of a node export NODE_NAME = banks kubectl get node ${ NODE_NAME } -o jsonpath = '{.status.addresses[0].address}' Kill a namespace stuck as \"Terminating\" See: https://stackoverflow.com/a/53661717/322358 export NAMESPACE = your-rogue-namespace kubectl proxy & kubectl get namespace $NAMESPACE -o json | jq '.spec = {\"finalizers\":[]}' >temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127 .0.0.1:8001/api/v1/namespaces/ $NAMESPACE /finalize","title":"Kubernetes"},{"location":"99.snippets/kubernetes/#wiping-a-kubernetes-installation-and-starting-again","text":"Because sometimes stuff goes wrong... kubeadm reset ; \\ sudo apt-get -y purge kubeadm kubectl kubelet kubernetes-cni kube* ; \\ sudo apt-get -y autoremove ; \\ sudo rm -rf ~/.kube ; \\ sudo rm -rf /etc/kubernetes ; \\ sudo rm -rf /var/lib/etcd sudo shutdown -r now sudo apt-get update ; \\ sudo apt-get install -y kubelet kubeadm kubectl sudo kubeadm init \\ --pod-network-cidr = 10 .244.0.0/16 \\ --apiserver-advertise-address $( ip -4 addr show wg0 | grep inet | awk '{print $2}' | awk -F/ '{print $1}' ) To also ensure that rook configuration has been removed sudo rm -rf /var/lib/rook/*","title":"Wiping a kubernetes installation and starting again"},{"location":"99.snippets/kubernetes/#getting-the-ip-address-of-a-node","text":"export NODE_NAME = banks kubectl get node ${ NODE_NAME } -o jsonpath = '{.status.addresses[0].address}'","title":"Getting the IP address of a node"},{"location":"99.snippets/kubernetes/#kill-a-namespace-stuck-as-terminating","text":"See: https://stackoverflow.com/a/53661717/322358 export NAMESPACE = your-rogue-namespace kubectl proxy & kubectl get namespace $NAMESPACE -o json | jq '.spec = {\"finalizers\":[]}' >temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127 .0.0.1:8001/api/v1/namespaces/ $NAMESPACE /finalize","title":"Kill a namespace stuck as \"Terminating\""},{"location":"99.snippets/storage.disks/","text":"How to setup encrypted disks for usage by Ceph If Rook determines that a device is not available (has existing partitions or a formatted file system ) then it will skip consuming the devices. As a result we need to encrypt the disk and leave it without partitions or a filesystem for it to be read correctly. Finding the relevant disks is done with lsblk . example lsblk $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID } Auto-mount encrypted devices at boot We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Storage"},{"location":"99.snippets/storage.disks/#how-to-setup-encrypted-disks-for-usage-by-ceph","text":"If Rook determines that a device is not available (has existing partitions or a formatted file system ) then it will skip consuming the devices. As a result we need to encrypt the disk and leave it without partitions or a filesystem for it to be read correctly. Finding the relevant disks is done with lsblk . example lsblk $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 89 .1M 1 loop /snap/core/8039 loop1 7 :1 0 88 .5M 1 loop /snap/core/7270 sda 8 :0 0 111 .3G 0 disk \u251c\u2500sda1 8 :1 0 1M 0 part \u2514\u2500sda2 8 :2 0 111 .3G 0 part / sdb 8 :16 0 1 .8T 0 disk sdc 8 :32 0 1 .8T 0 disk sr0 11 :0 1 1024M 0 rom # Create a keyfile (for automounting when plugged in) # This will take a number of minutes... dd if = /dev/random of = /root/secretkey bs = 1 count = 4096 chmod 0400 /root/secretkey # Set the disk export d = sdb export DISK = /dev/ ${ d } # Reset using fdisk (write new GPT and single partition) printf \"g\\nn\\n1\\n\\n\\nw\\n\" | sudo fdisk \" ${ DISK } \" # Encrypt the partition cryptsetup luksFormat -s 512 -c aes-xts-plain64 ${ DISK } 1 # Add the keyfile cryptsetup luksAddKey ${ DISK } 1 /root/secretkey # Open and format cryptsetup open open -d /root/secretkey ${ DISK } 1 luks- ${ d } mkfs.btrfs -f -L DATA /dev/mapper/luks- ${ d } # Mount mkdir -p /mnt/ ${ BLKID } mount -t btrfs -o defaults,noatime,compress = lzo,autodefrag /dev/mapper/luks- $d /mnt/ ${ BLKID }","title":"How to setup encrypted disks for usage by Ceph"},{"location":"99.snippets/storage.disks/#auto-mount-encrypted-devices-at-boot","text":"We'll now configure the system to automatically unlock the encrypted partitions on boot. Edit the /etc/crypttab file to provide the nexessary information. For that we'll need the UUID for each block device which can be found from the blkid command. For more details on the principles and processes behind the below see the excellent Arch Wiki . The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems. crypttab is read before fstab , so that dm-crypt containers can be unlocked before the file system inside is mounted. # Get the UUID export d = sdb export DISK = /dev/ ${ d } export BLKID = $( blkid ${ DISK } 1 | awk -F '\"' '{print $2}' ) # Now edit the crypttab # file: /etc/crypttab # Fields are: name, underlying device, passphrase, cryptsetup options. # The below mounts the device with UUID into /dev/mapper/data-uuid and unlocks using the secretkey echo \"data- ${ BLKID } UUID= ${ BLKID } /root/secretkey luks,retry=1,timeout=180\" >> /etc/crypttab # Add to fstab # file: /etc/fstab echo \"/dev/mapper/data- ${ BLKID } /data/ ${ BLKID } btrfs defaults 0 2\" >> /etc/fstab","title":"Auto-mount encrypted devices at boot"}]}